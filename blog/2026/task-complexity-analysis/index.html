<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Task Complexity Analysis | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We analyzed and compared 3 different sampling strategies to sample 100 questions from MATH 500 and Hendrycks' 4500 (the remaining questions from Hendrycks' math dataset that wasn't selected for MATH500). We ran the subsets on 13 different small (&lt;=7b), open-weight models' and analyzed which sampling strategy caused the highest shift in model rankings for the entire 13 models subset, for the high performing models, and for the low-performing models. Resampling based on a higher &lt;i&gt;apparent difficulty&lt;/i&gt; (the levels assigned by Hendrycks' creators) than MATH 500 results in a more stable ranking for the low performing models, but the baseline apparent difficulty split is the most stable for the top performing models."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/task-complexity-analysis/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Task Complexity Analysis",
            "description": "We analyzed and compared 3 different sampling strategies to sample 100 questions from MATH 500 and Hendrycks' 4500 (the remaining questions from Hendrycks' math dataset that wasn't selected for MATH500). We ran the subsets on 13 different small (<=7b), open-weight models' and analyzed which sampling strategy caused the highest shift in model rankings for the entire 13 models subset, for the high performing models, and for the low-performing models. Resampling based on a higher <i>apparent difficulty</i> (the levels assigned by Hendrycks' creators) than MATH 500 results in a more stable ranking for the low performing models, but the baseline apparent difficulty split is the most stable for the top performing models.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Task Complexity Analysis</h1> <p>We analyzed and compared 3 different sampling strategies to sample 100 questions from MATH 500 and Hendrycks' 4500 (the remaining questions from Hendrycks' math dataset that wasn't selected for MATH500). We ran the subsets on 13 different small (&lt;=7b), open-weight models' and analyzed which sampling strategy caused the highest shift in model rankings for the entire 13 models subset, for the high performing models, and for the low-performing models. Resampling based on a higher <i>apparent difficulty</i> (the levels assigned by Hendrycks' creators) than MATH 500 results in a more stable ranking for the low performing models, but the baseline apparent difficulty split is the most stable for the top performing models.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#method">Method</a> </div> <div> <a href="#results">Results</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#appendix">Appendix</a> </div> </nav> </d-contents> <style>figcaption{color:white!important}</style> <style>.info-panel{background-color:#e7f3ff;border-left:4px solid #2196f3;padding:15px 20px;margin:20px 0;border-radius:4px;color:black!important}.info-panel strong{color:#1976d2}</style> <h2 id="introduction">Introduction</h2> <p>Chasing absolute performance has paved the way of very large language model sizes, but there is still strong interest in language models of smaller sizes <d-cite key="belcak2025small"></d-cite> that has been fueled by the need for efficiency and accessibility in training, and even as ways to aid inference of their larger counterparts <d-cite key="chen2023accelerating"></d-cite>.</p> <p>However these models are often subjected to the same tasks as their larger counterparts in order to try to measure their performance. We argue this may not inherently yield useful information as many contemporary tasks are made as difficult as possible to avoid being quickly saturated or highlight frontier capabilities that many large language models are tested for.</p> <p>Smaller language models’ performance will typically be low and noisy, meaning it will be hard to know for sure which among a group of small language models perform better than the other expressed in “shakeups” or arbitrary changes in the ranking by performance score. In this blogpost, we show that this noisy signal is observable in MATH 500. By revisiting the Hendrcyks’ Math as the original source of this popular task, we show that simple changes to the resampling strategies can produce an alternative “MATH 100” that is more informative in observing model performance of smaller language models.</p> <p>We were curious to see whether:</p> <ol> <li>There are any resampling strategies that would result in a less noisy model accuracy ranking (with less shakeup s) compared to the “no constraint” baseline sampling strategy, for both MATH 500 and Hendryck’s 4500?</li> <li>For the lower and higher performing sets of models that we test, do specific sampling strategies result in different amounts of model accuracy ranking shakeups?</li> </ol> <p>We tested our method on 13 small (under 7b parameters) open weights models over various generations.</p> <h2 id="method">Method</h2> <p>We ran each of the 13 models on both MATH 500 and Hendrycks’ 4500 once and recorded their responses.</p> <p>We then resampled 100 questions + responses from both datasets, 150,000 times with 3 different strategies each and characterize what’s happening.</p> <p>Sampling strategies are constraints (or lack thereof) that we employed in order to pick the 100 questions from each of the two datasets. The ones we dive into in the main blogpost revolve around tweaking the proportion of the questions from the different <em>apparent difficulty</em> (the levels assigned by Hendrycks’ creators) levels. The various strategies is described <a href="#sampling-methods--sampling-strategy">here</a>, and there is a deeper dive on it in the [Appendix].(#resampling-strategies)</p> <p>We then used the resampled responses to compute the models’ ranks, all of the associated statistics and individual accuracies (over the 100 questions, resampled 150,000 ways per strategy on each dataset) to examine which sampling strategy suits the stronger-performing models and which ones suit the lower performing ones.</p> <p>A suitable strategy in this case is one that results in a less noisy, less volatile ranking over the sample, characterized by a low average Shannon entropy value (over the 150,000 samples generated by a single strategy). This is described in <a href="#shannon-entropy-of-rank-distribution">Shannon Entropy of Rank Distribution</a>.</p> <p>We also computed Spearman’s ρ and Kendall’s τ and distance (aggregated as medians for each strategy, over the 150,000 resamples per strategy), but the average entropy proved to be more sensitive.</p> <p>We then also computed overall volatility, with the calculations in the <a href="#overall-volatility">corresponding section in the appendix</a>.</p> <p>As far as individual models are concerned, we computed the average Shannon entropy of its rank over the 150,000 runs of the sampling strategy, the coefficient of variation (stddev/mean) of its rank, as well as the <a href="#stability-score">stability score</a> which is the model rank’s variance over the entire run divided by the maximum possible variance.</p> <div class="info-panel"> A sampling strategy that yields <i>lower</i> volatility and entropy, or a <i>high</i> stability score is what we are looking for. </div> <h3 id="dataset">Dataset</h3> <p>MATH 500 is derived from Hendrycks’ Math dataset <d-cite key="lightman2023lets"></d-cite>. Hendrycks’ 4500 is the remaining questions in the dataset that aren’t in the MATH 500 dataset.</p> <p>We resampled 100 questions from MATH 500 (denoted <em>M100</em>) and 100 from Hendrycks’ 4500 (denoted <em>H100</em>). We generated 1,000,000 samples for each subset. Each subset consists of a sampling method + source dataset combination.</p> <h3 id="sampling-methods--sampling-strategy">Sampling Methods / Sampling Strategy</h3> <table> <thead> <tr> <th style="text-align: left">Subset Name</th> <th style="text-align: left">Explanation</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">noconstr</td> <td style="text-align: left">No constraints, apart from no question gets sampled more than once.</td> </tr> <tr> <td style="text-align: left">base_subjlvl</td> <td style="text-align: left">Matches MATH500’s subject and level makeup, scaled down to 100 questions</td> </tr> <tr> <td style="text-align: left">high_subjlvl</td> <td style="text-align: left">Lv. 4 (x35) and Lv. 5 (x35) questions with an even subject split. The remaining 30 are unconstrained.</td> </tr> </tbody> </table> <p>Note that we also tested other empirical difficulty based sampling methods, but they performed worse than the baseline and isn’t discussed here. A short discussion of this is in <a href="#other-sampling-strategies">Other Sampling Strategies</a>.</p> <p>The <em>base_subjlvl</em> constraints are fleshed out in <a href="#math-500-base-subject-level-split-scaled-down-to-100-questions">its own section in the Appendix</a>.</p> <h2 id="models">Models</h2> <p>We tested 13 small, open-weights models from various generations, with parameter counts ranging from 1b to 7b. All are dense models except for the granite-4-h-tiny (7b total with 1b active).</p> <table> <thead> <tr> <th style="text-align: left">Model Name</th> <th style="text-align: right">Proportion Correct - Math500</th> <th style="text-align: right">Proportion Correct - Hendrycks’ 4500</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">qwen/qwen3-4b-2507</td> <td style="text-align: right">0.738</td> <td style="text-align: right">0.743</td> </tr> <tr> <td style="text-align: left">nemotron-research-reasoning-qwen-1.5b</td> <td style="text-align: right">0.144</td> <td style="text-align: right">0.645</td> </tr> <tr> <td style="text-align: left">llama-3.1-nemotron-nano-4b-v1.1</td> <td style="text-align: right">0.590</td> <td style="text-align: right">0.573</td> </tr> <tr> <td style="text-align: left">internvl3_5-2b</td> <td style="text-align: right">0.478</td> <td style="text-align: right">0.473</td> </tr> <tr> <td style="text-align: left">granite-3.3-2b-instruct</td> <td style="text-align: right">0.402</td> <td style="text-align: right">0.452</td> </tr> <tr> <td style="text-align: left">smollm3-3b-128k</td> <td style="text-align: right">0.480</td> <td style="text-align: right">0.359</td> </tr> <tr> <td style="text-align: left">liquid/lfm2-1.2b</td> <td style="text-align: right">0.348</td> <td style="text-align: right">0.345</td> </tr> <tr> <td style="text-align: left">ibm/granite-4-h-tiny</td> <td style="text-align: right">0.336</td> <td style="text-align: right">0.343</td> </tr> <tr> <td style="text-align: left">qwen2.5-vl-3b-instruct</td> <td style="text-align: right">0.314</td> <td style="text-align: right">0.329</td> </tr> <tr> <td style="text-align: left">granite-3.2-2b-instruct</td> <td style="text-align: right">0.254</td> <td style="text-align: right">0.273</td> </tr> <tr> <td style="text-align: left">llama-3.2-3b-instruct</td> <td style="text-align: right">0.262</td> <td style="text-align: right">0.265</td> </tr> <tr> <td style="text-align: left">google/gemma-3-1b</td> <td style="text-align: right">0.248</td> <td style="text-align: right">0.083</td> </tr> <tr> <td style="text-align: left">mistralai/mistral-7b-instruct-v0.3</td> <td style="text-align: right">0.058</td> <td style="text-align: right">0.071</td> </tr> </tbody> </table> <p>The accuracy spread is quite high. Models generally do worse on the entire H4500, which suggests that H4500 has question types that aren’t in these models’ training datasets.</p> <p>This large spread causes an interesting dichotomy regarding the ranking shakeups. The sampling strategies that cause top 5 models’ to be stable are the ones that aren’t optimal for the bottom 8 models, outlined in <a href="#top-5-and-bottom-8-models-ranking-stability">Top 5 and Bottom 8 Models’ Ranking Stability</a></p> <h2 id="evaluation-pipeline">Evaluation Pipeline</h2> <p>Each model is run once on H4500 and once in M500, and their responses recorded for resampling later. More details are provided in the <a href="#technical-details">Appendix</a></p> <h3 id="answer-extraction-and-evaluation">Answer Extraction and Evaluation</h3> <p>Answer extraction is via regex (to find the “boxed” marker), and the extracted LLM answer is checked via an exact string match against the dataset’s solution (gold label), or checking that the absolute value of the LLM’s numerical answer is within 1e-4 of the gold label.</p> <h2 id="results">Results</h2> <h3 id="subset-level-measures">Subset Level Measures</h3> <table> <thead> <tr> <th style="text-align: left">subset</th> <th style="text-align: right">Kendall τ Median</th> <th style="text-align: right">Spearman ρ Median</th> <th style="text-align: right">Kendall Distance Median</th> <th style="text-align: right">Mean Entropy</th> <th style="text-align: right">Overall Volatility</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">h100_0_noconstr</td> <td style="text-align: right">0.846</td> <td style="text-align: right">0.945</td> <td style="text-align: right">0.077</td> <td style="text-align: right">1.510</td> <td style="text-align: right">0.813</td> </tr> <tr> <td style="text-align: left">h100_base_subjlvl</td> <td style="text-align: right">0.846</td> <td style="text-align: right">0.951</td> <td style="text-align: right">0.077</td> <td style="text-align: right">1.486</td> <td style="text-align: right">0.798</td> </tr> <tr> <td style="text-align: left">h100_high_subjlvl</td> <td style="text-align: right">0.872</td> <td style="text-align: right">0.956</td> <td style="text-align: right">0.064</td> <td style="text-align: right">1.405</td> <td style="text-align: right">0.735</td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: left">subset</th> <th style="text-align: right">Kendall τ Median</th> <th style="text-align: right">Spearman ρ Median</th> <th style="text-align: right">Kendall Distance Median</th> <th style="text-align: right">Mean Entropy</th> <th style="text-align: right">Overall Volatility</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">m100_0_noconstr</td> <td style="text-align: right">0.872</td> <td style="text-align: right">0.962</td> <td style="text-align: right">0.064</td> <td style="text-align: right">1.259</td> <td style="text-align: right">0.659</td> </tr> <tr> <td style="text-align: left">m100_base_subjlvl</td> <td style="text-align: right">0.872</td> <td style="text-align: right">0.962</td> <td style="text-align: right">0.064</td> <td style="text-align: right">1.269</td> <td style="text-align: right">0.665</td> </tr> <tr> <td style="text-align: left">m100_high_subjlvl</td> <td style="text-align: right">0.897</td> <td style="text-align: right">0.967</td> <td style="text-align: right">0.051</td> <td style="text-align: right">1.127</td> <td style="text-align: right">0.589</td> </tr> </tbody> </table> <p>The <em>base_subjlvl</em> strategy (mimicking MATH 500’s subject-level makeup) is more stable than having no constraints to the sampling, but forcing questions with higher apparent difficulty (human-determined level) reduces the entropy even further. <a href="#spearmans-rank-correlation-coefficient">Spearman’s ρ</a> and <a href="#kendalls-tau-%CF%84">Kendall’s τ</a> and <a href="#kendall-tau-distance">Kendall’s distance</a> are also lower with the <em>higher_subjlvl</em> sampling strategy for both datasets.</p> <h4 id="top-5-and-bottom-8-models-ranking-stability">Top 5 and Bottom 8 Models’ ranking stability</h4> <div class="info-panel"> The top 5 models show a markedly different behaviour regarding ranking stability than the bottom 8, which is why we chose to divide the 13 models up this way. </div> <p>See <a href="#individual-model-level-measures">Individual Model Level Measures</a> for an illustration of how steady the top 5 are compared to the bottom 8, over 5 iterations, and also in <a href="#most-frequent-ranks-for-each-sampling-strategy">Most Frequent Rank for Each Sampling Strategy</a>.</p> <table> <thead> <tr> <th style="text-align: left">subset</th> <th style="text-align: right">Coeff. Var. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Stab. Score Top5</th> <th style="text-align: right">Coeff. Var. Bot8</th> <th style="text-align: right">Entropy Bot8</th> <th style="text-align: right">Stab. Score Bot8</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">h100_0_noconstr</td> <td style="text-align: right">0.168</td> <td style="text-align: right">0.935</td> <td style="text-align: right">0.993</td> <td style="text-align: right">0.121</td> <td style="text-align: right">1.869</td> <td style="text-align: right">0.967</td> </tr> <tr> <td style="text-align: left">h100_base_subjlvl</td> <td style="text-align: right">0.164</td> <td style="text-align: right">0.903</td> <td style="text-align: right">0.993</td> <td style="text-align: right">0.119</td> <td style="text-align: right">1.851</td> <td style="text-align: right">0.968</td> </tr> <tr> <td style="text-align: left">h100_high_subjlvl</td> <td style="text-align: right">0.178</td> <td style="text-align: right">0.971</td> <td style="text-align: right">0.990</td> <td style="text-align: right">0.107</td> <td style="text-align: right">1.676</td> <td style="text-align: right">0.976</td> </tr> </tbody> </table> <p>Resampling with more emphasis on questions with higher apparent difficulty (<em>high_subjlvl</em>) yields lower entropy for the bottom 8 models, but higher entropy for the top 5 models.</p> <table> <thead> <tr> <th style="text-align: left">subset</th> <th style="text-align: right">Coeff. Var. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Stab. Score Top5</th> <th style="text-align: right">Coeff. Var. Bot8</th> <th style="text-align: right">Entropy Bot8</th> <th style="text-align: right">Stab. Score Bot8</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">m100_0_noconstr</td> <td style="text-align: right">0.068</td> <td style="text-align: right">0.554</td> <td style="text-align: right">0.996</td> <td style="text-align: right">0.118</td> <td style="text-align: right">1.699</td> <td style="text-align: right">0.975</td> </tr> <tr> <td style="text-align: left">m100_base_subjlvl</td> <td style="text-align: right">0.069</td> <td style="text-align: right">0.557</td> <td style="text-align: right">0.996</td> <td style="text-align: right">0.119</td> <td style="text-align: right">1.714</td> <td style="text-align: right">0.975</td> </tr> <tr> <td style="text-align: left">m100_high_subjlvl</td> <td style="text-align: right">0.071</td> <td style="text-align: right">0.502</td> <td style="text-align: right">0.996</td> <td style="text-align: right">0.105</td> <td style="text-align: right">1.518</td> <td style="text-align: right">0.980</td> </tr> </tbody> </table> <p>M100 is more stable in general, with lower entropy overall. However, in M100’s case, <em>high_subjlvl</em> yielded lower entropies for both the top 5 models and the bottom 8 models.</p> <p>The entropy patterns agree with the stability score patterns.</p> <h3 id="individual-model-level-measures">Individual Model Level Measures</h3> <p>The shifts in rankings over 5 iterations on each sampling strategy are shown below, in the style of <d-cite key="alzahrani2024benchmarks"></d-cite>.</p> <p>Note how the bottom 8 models move around a lot more than the top 5.</p> <div class="l-screen" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_base_subjlvl-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_base_subjlvl-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_base_subjlvl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_base_subjlvl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">H100 ranking shifts in a sample of 5 runs of the <i>base_subjlvl</i> sampling strategy.</figcaption> </figure> </div> <div class="l-screen" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_high_subjlvl-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_high_subjlvl-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_high_subjlvl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_high_subjlvl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">H100 ranking shifts in a sample of 5 runs of the <i>high_subjlvl</i> sampling strategy. This moves around the least.</figcaption> </figure> </div> <div class="l-screen" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_0_noconstr-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_0_noconstr-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_0_noconstr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_h100_0_noconstr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">H100 ranking shifts in a sample of 5 runs of the <i>noconstr</i> sampling strategy. This shifts around the most.</figcaption> </figure> </div> <div class="l-screen" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_base_subjlvl-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_base_subjlvl-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_base_subjlvl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_base_subjlvl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">M100 ranking shifts in a sample of 5 runs of the <i>base_subjlvl</i> sampling strategy.</figcaption> </figure> </div> <div class="l-screen" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_high_subjlvl-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_high_subjlvl-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_high_subjlvl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_high_subjlvl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">M100 ranking shifts in a sample of 5 runs of the <i>high_subjlvl</i> sampling strategy. This moves around the least.</figcaption> </figure> </div> <div class="l-screen" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_0_noconstr-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_0_noconstr-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_0_noconstr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_m100_0_noconstr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">M100 ranking shifts in a sample of 5 runs of the <i>noconstr</i> sampling strategy. This shifts around the most.</figcaption> </figure> </div> <h2 id="conclusion">Conclusion</h2> <h3 id="resampling-strategies">Resampling Strategies</h3> <ul> <li>The bottom 8 models’ rankings are more stable with the <em>high_subjlvl</em> sampling strategy compared to <em>no_constraint</em> (11% lower entropy) and <em>base_subjlvl</em> (10% lower entropy).</li> <li>The top 5 models’ rankings are more stable with the <em>base_subjlvl</em> sampling strategy compared to the <em>high_subjlvl</em> (7% lower entropy) or <em>no_constraint</em> (4% lower entropy)</li> </ul> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/h100_t5_raw_accuracies_high_subjlvl_vs_base_subjlvl_vs_no_constraints-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/h100_t5_raw_accuracies_high_subjlvl_vs_base_subjlvl_vs_no_constraints-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/h100_t5_raw_accuracies_high_subjlvl_vs_base_subjlvl_vs_no_constraints-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/h100_t5_raw_accuracies_high_subjlvl_vs_base_subjlvl_vs_no_constraints.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">H100 Top 5: blue (base_subjlvl) has the least entropy, green (no constraints) has more entropy, orange (high_subjlvl) has most entropy.</figcaption> </figure> </div> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/h100_b8_raw_accuracies_high_subjlvl_vs_base_subjlvl_vs_no_constraints-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/h100_b8_raw_accuracies_high_subjlvl_vs_base_subjlvl_vs_no_constraints-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/h100_b8_raw_accuracies_high_subjlvl_vs_base_subjlvl_vs_no_constraints-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/h100_b8_raw_accuracies_high_subjlvl_vs_base_subjlvl_vs_no_constraints.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">H100 Bottom 8: orange (high_subjlvl) has the least entropy, blue (base_subjlvl) has slightly more, and green (no constraints) has the most entropy.</figcaption> </figure> </div> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_baselines-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_baselines-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_baselines-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_baselines.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">M100 Top 5: orange (high_subjlvl) has the least entropy, green (no_constraints) has more entropy, and blue (base_subjlvl) has most entropy.</figcaption> </figure> </div> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_no_constraints-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_no_constraints-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_no_constraints-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_no_constraints.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">M100 Bottom 8: orange (high_subjlvl) has the least entropy, green (no_constraints)) has slightly more, and blue (base_subjlvl) has the most entropy.</figcaption> </figure> </div> <p>It can be seen that for MATH 500 subsets, the individual (not collective) bottom 8 models’ accuracy spread is much tighter with the <em>high_subjlvl</em> sampling strategy (green) when compared to the <em>base_subjlvl</em> (blue). The top 5 models’ spread is tighter as well (red vs. orange respectively).</p> <p>The intuition behind this phenomenon seems to be that if the badly performing models are given more difficult questions, it is easier to distinguish between their performances. If they are given easier questions, there is more of chance that they might be able to arrive at “lucky” guess, resulting in a less stable rank (more entropy).</p> <p>The higher-performing models’ rankings are more stable with the <em>base_subjlvl</em> strategy (despite it being easier for humans) because there are questions in the lower levels that seems to have stumped most of the models. Given a sampling strategy, the top 5 models perform consistently (answers correctly) otherwise; the variance in rank is low since they arise from that small set of questions that all of the models get wrong. Increasing the difficulty of the questions in either the subject-level/apparent or the empirical sense causes more deviation because some models can answer the more difficult questions correctly some of the time.</p> <p>It should also be noted that longer questions seem to be more difficult for the models to solve in general. See <a href="#question-length-vs-accuracy">Question Length vs. Accuracy</a>.</p> <h3 id="math-100-vs-hendrycks-100">MATH 100 vs. Hendrycks’ 100</h3> <p>M100-based accuracy rankings is more stable than H100 in general, perhaps due to there being less questions. It is more informative to resample from H100 because even the models with good M100 accuracy aren’t as consistent with their H100 performance given the same resampling strategy.</p> <h2 id="appendix">Appendix</h2> <h3 id="technical-details">Technical Details</h3> <ul> <li>Temperature is 1.0</li> <li>Output length limit is 8192 tokens</li> </ul> <p>This was run on a PC with the following:</p> <ul> <li>6GB GTX 1060</li> <li>LMStudio 0.3.23</li> <li>LMStudio’s CUDA llama.cpp v1.56.0 runtime</li> <li>CUDA 12.9</li> <li>Nvidia driver version 575.57.08.</li> </ul> <h3 id="prompts">Prompts</h3> <p>The user prompt is done in the style of SimpleRL-Zoo <d-cite key="zeng2025simplerl"></d-cite>, albeit slightly modified with an extra instruction at the end to emphasize the usage of “\boxed”.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>user_prompt_simplerl_zoo = "{}\nPlease reason step by step,
and put your final answer within \\boxed.
PUT YOUR CORRECT FINAL ANSWER WITHIN \\boxed!!!"
</code></pre></div></div> <p>The system prompt is the usual</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You are a helpful assistant
</code></pre></div></div> <h3 id="alternate-experiment-branch">Alternate Experiment Branch</h3> <p>There are 3 sampling strategy families that we actually tested: <em>noconstr</em> (no constraint) <em>subjlvl</em> (subject-level a.k.a. apparent difficulty), and <em>empdiffic</em> (empirical difficulty, see <a href="#empirical-difficulty">Empirical Difficulty</a>), with 3 levels: <em>base</em> (matching MATH 500’s), <em>low</em>, <em>mid</em> and <em>high</em> , for a total of 9.</p> <p>The complete list that we tested is as follows:</p> <table> <thead> <tr> <th style="text-align: left">Subset Name</th> <th style="text-align: left">Explanation</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">noconstr</td> <td style="text-align: left">No constraints apart from no question gets sampled more than once.</td> </tr> <tr> <td style="text-align: left">base_subjlvl</td> <td style="text-align: left">Matches MATH500’s subject and level makeup, scaled down to 100 questions</td> </tr> <tr> <td style="text-align: left">low_subjlvl</td> <td style="text-align: left">Lv. 2 (x35) and Lv. 3 (x35) questions with an even subject split. The remaining 30 are unconstrained.</td> </tr> <tr> <td style="text-align: left">mid_subjlvl</td> <td style="text-align: left">Lv. 3 (x35) and Lv. 4 (x35) questions with an even subject split. The remaining 30 are unconstrained.</td> </tr> <tr> <td style="text-align: left">high_subjlvl</td> <td style="text-align: left">Lv. 4 (x35) and Lv. 5 (x35) questions with an even subject split. The remaining 30 are unconstrained.</td> </tr> <tr> <td style="text-align: left">base_empdiffic</td> <td style="text-align: left">Matches MATH500’s empirical difficulty (proportion correct) bins. See the “Empirical Difficulty” section above.</td> </tr> <tr> <td style="text-align: left">low_empdiffic</td> <td style="text-align: left">35% from the easiest bin (p0-p25), 35% from the 2nd bin (p25-p50), the rest can come from any other bin.</td> </tr> <tr> <td style="text-align: left">mid_empdiffic</td> <td style="text-align: left">35% from the 2nd bin (p25-p50), 35% from the 3rd bin (p60-p75), the rest can come from any other bin.</td> </tr> <tr> <td style="text-align: left">high_empdiffic</td> <td style="text-align: left">35% from the 3rd bin (p50-p75), 35% from the hardest bin (p75-p100), the rest can come from any other bin.</td> </tr> </tbody> </table> <p>The <em>subjlvl</em> strategies and their results are outlined in the main blog.</p> <h4 id="empirical-difficulty">Empirical Difficulty</h4> <p>We thought it would be a good idea to try to match the difficulty level (collective accuracy of the 13 models on a particular question).</p> <p>To that end, the <em>base_empdiffic</em> sampling strategy matched MATH500’s empirical difficulty: we cut the dataset into 4 difficulty bins based on the collective accuracy of the 13 models on a particular question, with the cutoffs mimicking MATH 500’s results with the 13 models that we tested.</p> <p>The cutoffs for each bin are shown in the table below.</p> <p>A histogram of the empirical difficulty is shown below:</p> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/emp_diff_hist-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/emp_diff_hist-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/emp_diff_hist-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/emp_diff_hist.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Empirical Difficulties of the entire H4500 and M500 dataset. Note the P25 and P50 are higher for H4500.</figcaption> </figure> </div> <p>The bin edges are shown below.</p> <table> <thead> <tr> <th style="text-align: left">Measure</th> <th style="text-align: right">Proportion Correct, M500</th> <th style="text-align: right">Proportion Correct, H4500</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">count</td> <td style="text-align: right">4500</td> <td style="text-align: right">500</td> </tr> <tr> <td style="text-align: left">mean</td> <td style="text-align: right">0.381</td> <td style="text-align: right">0.358</td> </tr> <tr> <td style="text-align: left">std</td> <td style="text-align: right">0.257</td> <td style="text-align: right">0.277</td> </tr> <tr> <td style="text-align: left">25%</td> <td style="text-align: right">0.154</td> <td style="text-align: right">0.077</td> </tr> <tr> <td style="text-align: left">50%</td> <td style="text-align: right">0.385</td> <td style="text-align: right">0.308</td> </tr> <tr> <td style="text-align: left">75%</td> <td style="text-align: right">0.615</td> <td style="text-align: right">0.615</td> </tr> </tbody> </table> <p>In this case, 25 questions are sampled from each difficulty bin (of which there are 4 divided by <em>proportion correct</em>: 0 to 0.154 as the most difficult bin, 0.154 to 0.385, 0.385 to 0.615 and 0.615 to 1.0 as the easiest bin).</p> <p>It should be noted that the mean, p25 and median for Hendrycks’ is significantly lower and that the standard deviation is higher.</p> <h4 id="empirical-difficulty-based-sampling-strategies-performances">Empirical Difficulty Based Sampling Strategies’ Performances</h4> <p>Unfortunately the empirical difficulty strategies result in higher entropy and more ranking shakeup than the subject-level based strategies as well as the <em>no_constraints</em> strategy.</p> <ul> <li>For the bottom 8 models, <em>mid_empdiffic</em> is more stable than <em>base_empdiffic</em> (entropy is lower by 3%).</li> <li>The same holds true for the top 5 models as well, with a more pronounced effect (12% lower entropy compared to <em>base_empdiffic</em>)</li> <li> <em>high_empdiffic</em> results have the highest entropy.</li> </ul> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_high_empdiffic-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_high_empdiffic-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_high_empdiffic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/m100_t5_b8_raw_accuracies_high_subjlvl_vs_high_empdiffic.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">M100. Note how blue (high_empdiffic, bottom 8) is more modal than green (high_subjlvl, bottom 8), and orange (high_empdiffic, top 5) is more modal than red (high_subjlvl, top 5), resulting in higher entropy for the empirical difficulty subsets).</figcaption> </figure> </div> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/h100_t5_b8_raw_accuracies_high_subjlvl_vs_high_empdiffic-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/h100_t5_b8_raw_accuracies_high_subjlvl_vs_high_empdiffic-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/h100_t5_b8_raw_accuracies_high_subjlvl_vs_high_empdiffic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/h100_t5_b8_raw_accuracies_high_subjlvl_vs_high_empdiffic.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">H100. Note how blue (high_empdiffic, bottom 8) is more modal than green (high_subjlvl, bottom 8), and orange (high_empdiffic, top 5) is more modal than red (high_subjlvl, top 5), resulting in higher entropy (for empirical difficulty subsets). The difference is more pronounced here than in M100.</figcaption> </figure> </div> <p>These are the complete results for the H100 subsets with all the strategies we tested. We sorted this by the bottom 8 models’ entropy in ascending order. Note the opposing behaviour of the top 5 models.</p> <table> <thead> <tr> <th style="text-align: left">subset</th> <th style="text-align: right">Std. Dev. Top5</th> <th style="text-align: right">Coeff. Var. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Std. Dev. Bot8</th> <th style="text-align: right">Coeff. Var. Bot8</th> <th style="text-align: right">Entropy Bot8</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">h100_high_subjlvl</td> <td style="text-align: right">0.522</td> <td style="text-align: right">0.178</td> <td style="text-align: right">0.971</td> <td style="text-align: right">0.868</td> <td style="text-align: right">0.107</td> <td style="text-align: right">1.676</td> </tr> <tr> <td style="text-align: left">h100_mid_subjlvl</td> <td style="text-align: right">0.467</td> <td style="text-align: right">0.163</td> <td style="text-align: right">0.925</td> <td style="text-align: right">0.934</td> <td style="text-align: right">0.111</td> <td style="text-align: right">1.750</td> </tr> <tr> <td style="text-align: left">h100_base_subjlvl</td> <td style="text-align: right">0.459</td> <td style="text-align: right">0.164</td> <td style="text-align: right">0.903</td> <td style="text-align: right">1.010</td> <td style="text-align: right">0.119</td> <td style="text-align: right">1.851</td> </tr> <tr> <td style="text-align: left">h100_low_subjlvl</td> <td style="text-align: right">0.512</td> <td style="text-align: right">0.176</td> <td style="text-align: right">1.038</td> <td style="text-align: right">1.029</td> <td style="text-align: right">0.120</td> <td style="text-align: right">1.858</td> </tr> <tr> <td style="text-align: left">h100_0_noconstr</td> <td style="text-align: right">0.471</td> <td style="text-align: right">0.168</td> <td style="text-align: right">0.935</td> <td style="text-align: right">1.026</td> <td style="text-align: right">0.121</td> <td style="text-align: right">1.869</td> </tr> <tr> <td style="text-align: left">h100_mid_empdiffic</td> <td style="text-align: right">0.506</td> <td style="text-align: right">0.182</td> <td style="text-align: right">0.983</td> <td style="text-align: right">1.031</td> <td style="text-align: right">0.123</td> <td style="text-align: right">1.888</td> </tr> <tr> <td style="text-align: left">h100_low_empdiffic</td> <td style="text-align: right">0.587</td> <td style="text-align: right">0.196</td> <td style="text-align: right">1.182</td> <td style="text-align: right">1.100</td> <td style="text-align: right">0.127</td> <td style="text-align: right">1.933</td> </tr> <tr> <td style="text-align: left">h100_base_empdiffic</td> <td style="text-align: right">0.552</td> <td style="text-align: right">0.203</td> <td style="text-align: right">1.117</td> <td style="text-align: right">1.174</td> <td style="text-align: right">0.137</td> <td style="text-align: right">2.031</td> </tr> <tr> <td style="text-align: left">h100_high_empdiffic</td> <td style="text-align: right">0.695</td> <td style="text-align: right">0.242</td> <td style="text-align: right">1.327</td> <td style="text-align: right">1.214</td> <td style="text-align: right">0.148</td> <td style="text-align: right">2.088</td> </tr> </tbody> </table> <div class="info-panel"> subjlvl: "base" is the most stable for the top 5 models. "high" is the most stable for the bottom 8 models. <br><br> empdiffic: "mid" is the most stable for all the models. "high" is the least stable. </div> <p>These are the entropy and overall volatility of all the sampling strategies we tested (without splitting it into top 5/bottom 8 measures), ordered by volatility ascending.</p> <table> <thead> <tr> <th style="text-align: left">subset</th> <th style="text-align: right">mean_entropy</th> <th style="text-align: right">overall_volatility</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">h100_high_subjlvl</td> <td style="text-align: right">1.405</td> <td style="text-align: right">0.735</td> </tr> <tr> <td style="text-align: left">h100_mid_subjlvl</td> <td style="text-align: right">1.432</td> <td style="text-align: right">0.754</td> </tr> <tr> <td style="text-align: left">h100_base_subjlvl</td> <td style="text-align: right">1.486</td> <td style="text-align: right">0.798</td> </tr> <tr> <td style="text-align: left">h100_0_noconstr</td> <td style="text-align: right">1.510</td> <td style="text-align: right">0.813</td> </tr> <tr> <td style="text-align: left">h100_mid_empdiffic</td> <td style="text-align: right">1.540</td> <td style="text-align: right">0.829</td> </tr> <tr> <td style="text-align: left">h100_low_subjlvl</td> <td style="text-align: right">1.543</td> <td style="text-align: right">0.830</td> </tr> <tr> <td style="text-align: left">h100_low_empdiffic</td> <td style="text-align: right">1.644</td> <td style="text-align: right">0.903</td> </tr> <tr> <td style="text-align: left">h100_base_empdiffic</td> <td style="text-align: right">1.680</td> <td style="text-align: right">0.935</td> </tr> <tr> <td style="text-align: left">h100_high_empdiffic</td> <td style="text-align: right">1.795</td> <td style="text-align: right">1.014</td> </tr> </tbody> </table> <p>It’s interesting to note that low_subjlvl is the least stable of all the subjlvl based strategies for H100.</p> <table> <thead> <tr> <th style="text-align: left">subset</th> <th style="text-align: right">mean_entropy</th> <th style="text-align: right">overall_volatility</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">m100_low_subjlvl</td> <td style="text-align: right">1.080</td> <td style="text-align: right">0.552</td> </tr> <tr> <td style="text-align: left">m100_mid_subjlvl</td> <td style="text-align: right">1.110</td> <td style="text-align: right">0.561</td> </tr> <tr> <td style="text-align: left">m100_high_subjlvl</td> <td style="text-align: right">1.127</td> <td style="text-align: right">0.589</td> </tr> <tr> <td style="text-align: left">m100_low_empdiffic</td> <td style="text-align: right">1.279</td> <td style="text-align: right">0.657</td> </tr> <tr> <td style="text-align: left">m100_0_noconstr</td> <td style="text-align: right">1.259</td> <td style="text-align: right">0.659</td> </tr> <tr> <td style="text-align: left">m100_base_subjlvl</td> <td style="text-align: right">1.269</td> <td style="text-align: right">0.665</td> </tr> <tr> <td style="text-align: left">m100_mid_empdiffic</td> <td style="text-align: right">1.313</td> <td style="text-align: right">0.686</td> </tr> <tr> <td style="text-align: left">m100_base_empdiffic</td> <td style="text-align: right">1.316</td> <td style="text-align: right">0.705</td> </tr> <tr> <td style="text-align: left">m100_high_empdiffic</td> <td style="text-align: right">1.580</td> <td style="text-align: right">0.888</td> </tr> </tbody> </table> <div class="info-panel"> H100: no_constraint is more volatile than base_subjlvl. For subjlvl from least to most stable: high-mid-base-low <br><br> M100: base_subjlvl is more volatile than no_constraint. For subjlvl from least to most stable: low-mid-high-base. </div> <h4 id="most-frequent-ranks-for-each-sampling-strategy">Most Frequent Ranks for Each Sampling Strategy</h4> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_per_subset_sorted_h100_kt_v1-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_per_subset_sorted_h100_kt_v1-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_per_subset_sorted_h100_kt_v1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_per_subset_sorted_h100_kt_v1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Most frequent rankings for each sampling strategy in M100, ordered by Kendall's tau descending. The empirical-difficulty-based strategies exhibit the most ranking shakeup.</figcaption> </figure> </div> <div class="l-page" style="max-width: 3000px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_per_subset_sorted_m100_kt_v1-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_per_subset_sorted_m100_kt_v1-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_per_subset_sorted_m100_kt_v1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/ranking_shifts_per_subset_sorted_m100_kt_v1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Most frequent rankings for each sampling strategy in M100, ordered by Kendall's tau descending. It is more stable than H100 in general. </figcaption> </figure> </div> <h3 id="empirical-difficulty-aggregate-stats">Empirical Difficulty Aggregate Stats</h3> <table> <thead> <tr> <th style="text-align: right">Difficulty Bin</th> <th style="text-align: right">Prop. Corr. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Prop. Corr. Bot8</th> <th style="text-align: right">Entropy Bot8</th> <th style="text-align: left">dataset</th> </tr> </thead> <tbody> <tr> <td style="text-align: right">3</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: right">2</td> <td style="text-align: right">0.265</td> <td style="text-align: right">5.165</td> <td style="text-align: right">0.075</td> <td style="text-align: right">4.369</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: right">1</td> <td style="text-align: right">0.637</td> <td style="text-align: right">6.363</td> <td style="text-align: right">0.337</td> <td style="text-align: right">6.194</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: right">0</td> <td style="text-align: right">0.834</td> <td style="text-align: right">6.043</td> <td style="text-align: right">0.719</td> <td style="text-align: right">6.365</td> <td style="text-align: left">MATH 500</td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: right">Difficulty Bin</th> <th style="text-align: right">Prop. Corr. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Prop. Corr. Bot8</th> <th style="text-align: right">Entropy Bot8</th> <th style="text-align: left">dataset</th> </tr> </thead> <tbody> <tr> <td style="text-align: right">3</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: right">2</td> <td style="text-align: right">0.311</td> <td style="text-align: right">7.650</td> <td style="text-align: right">0.062</td> <td style="text-align: right">6.504</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: right">1</td> <td style="text-align: right">0.731</td> <td style="text-align: right">8.843</td> <td style="text-align: right">0.282</td> <td style="text-align: right">8.361</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: right">0</td> <td style="text-align: right">0.951</td> <td style="text-align: right">8.283</td> <td style="text-align: right">0.655</td> <td style="text-align: right">8.381</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> </tbody> </table> <p>“3” is the most difficult bin and “0” is the easiest. Dividing by empirical difficulty causes a greater entropy in rankings, because the performance of the models tested varied very widely.</p> <h3 id="apparent-difficulty-aggregate-stats---level">Apparent Difficulty Aggregate Stats - Level</h3> <table> <thead> <tr> <th style="text-align: right">Level</th> <th style="text-align: right">Prop. Corr. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Prop. Corr. Bot8</th> <th style="text-align: right">Entropy Bot8</th> <th style="text-align: left">dataset</th> </tr> </thead> <tbody> <tr> <td style="text-align: right">1</td> <td style="text-align: right">0.651</td> <td style="text-align: right">4.942</td> <td style="text-align: right">0.544</td> <td style="text-align: right">5.231</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: right">2</td> <td style="text-align: right">0.622</td> <td style="text-align: right">5.635</td> <td style="text-align: right">0.426</td> <td style="text-align: right">5.727</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: right">3</td> <td style="text-align: right">0.531</td> <td style="text-align: right">5.631</td> <td style="text-align: right">0.340</td> <td style="text-align: right">5.656</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: right">4</td> <td style="text-align: right">0.434</td> <td style="text-align: right">5.628</td> <td style="text-align: right">0.230</td> <td style="text-align: right">5.464</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: right">5</td> <td style="text-align: right">0.297</td> <td style="text-align: right">5.293</td> <td style="text-align: right">0.125</td> <td style="text-align: right">4.898</td> <td style="text-align: left">MATH 500</td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: right">Level</th> <th style="text-align: right">Prop. Corr. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Prop. Corr. Bot8</th> <th style="text-align: right">Entropy Bot8</th> <th style="text-align: left">dataset</th> </tr> </thead> <tbody> <tr> <td style="text-align: right">1</td> <td style="text-align: right">0.772</td> <td style="text-align: right">7.327</td> <td style="text-align: right">0.488</td> <td style="text-align: right">7.338</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: right">2</td> <td style="text-align: right">0.687</td> <td style="text-align: right">7.924</td> <td style="text-align: right">0.362</td> <td style="text-align: right">7.754</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: right">3</td> <td style="text-align: right">0.636</td> <td style="text-align: right">8.090</td> <td style="text-align: right">0.297</td> <td style="text-align: right">7.799</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: right">4</td> <td style="text-align: right">0.540</td> <td style="text-align: right">7.983</td> <td style="text-align: right">0.214</td> <td style="text-align: right">7.526</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: right">5</td> <td style="text-align: right">0.422</td> <td style="text-align: right">7.828</td> <td style="text-align: right">0.120</td> <td style="text-align: right">7.043</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> </tbody> </table> <p>It’s interesting to note that the <em>middle</em> levels (as determined by those who wrote Hendrycks’) yielded higher entropies than the higher levels or the lower ones, across all the models and on both datasets.</p> <p>The proportion correct does go down as the level increases, so as a whole the higher question levels (those with higher apparently difficulty to humans) are more difficult for LLMs.</p> <h3 id="apparent-difficulty-aggregate-stats---subject-and-level">Apparent Difficulty Aggregate Stats - Subject and Level</h3> <p>To keep this segment short, we’re just displaying the 5 worst subject-levels for the Top 5 models.</p> <table> <thead> <tr> <th style="text-align: left">Subject-Level</th> <th style="text-align: right">Prop. Corr. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Prop. Corr. Bot8</th> <th style="text-align: right">Entropy Bot8</th> <th style="text-align: left">dataset</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Geometry-1</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: right">0.000</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: left">Geometry-5</td> <td style="text-align: right">0.092</td> <td style="text-align: right">1.792</td> <td style="text-align: right">0.010</td> <td style="text-align: right">0.000</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: left">Intermediate Algebra-5</td> <td style="text-align: right">0.167</td> <td style="text-align: right">3.401</td> <td style="text-align: right">0.038</td> <td style="text-align: right">2.398</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: left">Intermediate Algebra-4</td> <td style="text-align: right">0.226</td> <td style="text-align: right">3.258</td> <td style="text-align: right">0.114</td> <td style="text-align: right">3.045</td> <td style="text-align: left">MATH 500</td> </tr> <tr> <td style="text-align: left">Precalculus-4</td> <td style="text-align: right">0.231</td> <td style="text-align: right">2.708</td> <td style="text-align: right">0.038</td> <td style="text-align: right">1.386</td> <td style="text-align: left">MATH 500</td> </tr> </tbody> </table> <p>Level 1 Geometry in MATH 500 stumps both top 5 and bottom 8 model groups.</p> <table> <thead> <tr> <th style="text-align: left">Subject-Level</th> <th style="text-align: right">Prop. Corr. Top5</th> <th style="text-align: right">Entropy Top5</th> <th style="text-align: right">Prop. Corr. Bot8</th> <th style="text-align: right">Entropy Bot8</th> <th style="text-align: left">dataset</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Precalculus-5</td> <td style="text-align: right">0.224</td> <td style="text-align: right">4.927</td> <td style="text-align: right">0.032</td> <td style="text-align: right">3.434</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: left">Precalculus-4</td> <td style="text-align: right">0.299</td> <td style="text-align: right">5.017</td> <td style="text-align: right">0.073</td> <td style="text-align: right">4.078</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: left">Geometry-5</td> <td style="text-align: right">0.314</td> <td style="text-align: right">5.231</td> <td style="text-align: right">0.078</td> <td style="text-align: right">4.304</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: left">Intermediate Algebra-5</td> <td style="text-align: right">0.321</td> <td style="text-align: right">5.971</td> <td style="text-align: right">0.076</td> <td style="text-align: right">5.004</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> <tr> <td style="text-align: left">Counting &amp; Probability-5</td> <td style="text-align: right">0.359</td> <td style="text-align: right">5.293</td> <td style="text-align: right">0.083</td> <td style="text-align: right">4.304</td> <td style="text-align: left">Hendrycks’ 4500</td> </tr> </tbody> </table> <p>Level 1 geometry questions in Hendrycks’ do not seem to be as anomalous as those that made it into MATH 500. However, there are subject + levels with entropies in the 5-6 range in Hendrycks’ (max is still under 5 in MATH 500)</p> <h3 id="question-length-vs-accuracy">Question Length vs. Accuracy</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-task-complexity-analysis/emp_diff_vs_question_length_lineplot_errorbar_ci-480.webp 480w,/2026/assets/img/2026-04-27-task-complexity-analysis/emp_diff_vs_question_length_lineplot_errorbar_ci-800.webp 800w,/2026/assets/img/2026-04-27-task-complexity-analysis/emp_diff_vs_question_length_lineplot_errorbar_ci-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-task-complexity-analysis/emp_diff_vs_question_length_lineplot_errorbar_ci.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Longer questions are more difficult to answer correctly for the 13 models that we tested.</figcaption> </figure> <h3 id="equations">Equations</h3> <p>The formulas are presented below.</p> <h4 id="kendalls-tau-τ">Kendall’s Tau (τ)</h4> <p>Kendall’s tau measures rank correlation between two rankings:</p> \[\tau = \frac{n_c - n_d}{\binom{n}{2}}\] <p>where:</p> <ul> <li>$n_c$ = number of concordant pairs</li> <li>$n_d$ = number of discordant pairs</li> <li>$\binom{n}{2} = \frac{n(n-1)}{2}$ = total number of pairs</li> </ul> <h4 id="spearmans-rank-correlation-coefficient">Spearman’s rank correlation coefficient:</h4> \[\rho = 1 - \frac{6\sum d_i^2}{n(n^2-1)}\] <p>where:</p> <ul> <li>$d_i$ = difference between ranks for item $i$</li> <li>$n$ = number of items being ranked</li> </ul> <h4 id="kendall-tau-distance">Kendall Tau Distance</h4> <p>Normalized distance between two rankings:</p> \[D_K(R_1, R_2) = \frac{|\{(i,j) : R_1(i) &lt; R_1(j) \land R_2(i) &gt; R_2(j)\}|}{\binom{n}{2}}\] <p>This measures the proportion of discordant pairs, where $R_1$ and $R_2$ are two different rankings.</p> <h4 id="standard-deviation-of-ranks">Standard Deviation of Ranks</h4> \[\sigma_m = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(r_{m,i} - \bar{r}_m)^2}\] <p>where:</p> <ul> <li>$r_{m,i}$ = rank of model $m$ in sample $i$</li> <li>$\bar{r}_m$ = mean rank of model $m$ across all samples</li> <li>$N$ = number of samples</li> </ul> <h4 id="coefficient-of-variation">Coefficient of Variation</h4> \[CV_m = \frac{\sigma_m}{\bar{r}_m}\] <p>Range \(\text{Range}_m = \max_i(r_{m,i}) - \min_i(r_{m,i})\)</p> <h4 id="overall-volatility">Overall Volatility</h4> <p><strong>Overall volatility</strong> is the mean of the standard deviations of ranks across all models. It provides a single summary metric of how much rankings fluctuate across the entire model set.</p> <p>Step 1: Organize the Data Matrix</p> <p>Given $N$ samples and $M$ models, construct a ranking matrix $\mathbf{R}$:</p> \[\mathbf{R} = \begin{bmatrix} r_{1,1} &amp; r_{1,2} &amp; \cdots &amp; r_{1,M} \\ r_{2,1} &amp; r_{2,2} &amp; \cdots &amp; r_{2,M} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ r_{N,1} &amp; r_{N,2} &amp; \cdots &amp; r_{N,M} \end{bmatrix}\] <p>where:</p> <ul> <li>$r_{i,m}$ = rank of model $m$ in sample $i$</li> <li>Rows = samples (e.g., $N = 150{,}000$)</li> <li>Columns = models (e.g., $M = 13$)</li> </ul> <p>Step 2: Calculate Mean Rank per Model</p> <p>For each model $m$, compute the mean rank across all samples:</p> \[\bar{r}_m = \frac{1}{N}\sum_{i=1}^{N} r_{i,m}\] <p>This gives us a vector of mean ranks:</p> \[\bar{\mathbf{r}} = [\bar{r}_1, \bar{r}_2, \ldots, \bar{r}_M]\] <p>Step 3: Calculate Standard Deviation per Model</p> <p>For each model $m$, compute the standard deviation of ranks:</p> \[\sigma_m = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(r_{i,m} - \bar{r}_m)^2}\] <p>This gives us a vector of standard deviations:</p> \[\boldsymbol{\sigma} = [\sigma_1, \sigma_2, \ldots, \sigma_M]\] <p><strong>Interpretation of $\sigma_m$:</strong></p> <ul> <li> <strong>Low $\sigma_m$</strong> (e.g., $\sigma_m &lt; 1$): Model $m$ has very consistent ranking across samples</li> <li> <strong>High $\sigma_m$</strong> (e.g., $\sigma_m &gt; 3$): Model $m$’s ranking varies substantially across samples</li> </ul> <p>Step 4: Calculate Overall Volatility</p> <p>Overall volatility is simply the arithmetic mean of all per-model standard deviations:</p> \[\text{Overall Volatility} = \frac{1}{M}\sum_{m=1}^{M} \sigma_m\] <p>Equivalently, in vector notation:</p> \[\text{Overall Volatility} = \frac{1}{M}\|\boldsymbol{\sigma}\|_1 = \frac{1}{M}\sum_{m=1}^{M} \sigma_m\] <h3 id="stability-score">Stability Score</h3> \[S_m = 1 - \frac{\sigma_m^2}{\sigma_{\max}^2}\] <p>where:</p> <ul> <li>$S_m$ = stability score for model $m$ (range: [0, 1])</li> <li>$\sigma_m^2$ = variance of model $m$’s ranks across samples</li> <li>$\sigma_{\max}^2$ = maximum possible variance = $\frac{(M-1)^2}{4}$</li> <li>$M$ = total number of models</li> </ul> <p>Step 1: Calculate Variance per Model</p> \[\sigma_m^2 = \frac{1}{N}\sum_{i=1}^{N}(r_{i,m} - \bar{r}_m)^2\] <p>where:</p> <ul> <li>$r_{i,m}$ = rank of model $m$ in sample $i$</li> <li>$\bar{r}_m$ = mean rank of model $m$</li> <li>$N$ = number of samples</li> </ul> <p>Step 2: Calculate Maximum Possible Variance</p> <p>For $M$ models with ranks ${1, 2, \ldots, M}$:</p> \[\sigma_{\max}^2 = \frac{(M-1)^2}{4}\] <p>This is the variance when a model alternates between rank 1 and rank $M$ with equal probability.</p> <p>Step 3: Normalize to Get Stability Score</p> \[S_m = 1 - \frac{\sigma_m^2}{\sigma_{\max}^2}\] <h2 id="individual-model-level-measures-1">Individual Model Level Measures</h2> <h3 id="mean-absolute-deviation-mad">Mean Absolute Deviation (MAD)</h3> \[\text{MAD}_m = \frac{1}{N}\sum_{i=1}^{N}|r_{m,i} - \tilde{r}_m|\] <p>where $\tilde{r}_m$ = median rank of model $m$</p> <h3 id="shannon-entropy-of-rank-distribution">Shannon Entropy of Rank Distribution</h3> \[H_m = -\sum_{r=1}^{n} p(r) \log_2 p(r)\] <p>where:</p> <ul> <li>$p(r)$ = probability that model $m$ has rank $r$</li> <li>Higher entropy indicates more uncertainty/variability</li> </ul> <h2 id="math-500-base-subject-level-split-scaled-down-to-100-questions">MATH 500 base subject-level split scaled down to 100 questions.</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    constraints_base_subjlvl = [
        {'subject': 'Intermediate Algebra', 'level': 5, 'count': 7},
        {'subject': 'Algebra', 'level': 4, 'count': 6},
        {'subject': 'Algebra', 'level': 5, 'count': 6},
        {'subject': 'Algebra', 'level': 3, 'count': 5},
        {'subject': 'Intermediate Algebra', 'level': 4, 'count': 5},
        {'subject': 'Algebra', 'level': 2, 'count': 4},
        {'subject': 'Prealgebra', 'level': 4, 'count': 4},
        {'subject': 'Number Theory', 'level': 4, 'count': 4},
        {'subject': 'Intermediate Algebra', 'level': 3, 'count': 4},
        {'subject': 'Prealgebra', 'level': 5, 'count': 4},
        {'subject': 'Prealgebra', 'level': 2, 'count': 4},
        {'subject': 'Prealgebra', 'level': 3, 'count': 3},
        {'subject': 'Algebra', 'level': 1, 'count': 3},
        {'subject': 'Number Theory', 'level': 3, 'count': 3},
        {'subject': 'Precalculus', 'level': 3, 'count': 3},
        {'subject': 'Precalculus', 'level': 2, 'count': 3},
        {'subject': 'Counting &amp; Probability', 'level': 4, 'count': 3},
        {'subject': 'Geometry', 'level': 5, 'count': 3},
        {'subject': 'Precalculus', 'level': 4, 'count': 3},
        {'subject': 'Intermediate Algebra', 'level': 2, 'count': 2},
        {'subject': 'Number Theory', 'level': 5, 'count': 2},
        {'subject': 'Precalculus', 'level': 5, 'count': 2},
        {'subject': 'Counting &amp; Probability', 'level': 5, 'count': 2},
        {'subject': 'Geometry', 'level': 4, 'count': 2},
        {'subject': 'Number Theory', 'level': 2, 'count': 2},
        {'subject': 'Geometry', 'level': 2, 'count': 1},
        {'subject': 'Geometry', 'level': 3, 'count': 2},
        {'subject': 'Prealgebra', 'level': 1, 'count': 1},
        {'subject': 'Counting &amp; Probability', 'level': 2, 'count': 1},
        {'subject': 'Intermediate Algebra', 'level': 1, 'count': 1},
        {'subject': 'Number Theory', 'level': 1, 'count': 1},
        {'subject': 'Counting &amp; Probability', 'level': 3, 'count': 1},
        {'subject': 'Precalculus', 'level': 1, 'count': 1},
        {'subject': 'Geometry', 'level': 1, 'count': 1},
        {'subject': 'Counting &amp; Probability', 'level': 1, 'count': 1}
    ]
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-task-complexity-analysis.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/witness-problem/">The Witness Problem in Multi-Agent Cooperation</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>