<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Rethinking the Diffusion Model from a Langevin Perspective | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Diffusion models are typically introduced through fragmented perspectives involving VAEs, score matching, or flow matching, with dense, technically demanding mathematical derivations. This article presents a fresh Langevin perspective on the core theory of diffusion models, offering a simpler, cleaner, and more intuitive approach to the following questions: 1. Why are diffusion models more than just VAEs? 2. What is the difference between ODE and SDE framework? 3. How can VAE, Score Matching, and Flow Matching be unified under Maximum Likelihood? 4. Why should neural networks model score functions (or their variants), and how can we generalize to discrete diffusion models where score functions don't exist? We demonstrate that the Langevin perspective provides strong pedagogical value for both learners and experienced researchers seeking deeper intuition. "> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/rethinking-diffusion-Langevin/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}details{background:transparent!important;border:1px solid rgba(0,0,0,0.08);border-radius:4px;padding:.75rem 1rem;margin:1rem 0}details>summary{cursor:pointer;font-weight:500;list-style:none;background:transparent!important}details[open]{background:transparent!important}.mermaid,svg.mermaid{background:transparent!important;background-color:transparent!important}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Rethinking the Diffusion Model from a Langevin Perspective",
            "description": "Diffusion models are typically introduced through fragmented perspectives involving VAEs, score matching, or flow matching, with dense, technically demanding mathematical derivations. This article presents a fresh Langevin perspective on the core theory of diffusion models, offering a simpler, cleaner, and more intuitive approach to the following questions: 1. Why are diffusion models more than just VAEs? 2. What is the difference between ODE and SDE framework? 3. How can VAE, Score Matching, and Flow Matching be unified under Maximum Likelihood? 4. Why should neural networks model score functions (or their variants), and how can we generalize to discrete diffusion models where score functions don't exist? We demonstrate that the Langevin perspective provides strong pedagogical value for both learners and experienced researchers seeking deeper intuition.
",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Yuan Lan",
                "authorURL": "https://en.wikipedia.org/wiki/Albert_Einstein",
                "affiliations": [
                  {
                    "name": "Theory Lab, Huawei Technology Limited",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Rethinking the Diffusion Model from a Langevin Perspective</h1> <p>Diffusion models are typically introduced through fragmented perspectives involving VAEs, score matching, or flow matching, with dense, technically demanding mathematical derivations. This article presents a fresh Langevin perspective on the core theory of diffusion models, offering a simpler, cleaner, and more intuitive approach to the following questions: 1. Why are diffusion models more than just VAEs? 2. What is the difference between ODE and SDE framework? 3. How can VAE, Score Matching, and Flow Matching be unified under Maximum Likelihood? 4. Why should neural networks model score functions (or their variants), and how can we generalize to discrete diffusion models where score functions don't exist? We demonstrate that the Langevin perspective provides strong pedagogical value for both learners and experienced researchers seeking deeper intuition. </p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#equations">Equations</a> </div> <div> <a href="#images-and-figures">Images and Figures</a> </div> <ul> <li> <a href="#interactive-figures">Interactive Figures</a> </li> </ul> <div> <a href="#citations">Citations</a> </div> <div> <a href="#footnotes">Footnotes</a> </div> <div> <a href="#code-blocks">Code Blocks</a> </div> <div> <a href="#diagrams">Diagrams</a> </div> <div> <a href="#tweets">Tweets</a> </div> <div> <a href="#layouts">Layouts</a> </div> <div> <a href="#other-typography">Other Typography?</a> </div> </nav> </d-contents> <p>Modern diffusion models are built upon two fundamental processes: the forward process, which gradually corrupts data with noise during training, and the backward process, which generates data by sampling from noise. The development of diffusion models has diverged into several branches, resulting in different perspectives on these processes. Most interpretations fall into three main frameworks: the Variational Autoencoder (VAE) perspective, the score-based perspective, and the flow-based perspective. Although there are many tutorials available, learning the core theory of diffusion models remains challenging due to mathematically dense derivations and fragmented intuitions scattered across these different approaches.</p> <p>The VAE perspective interprets the forward and backward diffusion processes as an encoder and decoder, respectively, and using the Evidence Lower Bound (ELBO) as the training objective <d-cite key="Luo2022UnderstandingDM"></d-cite><d-cite key="Ho2020DenoisingDP"></d-cite>. While this approach is intuitive for much of the machine learning community, it involves dense mathematical derivations of the backward processes and often blurs a crucial intuition: in VAEs, encoder and decoder are approximations of a prior-posterior pair, whereas in diffusion models, the prior and posterior are an exact prior-posterior pair.</p> <p>The score-based perspective <d-cite key="Song2020ScoreBasedGM"></d-cite> usually starts from the mathematical exactness of the forward and backward process pair from the point of view of stochastic equations. It typically introduces the forward process first and treats the backward process as an oracle, often citing Anderson (1982) <d-cite key="Anderson1982ReversetimeDE"></d-cite>. However, fully grasping the derivation of the backward process requires familiarity with advanced mathematical concepts like the Kolmogorov equations and the continuity equation, making this approach less accessible. Moreover, the choice of score matching objective is often treated as a given rather than derived from first principles, which obscures its connection to maximum likelihood.</p> <p>A third valuable perspective is the flow-based viewpoint <d-cite key="liu2022flow"></d-cite>, which has become increasingly popular in modern diffusion models. This approach is theoretically equivalent to both the VAE and score-based frameworks, but it distinguishes itself by emphasizing an intuitive and visually accessible straight-line interpolation between data and noise. While this simplicity makes the flow-based perspective appealing and approachable, it also carries the risk of oversimplification, potentially overlooking the intricate pairing between the forward and backward processes that underpin the theoretical foundation of diffusion models.</p> <p>In this article, we aims to offer a perspective that is both mathematically simple and nuanced: the Langevin perspective. This approach maintains a emphasis on the exactness of the forward and backward processes, while relying only on fundamental techinques of stochastic differential equations (SDEs). The central insight is encapsulated in the following triangle relationship:</p> <div class="row mt-3"> <div class="col-md-12 col-lg-10 offset-lg-1 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-backward-langevin-480.webp 480w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-backward-langevin-800.webp 800w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-backward-langevin-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-backward-langevin.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>which illustrates the connection among the forward, backward diffusion process and the Langevin dynamics.</p> <h1 id="langevin-dynamics-as-identity-operation">Langevin Dynamics as ‘Identity’ Operation</h1> <p><strong>Langevin Dynamics</strong> is a special diffusion process that can generate samples from a probability distribution \(p(\mathbf{x})\). It is defined as:</p> \[d\mathbf{x}_t = g\, \mathbf{s}(\mathbf{x}_t) dt + \sqrt{2 g}\, d\mathbf{W}_t,\] <p>where \(\mathbf{s}(\mathbf{x}) = \nabla_{\mathbf{x}} \log p(\mathbf{x})\) is the score function of \(p(\mathbf{x})\), $g$ is an arbitrary positive constant. The $d\mathbf{W}$ term is a Brownian noise what can be treated as $\sqrt{dt} \boldsymbol{\epsilon}$, where $\boldsymbol{\epsilon}$ is a standard Gaussian noise. This dynamics is often used as a Monte Carlo sampler to draw samples from \(p(\mathbf{x})\), since \(p(\mathbf{x})\) is its stationary distribution—the distribution that \(\mathbf{x}_t\) converges to and and remains at as \(t \to \infty\), regardless of the initial distribution of \(\mathbf{x}_0\).</p> <details> <summary><em>If you accept that $p(\mathbf{x})$ is the stationary distribution of Langevin dynamics, skip this. Otherwise, see the short argument below:</em> (click to expand)</summary> <ol> <li> <p>Set $g = 1$ by rescaling time as $t’ = g t$: under this change of variables, $d\mathbf{x}_t = g\,\mathbf{s}(\mathbf{x}_t)dt + \sqrt{2g}\,d\mathbf{W}_t$ becomes the same equation with $g=1$, so $g$ only fixes the time unit and does not change the stationary distribution.</p> </li> <li> <p>Write the dynamics in “energy” form as \(d\mathbf{x}_t = -\nabla E(\mathbf{x})\,dt + \sqrt{2}\,d\mathbf{W}_t\). The random term $d\mathbf{W}_t$ perturbs the system toward equilibrium, where states with the same energy $E(\mathbf{x})$ have equal probability. Thus, the stationary distribution is $p(\mathbf{x}) = f(E(\mathbf{x}))$ for some function $f$.</p> </li> <li> <p>Consider $N$ independent copies $\mathbf{x}_1, \dots, \mathbf{x}_N$. Their joint density is the product $p(\mathbf{x}_1) \cdots p(\mathbf{x}_N)$. Treating them as a single system, the total energy is additive: $E(\mathbf{x}_1, \dots, \mathbf{x}_N) = \sum E(\mathbf{x}_i)$. So the joint stationary density must also be $g(\sum E(\mathbf{x}_i))$ for some function $g$. The only function satisfying both the product (independence) and sum (additivity) forms for all $N$ is the exponential: $f(E) = e^{-\beta E}$, yielding $p(\mathbf{x}) \propto e^{-\beta E(\mathbf{x})}$.</p> </li> <li> <p>To find $\beta$, take $E(\mathbf{x}) = \frac{1}{2} |\mathbf{x}|^2$, giving the Ornstein–Uhlenbeck process $d\mathbf{x}_t = -\mathbf{x}\,dt + \sqrt{2}\,d\mathbf{W}_t$ with known stationary $\mathcal{N}(0, I)$, density $\propto e^{-\frac{1}{2} |\mathbf{x}|^2}$. Matching forms gives $\beta = 1$.</p> </li> </ol> <p>Thus, the dynamics \(d\mathbf{x}_t = -\nabla E(\mathbf{x})\,dt + \sqrt{2}\,d\mathbf{W}_t\) has stationary distribution \(\propto e^{-E(\mathbf{x})}\), and \(d\mathbf{x}_t = \nabla_{\mathbf{x}} \log p(\mathbf{x}) \, dt + \sqrt{2} \, d\mathbf{W}_t\) has stationary distribution $p(\mathbf{x})$.</p> </details> <p>Langevin dynamics, while widely used for sampling from complex distributions, becomes inefficient in high-dimensional or multi-modal settings due to slow mixing and sensitivity to hyperparameters such as step size and noise scale. However, Langevin dynamics play a crucial role as the foundation of diffusion models due to an important property: for \(p(\mathbf{x})\), Langevin dynamics act as an “identity” operation on the distribution, transforming a sample from \(p(\mathbf{x})\) into a new, independent sample from the same distribution.</p> <div class="row mt-3"> <div class="col-md-10 offset-md-1 col-lg-8 offset-lg-2 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/langevin_id-480.webp 480w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/langevin_id-800.webp 800w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/langevin_id-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/langevin_id.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Langevin dynamics acts as an identity operation on $p(\mathbf{x})$: starting from a sample $\mathbf{x}_0 \sim p(\mathbf{x})$, it produces a new, independent sample from the same distribution. </div> <h1 id="spliting-the-identity-forward-and-backward-processes-in-diffusion-models">Spliting the Identity: Forward and Backward Processes in diffusion models</h1> <p>One key reason Langevin dynamics struggles in high-dimensional settings is the challenge of initialization. The score function must be learned from real data, so it is only accurate near true data points and poorly estimated elsewhere. However, in generative modeling, we want to generate entirely new data samples—which means we need to start Langevin dynamics from points that may not resemble real data at all. Finding a good initialization that is close to the true data manifold is difficult, making effective generation with Langevin dynamics hard in practice.</p> <p>An enhancement to Langevin dynamics is the Annealed Langevin dynamics <d-cite key="song2019generative"></d-cite>. Instead of using a single Langevin sampler, this method involves training a sequence of Langevin dynamics, each corresponding to a different level of noise added to the data. Starting from pure noise, the method gradually reduces the noise level, switching between these samplers at each step. In this way, samples are progressively transformed from random noise into data-like samples, using Langevin dynamics that are effective for each stage of noise contamination. This approach highlights the importance of using multiple noise levels</p> <p>Diffusion models take this concept a step further by completely separating the training and inference processes: one process trains the model at different noise levels, while another process samples from noise to generate data.</p> <h3 id="the-forward-diffusion-process-for-training">The Forward Diffusion Process for training</h3> <p>The forward diffusion process in DDPM generates the necessary training data: clean images and their progressively noised counterparts. In continuous time, a very general way to describe such a process is by an Itô SDE of the form</p> \[d \mathbf{x}_t = f(\mathbf{x}_t, t)\, dt + g(t)\, d\mathbf{W}_t, \label{Forward Process}\] <p>where \(t \in [0,T]\) is the forward diffusion time, \(\mathbf{x}_t\) is the noise-contaminated image at time \(t\), \(\mathbf{W}_t\) is a Brownian motion, \(f(\mathbf{x}_t, t)\) is the drift, and \(g(t)\) scales the injected noise. Different choices of \(f\) and \(g\) correspond to different forward-diffusion parameterizations used in diffusion models.</p> <p>In practice, diffusion models are usually instantiated by choosing specific parameterizations of this SDE. The most common ones are the <strong>variance-preserving (VP)</strong> process, implemented in DDPMs as an Ornstein–Uhlenbeck dynamics that gently pulls samples toward the origin while injecting noise so that the marginal converges to a standard Gaussian; the <strong>variance-exploding (VE)</strong> process, where there is no restoring drift and the noise scale grows with time so that the variance “explodes”; and <strong>flow-matching</strong> formulations, which view generation as following a time-dependent flow that implements an “straight line” interpolation between data and noise under a carefully designed schedule.</p> <p>The table below summarizes these three forward processes in terms of their variables, noise-level schedules, closed-form noising relations, and SDEs (with \(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, I)\)):</p> <table> <thead> <tr> <th><strong>Name</strong></th> <th><strong>Variable notation</strong></th> <th><strong>Noise-level parameter</strong></th> <th><strong>Relation between initial and noisy variable</strong></th> <th><strong>Forward SDE (in time \(t\))</strong></th> </tr> </thead> <tbody> <tr> <td>Variance-preserving (VP)</td> <td>\(x_t\)</td> <td>\(\alpha_t = e^{-t}\)</td> <td>\(x_t = \sqrt{\alpha_t}\, x_0 + \sqrt{1-\alpha_t}\, \boldsymbol{\epsilon}\)</td> <td>\(d x_t = - \tfrac{1}{2} x_t\, dt + dW_t\)</td> </tr> <tr> <td>Variance-exploding (VE)</td> <td>\(z_t = x_t e^{\frac{t}{2}}\)</td> <td>\(\sigma_t = \sqrt{e^{t} - 1}\)</td> <td>\(z_t = z_0 + \sigma_t\, \boldsymbol{\epsilon}\)</td> <td>\(d z_t = e^{t/2}\, dW_t\)</td> </tr> <tr> <td>Flow</td> <td>\(r_t = x_t \frac{e^{\frac{t}{2}}}{1 + \sqrt{e^{t} - 1}}\)</td> <td>\(s_t = \dfrac{\sqrt{e^{t} - 1}}{1 + \sqrt{e^{t} - 1}}\)</td> <td>\(r_t = (1-s_t)\, r_0 + s_t\, \boldsymbol{\epsilon}\)</td> <td>\(\begin{aligned} d r_t &amp;= -\dfrac{r_t\, e^{t}}{2 \left( e^{t} - 1 + \sqrt{e^{t} - 1} \right)} \, dt \\\\ &amp;\quad + \dfrac{e^{t/2}}{1 + \sqrt{e^{t} - 1}} \, dW_t \end{aligned}\)</td> </tr> </tbody> </table> <p>No matter which notation we choose, A forward diffusion step with a step size of \(\Delta t\) acts as adding more noise to data, which is displayed in the following picture:</p> <div class="row mt-3"> <div class="col-md-10 offset-md-1 col-lg-8 offset-lg-2 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-480.webp 480w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-800.webp 800w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-mermaid">flowchart LR
    A["x_t ~ p_t(x)"] --&gt;|Forward| B["x_{t+Δt} ~ p_{t+Δt}(x)"]
</code></pre> <h3 id="the-backward-diffusion-process">The Backward Diffusion Process</h3> <p>The backward diffusion process is the conjugate of the forward process. While the forward process evolves \(p_t(\mathbf{x})\) toward \(\mathcal{N}(\mathbf{0},I)\), the backward process reverses this evolution, restoring \(\mathcal{N}(\mathbf{0},I)\) to \(p_t\).</p> <p>To derive it, we employ Langevin dynamics as a stepping stone, which provides a starightforward way to obtain the backward diffusion process:</p> <p>\(\ref{Langevin Dynamics}\) functions as an “identity” operation with respect to a distribution. Given that the backward process is the reverse of the forward process, the composition of the forward and backward process at time \(t\) must therefore reproduce the Langevin dynamics for \(p_t(\mathbf{x})\), as shown in the following picture</p> <div class="row mt-3"> <div class="col-md-12 col-lg-10 offset-lg-1 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-backward-langevin-480.webp 480w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-backward-langevin-800.webp 800w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-backward-langevin-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/forward-backward-langevin.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>To formalize this, consider the Langevin dynamics for \(p_t(\mathbf{x})\) with a distinct time variable \(\tau\), distinguished from the forward diffusion time \(t\). This dynamics can be decomposed into forward and backward components as follows:</p> \[\begin{split} d\mathbf{x}_\tau &amp;= \mathbf{s}(\mathbf{x}_\tau, t) d\tau + \sqrt{2}\, d\mathbf{W}_\tau, \\ &amp;= \underbrace{-\frac{1}{2} \mathbf{x}_\tau d\tau + d\mathbf{W}_\tau^{(1)}}_{\text{Forward}} + \underbrace{ \left( \frac{1}{2} \mathbf{x}_\tau + \mathbf{s}(\mathbf{x}_\tau, t) \right)d\tau + d\mathbf{W}_\tau^{(2)}}_{\text{Backward} }, \end{split}\] <p>where \(\mathbf{s}(\mathbf{x}, t) = \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\) is the score function of \(p_t(\mathbf{x})\). We have utilized the property that \(\sqrt{2}\, d\mathbf{W}_\tau = \sqrt{2 dt} \boldsymbol{\epsilon} = \sqrt{dt} \boldsymbol{\epsilon}_1 + \sqrt{dt} \boldsymbol{\epsilon}_2 = d\mathbf{W}_\tau^{(1)} + d\mathbf{W}_\tau^{(2)}\).</p> <p>The “Forward” part in this decomposition corresponds to the forward diffusion process, effectively <strong>increasing the forward diffusion time \(t\) by \(d\tau\)</strong>, bringing the distribution to \(p_{t + d\tau}(\mathbf{x})\). Since the forward and backward components combine to form an “identity” operation, the “Backward” part must reverse the forward process—<strong>decreasing the forward diffusion time \(t\) by \(d\tau\)</strong> and restoring the distribution back to \(p_t(\mathbf{x})\).</p> <p>Now we can define the backward process according to the backward part in the equation above, and a backward diffusion time \(t'\) different from the forward diffusion time \(t\):</p> \[d\mathbf{x}_{t'} = \left( \frac{1}{2} \mathbf{x}_{t'}+ \mathbf{s}(\mathbf{x}_{t'}, t) \right) dt' + d\mathbf{W}_{t'}.\] <p>One step of this backward diffusion process with \(dt' = \Delta t\) acts as a reversal of the forward process.</p> <div class="row mt-3"> <div class="col-md-10 offset-md-1 col-lg-8 offset-lg-2 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/backward2-480.webp 480w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/backward2-800.webp 800w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/backward2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/backward2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The backward diffusion process itself is also a standalone SDE that advances the backward diffusion time \(t'\). If \(\mathbf{x}_{t'} \sim q_{t'}(\mathbf{x})\), then one step of the backward diffusion process with \(dt' = \Delta t'\) brings it to \(\mathbf{x}_{t' + \Delta t'} \sim q_{t' + \Delta t'}(\mathbf{x})\).</p> <div class="row mt-3"> <div class="col-md-10 offset-md-1 col-lg-8 offset-lg-2 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/backward3-480.webp 480w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/backward3-800.webp 800w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/backward3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/backward3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>These two interpretations help us determine the relationship between the forward diffusion time \(t\) and the backward diffusion time \(t'\). Since \(dt'\) is interpreted as a “decrease” in the forward diffusion time \(t\), as well as a “increase” of the backward diffusion time \(t'\), we have</p> \[dt = -dt'\] <p>which means the backward diffusion time is the inverse of the forward. To make \(t'\) lies in the same range \([0, T]\) of the forward diffusion time, we define \(t = T - t'\). In this notation, the backward diffusion process [^Anderson1982ReversetimeDE] is</p> \[d\mathbf{x}_{t'} = \left( \frac{1}{2} \mathbf{x}_{t'}+ \mathbf{s}(\mathbf{x}_{t'}, T-t') \right) dt' + d\mathbf{W}_{t'}, \label{Backward Process}\] <p>in which \(t' \in [0,T]\) is the backward time, \(\mathbf{s}(\mathbf{x}, t) = \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\) is the score function of the density of \(\mathbf{x}_{t}\) in the forward process.</p> <h3 id="forward-backward-duality">Forward-Backward Duality</h3> <p>We have previously shown that a backward step is the reverse of a forward step: advancing time \(t'\) in the backward process corresponds to receding time \(t\) by the same amount in the forward process. What then occurs when we chain together a series of forward and backward steps? Consider the following process: start with \(\mathbf{x}_0\), evolve it via the \(\ref{Forward Process}\) to \(\mathbf{x}_T\), then take \(\mathbf{x}_T\) as the initial position \(\mathbf{x}_{0'}\) of the \(\ref{Backward Process}\) and evolve it to \(\mathbf{x}_{T'}\). This sequence is illustrated in the figure below.</p> <div class="row mt-3"> <div class="col-md-10 offset-md-1 col-lg-8 offset-lg-2 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/FastestDiffusionTheory_08-480.webp 480w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/FastestDiffusionTheory_08-800.webp 800w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/FastestDiffusionTheory_08-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/FastestDiffusionTheory_08.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The green arrows represent consecutive forward process steps that advance the forward diffusion time \(t\), while the blue arrows indicate consecutive backward process steps that advance the backward diffusion time \(t'\).</p> <p>We examine the relationship between \(\mathbf{x}_{t}\) in the forward diffusion process and \(\mathbf{x}_{t'=T-t}\) in the backward diffusion process. The composition of a forward and a backward step constitutes a Langevin dynamics step. This allows us to connect \(\mathbf{x}\) in the forward process with those in the backward process through Langevin dynamics steps, as illustrated below:</p> <div class="row mt-3"> <div class="col-md-10 offset-md-1 col-lg-8 offset-lg-2 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/FastestDiffusionTheory_09-480.webp 480w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/FastestDiffusionTheory_09-800.webp 800w,/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/FastestDiffusionTheory_09-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-rethinking-diffusion-Langevin/FastestDiffusionTheory_09.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Each horizontal row in this picture corresponds to consecutive steps of Langevin dynamics, which alters the samples while maintaining the same probability density</strong>. This illustrates the duality between the forward and backward diffusion processes: while \(\mathbf{x}_t\) (forward) and \(\mathbf{x}_{(T-t)'}\) (backward) are distinct samples, they obey the same probability distribution.</p> <ul> <li>It’s important to note that the backward diffusion process does not generate identical samples to the forward process; rather, it produces samples according to the same probability distribution, due to the identity property of Langevin dynamics.</li> </ul> <p>To formalize the duality, we define the densities of \(\mathbf{x}_t\) (forward) as \(p_t(\mathbf{x})\), the densities of \(\mathbf{x}_{t'}\) (backward) as \(q_{t'}(\mathbf{x})\). If we initialize</p> \[q_0(\mathbf{x}) = p_T(\mathbf{x}),\] <p>then their evolution are related by</p> \[q_{t'}(\mathbf{x}) = p_{T-t'}(\mathbf{x})\] <table> <tbody> <tr> <td>For large $T$, $p_T(\mathbf{x})$ converges to $$\mathcal{N}(\mathbf{x}</td> <td>\mathbf{0},I)\(. Thus, the backward process starts at $t'=0$ with\)\mathcal{N}(\mathbf{0},I)$$ and, after evolving to $t’=T$, generates samples from the data distribution:</td> </tr> </tbody> </table> \[q_T(\mathbf{x}) = p_0(\mathbf{x}) \quad \text{(data distribution)}.\] <p>This establishes an exact correspondence between the forward diffusion process and the backward diffusion process, indicating that the backward diffusion process can generate image data from pure Gaussian noise.</p> <p>We demonstrated that <strong>backward diffusion</strong>—the dual of the forward process—can generate image data from noise. However, this requires access to the <strong>score function</strong> $\mathbf{s}(\mathbf{x}, t) = \nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ at every timestep $t$. In practice, we approximate this function using a neural network. In the next section, we will explain how to train such score networks.</p> <h2 id="training-the-diffusion-model">Training the Diffusion Model</h2> <p>Previous section introduced <strong>Forward Process</strong> and <strong>Backward Process</strong> of Denoising Diffusion Probabilistic Model (DDPM).</p> <p><strong>Forward Process</strong></p> \[d \mathbf{x}_t = - \frac{1}{2} \mathbf{x}_t dt + d\mathbf{W}_t,\] <p>where $t \in [0,T]$ is the forward diffusion time. This process describes a gradual noising operation that transforms clean images into Gaussian noise.</p> <p><strong>Backward Process</strong></p> \[d\mathbf{x}_{t'} = \left( \frac{1}{2} \mathbf{x}_{t'}+ \mathbf{s}(\mathbf{x}_{t'}, T-t') \right) dt' + d\mathbf{W}_{t'},\] <p>where $t’ = T - t$ is the backward diffusion time, $\mathbf{s}(\mathbf{x}, t) = \nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ is the score function of the density of $\mathbf{x}_{t}$ in the forward process.</p> <p>Training the backward process raises two key questions: (1) What should we model, and (2) What training objective should we use? It is clear that the central object to model is the <strong>score function</strong>, which we typically represent with a neural network since it is unknown and generally intractable. In practice, there are several parameterizations of the score function—such as $x_0$ prediction, $\epsilon$ prediction, or velocity prediction from a flow-matching perspective—but all these variants are simply rescaled forms of the same underlying score function. However, it is less obvious how the score function should enter the training objective.</p> <p>The variational autoencoder (VAE) perspective addresses the training objective by maximizing the Evidence Lower Bound (ELBO), a principled but approximate surrogate for maximum likelihood. While ELBO provides a systematic way to derive training objectives, this perspective can obscure the fact that diffusion models inherently perform true maximum likelihood estimation, distinguishing them from conventional VAEs and contributing to their empirical success.</p> <p>Alternatively, the score matching perspective motivates the use of <strong>denoised score matching</strong>, a method closely connected to maximum likelihood (see Lyu, S. (2009), <em>Interpretation and generalization of score matching</em>). However, employing the score function as a prior assumption—rather than deriving its necessity from first principles—limits the generality of this approach. Specifically, it becomes difficult to extend score matching to more general or discrete diffusion systems, where the notion of a continuous score function becomes less transparent.</p> <p>The flow matching perspective recasts score matching as “velocity learning” along straight lines, which simplifies the formalism. However, while this approach is easier to understand from first glance, it does not clarify the connection to maximum likelihood.</p> <p>To address these shortcomings, our goal is to derive the training objective directly from first principles, beginning with the maximum likelihood framework itself. By doing so, we reveal the fundamental connection between diffusion model loss and exact maximum likelihood, without presupposing the existence or explicit usage of the score function.</p> <h3 id="from-maximal-likelihood-to-denoising-objective">From Maximal likelihood to Denoising objective</h3> <p>We now look at maximum likelihood training of a diffusion model. Suppose we have two distributions $p(\mathbf{x}, t)$ and $q(\mathbf{x}, t)$ that both evolve under the same forward diffusion process. Think of $p$ as the <strong>true data distribution</strong> pushed forward by the diffusion dynamics, and $q$ as the <strong>model distribution</strong>. At any fixed time $t$, their Kullback–Leibler (KL) divergence is</p> \[\mathrm{KL}\big(p_t \Vert q_t\big) = \int p(\mathbf{x}, t)\,\log \frac{p(\mathbf{x}, t)}{q(\mathbf{x}, t)}\,d\mathbf{x}.\] <p>Maximum likelihood training of $p$ corresponds to minimizing this KL divergence at the data time $t=0$. In a diffusion model, however, we introduce an explicit <strong>forward diffusion time</strong> $t$ and then learn a <strong>backward (reverse-time) process</strong> that maps from noisy states back to data. Intuitively, this suggests that we should “distribute” the KL objective over diffusion time: instead of treating the KL only at $t=0$, we examine how it evolves along the forward process.</p> <p>The way to “distribute” the KL is consdering the <strong>time derivative</strong> of the KL divergence along the forward dynamics, noting that</p> \[\mathrm{KL}\big(p_0 \Vert q_0\big) = -\int_0^{\infty}\frac{d}{dt} \mathrm{KL}\big(p_t \Vert q_t\big) dt\] <p>because $\mathrm{KL}\big(p_\infty \Vert q_\infty\big) = 0$ at infinitely large time $t$ since both $p$ and $q$ converges to the same Gaussian noise distribution.</p> <p>The term \(L_t = -\frac{d}{dt} \mathrm{KL}\big(p_t \Vert q_t\big)\) is exactly the training objective we needed for training the diffusion model.</p> <p>We will show that as long as the forward diffusion process takes the form:</p> \[d\mathbf{x} = f(\mathbf{x}, t) \, dt + g(t) \, d\mathbf{W}.\] <p>the training objective is</p> <p>\(\begin{align*} L_t &amp;= \frac{1}{2} g(t)^2 \int p(\mathbf{x}, t)\, \big\|\nabla \log p(\mathbf{x}, t) - \nabla \log q(\mathbf{x}, t)\big\|^2 d\mathbf{x} \\ &amp;\propto \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x}, t)} \big\|\nabla \log p(\mathbf{x}, t) - \nabla \log q(\mathbf{x}, t)\big\|^2 \end{align*}\) where the score functions \(\nabla \log p(\mathbf{x}, t)\) and \(\nabla \log q(\mathbf{x}, t)\) appears naturally inside the L2 objective.</p> <p>In practice, we model the \(\nabla \log q(\mathbf{x}, t)\) (or its rescaled version) as a neural network $\mathbf{s}_\theta(\mathbf{x}, t)$. The only thing remains to handle is the score of the true data distribution \(\nabla \log p(\mathbf{x}, t)\), which should be approximated by an empirical value from samples since we don’t know its vallue. In fact, we have</p> <p>\(\text{argmin}_{\mathbf{s}_\theta} \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x}, 0)} \big\|\nabla \log p(\mathbf{x}_t | \mathbf{x}_0) - \mathbf{s}_\theta\big\|^2 = \text{argmin}_{\mathbf{s}_\theta} \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x}, t)} \big\|\nabla \log p(\mathbf{x}, t) - \mathbf{s}_\theta\big\|^2\) where the LHS is the denoising score matching (DSM) loss while RHS is the score matching loss.</p> <details> <summary><em>Proof</em> (click to expand)</summary> <p>Let us write the <em>denoising score matching</em> (DSM) loss at time $t$ as \(L_{\text{DSM}}(\mathbf{s}_\theta) := \mathbb{E}_{\mathbf{x}_0 \sim p_0}\, \mathbb{E}_{\mathbf{x}_t \sim p_t(\cdot \mid \mathbf{x}_0)} \big\| \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t \mid \mathbf{x}_0) - \mathbf{s}_\theta(\mathbf{x}_t, t) \big\|^2,\) and the <em>score matching</em> (SM) loss on the marginal $p_t(\mathbf{x}_t)$ as \(L_{\text{SM}}(\mathbf{s}_\theta) := \mathbb{E}_{\mathbf{x}_t \sim p_t} \big\| \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t) - \mathbf{s}_\theta(\mathbf{x}_t, t) \big\|^2.\) Here $p_t(\mathbf{x}_t) = \int p_t(\mathbf{x}_t \mid \mathbf{x}_0)\,p_0(\mathbf{x}_0)\,d\mathbf{x}_0$ is the marginal of the forward process at time $t$.</p> <p>Define the conditional score \(\mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0) := \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t \mid \mathbf{x}_0),\) and the marginal score \(\mathbf{s}(\mathbf{x}_t, t) := \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t).\) Using $|\mathbf{a}-\mathbf{b}|^2 = |\mathbf{a}|^2 + |\mathbf{b}|^2 - 2\langle \mathbf{a}, \mathbf{b}\rangle$, we can expand both objectives. For DSM, $$ \begin{aligned} L_{\text{DSM}}(\mathbf{s}<em>\theta) &amp;= \mathbb{E}</em>{\mathbf{x}<em>0, \mathbf{x}_t} \big| \mathbf{s}</em>\theta(\mathbf{x}_t, t) \big|^2</p> <ul> <li>2\,\mathbb{E}<em>{\mathbf{x}_0, \mathbf{x}_t} \big\langle \mathbf{s}</em>\theta(\mathbf{x}_t, t), \mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0) \big\rangle <br> &amp;\quad</li> <li>\mathbb{E}<em>{\mathbf{x}_0, \mathbf{x}_t} \big|\mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0)\big|^2, \end{aligned} \(where expectations are taken under the joint $p_0(\mathbf{x}_0)\,p_t(\mathbf{x}_t \mid \mathbf{x}_0)$. Similarly, for SM we have\) \begin{aligned} L</em>{\text{SM}}(\mathbf{s}<em>\theta) &amp;= \mathbb{E}</em>{\mathbf{x}<em>t} \big| \mathbf{s}</em>\theta(\mathbf{x}_t, t) \big|^2</li> <li>2\,\mathbb{E}<em>{\mathbf{x}_t} \big\langle \mathbf{s}</em>\theta(\mathbf{x}_t, t), \mathbf{s}(\mathbf{x}_t, t) \big\rangle <br> &amp;\quad</li> <li>\mathbb{E}_{\mathbf{x}_t} \big|\mathbf{s}(\mathbf{x}_t, t)\big|^2. \end{aligned} $$</li> </ul> <p>The first terms coincide, because the marginal of the joint distribution is exactly $p_t(\mathbf{x}<em>t)$: \(\mathbb{E}_{\mathbf{x}_0, \mathbf{x}_t} \big\|\mathbf{s}_\theta(\mathbf{x}_t, t)\big\|^2 = \int p_t(\mathbf{x}_t) \big\|\mathbf{s}_\theta(\mathbf{x}_t, t)\big\|^2 \,d\mathbf{x}_t = \mathbb{E}_{\mathbf{x}_t} \big\|\mathbf{s}_\theta(\mathbf{x}_t, t)\big\|^2.\) The last terms, $\mathbb{E}</em>{\mathbf{x}<em>0, \mathbf{x}_t}|\mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0)|^2$ and $\mathbb{E}</em>{\mathbf{x}<em>t}|\mathbf{s}(\mathbf{x}_t, t)|^2$, do <strong>not</strong> depend on $\mathbf{s}</em>\theta$ at all, so they can only shift the loss by a constant.</p> <p>The only subtle point is the cross term. Because the inner product is linear, it is enough to prove that, for any (scalar) test function $f(\mathbf{x}<em>t)$, \(\mathbb{E}_{\mathbf{x}_0, \mathbf{x}_t} \big[ f(\mathbf{x}_t)\,\mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0) \big] = \mathbb{E}_{\mathbf{x}_t} \big[ f(\mathbf{x}_t)\,\mathbf{s}(\mathbf{x}_t, t) \big],\) and then apply this to each coordinate of $\mathbf{s}</em>\theta(\mathbf{x}_t, t)$.</p> <p>By definition of the score, \(\mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0) = \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t \mid \mathbf{x}_0) = \frac{\nabla_{\mathbf{x}_t} p_t(\mathbf{x}_t \mid \mathbf{x}_0)} {p_t(\mathbf{x}_t \mid \mathbf{x}_0)}.\) Therefore, \(\begin{aligned} \mathbb{E}_{\mathbf{x}_0, \mathbf{x}_t} \big[ f(\mathbf{x}_t)\,\mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0) \big] &amp;= \iint p_0(\mathbf{x}_0)\, p_t(\mathbf{x}_t \mid \mathbf{x}_0)\, f(\mathbf{x}_t)\, \frac{\nabla_{\mathbf{x}_t} p_t(\mathbf{x}_t \mid \mathbf{x}_0)} {p_t(\mathbf{x}_t \mid \mathbf{x}_0)} \,d\mathbf{x}_t\,d\mathbf{x}_0 \\ &amp;= \iint f(\mathbf{x}_t)\, \nabla_{\mathbf{x}_t} p_t(\mathbf{x}_t \mid \mathbf{x}_0)\, p_0(\mathbf{x}_0)\, \,d\mathbf{x}_t\,d\mathbf{x}_0. \end{aligned}\) Under mild regularity conditions we can interchange the order of integration and differentiation, obtaining \(\begin{aligned} \mathbb{E}_{\mathbf{x}_0, \mathbf{x}_t} \big[ f(\mathbf{x}_t)\,\mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0) \big] &amp;= \int f(\mathbf{x}_t)\, \nabla_{\mathbf{x}_t} \Big( \int p_t(\mathbf{x}_t \mid \mathbf{x}_0)\,p_0(\mathbf{x}_0)\,d\mathbf{x}_0 \Big) \,d\mathbf{x}_t \\ &amp;= \int f(\mathbf{x}_t)\,\nabla_{\mathbf{x}_t} p_t(\mathbf{x}_t)\,d\mathbf{x}_t \\ &amp;= \int p_t(\mathbf{x}_t)\, f(\mathbf{x}_t)\, \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)\,d\mathbf{x}_t \\ &amp;= \mathbb{E}_{\mathbf{x}_t} \big[ f(\mathbf{x}_t)\,\mathbf{s}(\mathbf{x}_t, t) \big]. \end{aligned}\) Taking $f(\mathbf{x}<em>t)$ to be each component of $\mathbf{s}</em>\theta(\mathbf{x}_t, t)$ shows that the DSM and SM cross terms are identical: \(\mathbb{E}_{\mathbf{x}_0, \mathbf{x}_t} \big\langle \mathbf{s}_\theta(\mathbf{x}_t, t), \mathbf{s}(\mathbf{x}_t \mid \mathbf{x}_0) \big\rangle = \mathbb{E}_{\mathbf{x}_t} \big\langle \mathbf{s}_\theta(\mathbf{x}_t, t), \mathbf{s}(\mathbf{x}_t, t) \big\rangle.\)</p> <p>Putting everything together, we have \(L_{\text{DSM}}(\mathbf{s}_\theta) = L_{\text{SM}}(\mathbf{s}_\theta) + C,\) where $C$ is a constant independent of $\mathbf{s}_\theta$. Hence both objectives are minimized by the same function, namely the true marginal score \(\mathbf{s}_\theta^\star(\mathbf{x}_t, t) = \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t).\)</p> </details> <p>To understand how score functions arise in the training objective, we will use the familar maximal likelihood method. However, maximal likelihood is for distributions. Before we proceed, we need to know how the forward diffusion process effects the distribution.</p> <p>While a stochastic differential equation (SDE) describes how individual samples evolve over time, the probability distribution of a collection of samples follows the corresponding Fokker-Planck equation. Consider a general SDE of the form:</p> \[d\mathbf{x} = f(\mathbf{x}, t) \, dt + g(t) \, d\mathbf{W}.\] <p>where $f(\mathbf{x}, t)$ is the drift term governing deterministic dynamics, $g(t)$ controls the diffusion strength, and $d\mathbf{W}$ represents Brownian motion increments. If we start with a collection of samples ${\mathbf{x}_0^{(i)}}$ drawn from an initial distribution $p_0(\mathbf{x})$ and evolve each sample according to this SDE, the resulting samples ${\mathbf{x}_t^{(i)}}$ will be distributed according to a time-evolved density $p_t(\mathbf{x})$. The question is: how does $p_t(\mathbf{x})$ relate to the SDE parameters $f$ and $g$?</p> <p>The answer is given by the <strong>Fokker-Planck equation</strong>, which governs how the probability density \(p(\mathbf{x}, t)\) evolves under an SDE:</p> \[\frac{\partial p}{\partial t} = -\nabla \cdot \left[f(\mathbf{x}, t) p\right] + \frac{1}{2} \nabla^2 \left[ g(t)^2 p \right].\] <p>This partial differential equation reveals how both drift and diffusion shape the distribution. The first term, \(-\nabla \cdot \left[f(\mathbf{x}, t) p\right]\), is the advection term: it transports probability mass along the drift field \(f(\mathbf{x}, t)\), shifting the distribution deterministically. The second term, \(\frac{1}{2} \nabla^2 \left[ g(t)^2 p \right]\), is the diffusion term: it spreads probability mass outward due to stochastic fluctuations, smoothing the distribution over time.</p> <details> <summary><em>A short explaination:</em> (click to expand)</summary> <p>You can find rigorous derivations of the Fokker-Planck equation in many textbooks; here we just sketch an intuitive, 1D argument that explains its form.</p> <ol> <li> <p><strong>Linearity.</strong> Particles following the SDE are independent, so if $p_1(x,t)$ and $p_2(x,t)$ each follow the evolution equation, then any mixture $a p_1 + (1-a)p_2$ should also follow it. This means the PDE for $p(x,t)$ must be linear: \(\frac{\partial p}{\partial t} + L[p] = 0,\) where $L[p]$ is a linear operator such as $a(x,t)p + b(x,t)\partial_x p + c(x,t)\partial_x^2 p$.</p> </li> <li> <p><strong>Conservation of probability.</strong> The density is normalized for all time, $\int_{-\infty}^{\infty} p(x,t)\,dx = 1$, so \(\int_{-\infty}^{\infty} \frac{\partial p}{\partial t}\,dx = 0.\) This holds if we can write $L[p]$ as a spatial derivative of a <strong>probability flux</strong> $J[p]$: \(L[p] = \frac{\partial J[p]}{\partial x},\) because then \(\int_{-\infty}^{\infty} \frac{\partial J[p]}{\partial x}\,dx = J[p]\big|_{-\infty}^{\infty} = 0\) whenever $J[p]$ vanishes at infinity. So we obtain the conservation form \(\frac{\partial p}{\partial t} + \frac{\partial J[p]}{\partial x} = 0.\)</p> </li> <li> <p><strong>Drift term $f(x,t)$.</strong> To determine $J[p]$ related to the drift $f(x,t)$, consider first a simple 1D case with <strong>constant speed</strong> $v$, so $dx = v\,dt$. After time $t$, a particle now at position $x$ must have come from $x - vt$ at time $0$, so \(p(x,t) = p(x - vt, 0).\) A $p(x,t)$ satisfying this relation also satisfy the equation \(\frac{\partial p}{\partial t} + \frac{\partial}{\partial x}\big(v\,p(x,t)\big) = 0.\) Therefore the flux $J_{\text{drift}}[p]$ for constant velocity is $v p$. If the speed depends on position and time, $dx = f(x,t)\,dt$, the drift contributes a flux $J_{\text{drift}}[p] = f(x,t)\,p(x,t)$.</p> </li> <li> <p>The noise term $g d W$: consider a simple case $dx = g dW$. Suppose $x(0) = 0$. At time $t$, the accumulation of $dW$ from time $0$ to time $t$ is a Gaussian noise with variance $t$. Therefore the variance of $x$ is $g^2 t$, whose probability distribution is $p(x, t) = \frac{1}{\sqrt{2\pi g^2 t} } e^{-\frac{x^2}{2 g^2 t}}$. It satisfies the following equation \(\frac{\partial p}{\partial t} -\frac{1}{2} \frac{\partial^2 g^2 p}{\partial x^2} = 0\) Therefore the noise term contributes a flux $J_{\text{noise}}[p] = -\frac{1}{2} \frac{\partial g^2 p}{\partial x}$.</p> </li> </ol> <p>Combine them together we have $J_[p] = f(x,t)\,p(x,t)-\frac{1}{2} \frac{\partial g^2 p}{\partial x}$ hence</p> \[\frac{\partial p}{\partial t} = -\frac{\partial}{\partial \mathbf{x}} \left[f(\mathbf{x}, t) p\right] + \frac{1}{2} g(t)^2 \frac{\partial^2 p}{\partial \mathbf{x}^2}.\] </details> <p>To make this precise, consider the <strong>time derivative</strong> of the KL divergence along the forward dynamics. Assume that both $p$ and $q$ satisfy the same Fokker–Planck equation with drift $f(\mathbf{x}, t)$ and diffusion strength $g(t)$:</p> \[\frac{\partial p}{\partial t} = -\nabla \cdot \big(f p\big) + \frac{1}{2} g(t)^2 \nabla^2 p, \qquad \frac{\partial q}{\partial t} = -\nabla \cdot \big(f q\big) + \frac{1}{2} g(t)^2 \nabla^2 q.\] <p>Writing the KL divergence as</p> \[\mathrm{KL}\big(p_t \Vert q_t\big) = \int p(\mathbf{x}, t) \log \frac{p(\mathbf{x}, t)}{q(\mathbf{x}, t)}\,d\mathbf{x},\] <p>and differentiating under the integral sign, we obtain</p> \[\frac{d}{dt} \mathrm{KL}\big(p_t \Vert q_t\big) = \int \left( \log \frac{p}{q} \right) \partial_t p\, d\mathbf{x} + \int p\,\frac{\partial_t q}{q}\, d\mathbf{x},\] <p>where all functions are evaluated at $(\mathbf{x}, t)$ and the integrals are over $\mathbf{x}$. Using the Fokker–Planck operator $\mathcal{L}$ defined by</p> \[\mathcal{L} u = -\nabla \cdot (f u) + \frac{1}{2} g(t)^2 \nabla^2 u,\] <p>we can rewrite this as</p> \[\frac{d}{dt} \mathrm{KL}\big(p_t \Vert q_t\big) = \int \left( \log \frac{p}{q} \right) \mathcal{L} p\, d\mathbf{x} - \int \frac{p}{q}\,\mathcal{L} q\, d\mathbf{x}.\] <details> <summary><strong>Derivation: how KL changes under the Fokker–Planck equation</strong></summary> Let $r = p/q$. Then $\log (p/q) = \log r$, and we can handle each term by integration by parts (assuming boundary terms vanish, e.g., in $\mathbb{R}^d$ with rapidly decaying densities). For the **drift part** of $\mathcal{L}$, we have $$ \int \log r \, \big[-\nabla \cdot (f p)\big]\, d\mathbf{x} = \int p \, f \cdot \nabla \log r\, d\mathbf{x}, $$ and $$ \int r \, \big[-\nabla \cdot (f q)\big]\, d\mathbf{x} = \int q \, f \cdot \nabla r\, d\mathbf{x}. $$ A direct computation using $r = p/q$ shows that these two contributions cancel **exactly**: $$ p\, f \cdot \nabla \log r - q\, f \cdot \nabla r = 0, $$ so the drift does **not** contribute to the time derivative of KL. The **diffusion part** is responsible for KL contraction. Using $\nabla p = p \nabla \log p$, $\nabla q = q \nabla \log q$, and $$ \nabla r = \nabla\left(\frac{p}{q}\right) = p \left(\nabla \log p - \nabla \log q\right), $$ one finds $$ \int \log r \cdot \frac{1}{2} g(t)^2 \nabla^2 p\, d\mathbf{x} = -\frac{1}{2} g(t)^2 \int \nabla \log r \cdot \nabla p\, d\mathbf{x}, $$ and $$ \int r \cdot \frac{1}{2} g(t)^2 \nabla^2 q\, d\mathbf{x} = -\frac{1}{2} g(t)^2 \int \nabla r \cdot \nabla q\, d\mathbf{x}. $$ Substituting the expressions above and simplifying, $$ \nabla \log r \cdot \nabla p = p \left( \nabla \log p - \nabla \log q \right) \cdot \nabla \log p, $$ $$ \nabla r \cdot \nabla q = p \left( \nabla \log p - \nabla \log q \right) \cdot \nabla \log q, $$ so that the diffusion contribution to $\frac{d}{dt} \mathrm{KL}$ becomes $$ -\frac{1}{2} g(t)^2 \int \nabla \log r \cdot \nabla p\, d\mathbf{x} +\frac{1}{2} g(t)^2 \int \nabla r \cdot \nabla q\, d\mathbf{x} = -\frac{1}{2} g(t)^2 \int p(\mathbf{x}, t) \big\|\nabla \log p - \nabla \log q\big\|^2 \, d\mathbf{x}. $$ </details> <p>Putting everything together, we obtain the key identity</p> \[\frac{d}{dt} \mathrm{KL}\big(p_t \Vert q_t\big) = -\frac{1}{2} g(t)^2 \int p(\mathbf{x}, t)\, \big\|\nabla \log p(\mathbf{x}, t) - \nabla \log q(\mathbf{x}, t)\big\|^2 d\mathbf{x} \;\leq\; 0.\] <p>Thus, along the forward diffusion process, the KL divergence between any two solutions of the same Fokker–Planck equation is <strong>non-increasing</strong>: diffusion strictly contracts KL (unless $p_t = q_t$ in the sense that $\nabla \log p = \nabla \log q$ almost everywhere). Drift does not affect this contraction; it only transports mass without changing the “distance” between $p_t$ and $q_t$ in KL. This monotone decrease of $\mathrm{KL}(p_t \Vert q_t)$ over $t$ underlies many stability results for stochastic dynamics, and in diffusion models it justifies decomposing the global maximum-likelihood objective into local-in-time terms associated with each diffusion step.</p> <p>DDPM is trained to removes the noise $\bar{\boldsymbol{\epsilon}}<em>i$ from $\mathbf{x}_i$ in the forward diffusion process, by training a denoising neural network $\boldsymbol{\epsilon}</em>\theta( \mathbf{x}, t_i )$ to predict and remove the noise $\bar{\boldsymbol{\epsilon}}_i $. This means that DDPM minimizes the <strong>denoising objective</strong> [^Ho2020DenoisingDP]:</p> <p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/" rel="external nofollow noopener" target="_blank">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html" rel="external nofollow noopener" target="_blank">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php" rel="external nofollow noopener" target="_blank">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br> code code code <br> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank" rel="external nofollow noopener">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank" rel="external nofollow noopener">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4" rel="external nofollow noopener" target="_blank">http://t.co/m4EIQPM9h4</a></p>— RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">October 5, 2014</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"> <a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin" rel="external nofollow noopener" target="_blank">jekyll-twitter-plugin</a></p> <hr> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com" rel="external nofollow noopener" target="_blank">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage" rel="external nofollow noopener" target="_blank">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org" rel="external nofollow noopener" target="_blank">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org" rel="external nofollow noopener" target="_blank">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com" rel="external nofollow noopener" target="_blank">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com" rel="external nofollow noopener" target="_blank">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">\(1600\)</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">\(12\)</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">\(1\)</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-rethinking-diffusion-Langevin.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">Symbolism Outside, Connectionism Inside: The Trend of Fusing LLMs and Automatic Programs with Symbolic Intermediate Representations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/sac-massive-sim/">Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/nlp-for-human-sciences/">Language as a Window Into the Mind: How NLP and LLMs Advance Human Sciences</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/model-misspecification-in-sbi/">Model Misspecification in Simulation-Based Inference - Recent Advances and Open Challenges</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>