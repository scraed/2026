<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Attention Sinks from the Graph Perspective | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We explore attention sinks in decoder-only transformers through the lens of message passing on graphs, revealing an intrinsic structural bias toward early tokens that may explain this phenomenon."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/attention-sinks-graph-perspective/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Attention Sinks from the Graph Perspective",
            "description": "We explore attention sinks in decoder-only transformers through the lens of message passing on graphs, revealing an intrinsic structural bias toward early tokens that may explain this phenomenon.",
            "published": "April 28, 2026",
            "authors": [
              
              {
                "author": "anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Attention Sinks from the Graph Perspective</h1> <p>We explore attention sinks in decoder-only transformers through the lens of message passing on graphs, revealing an intrinsic structural bias toward early tokens that may explain this phenomenon.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#attention-as-message-passing">Attention as Message-Passing</a> </div> <div> <a href="#causal-transformers-and-attention-sinks">Causal Transformers and Attention Sinks</a> </div> <div> <a href="#wrapping-up">Wrapping Up</a> </div> <div> <a href="#acknowledgements">Acknowledgements</a> </div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/hero.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Attention sinks have recently come back to the forefront of architecture discussion, especially due to their appearance in <a href="https://github.com/openai/gpt-oss" rel="external nofollow noopener" target="_blank">gpt-oss</a> (although in a different form than the effect we’re discussing today).</p> <p>As a mechanism, attention sinks are easy to describe: when trained, decoder-only transformer models tend to allocate a disproportionate amount of attention to the first few tokens, and especially to the first.</p> <p>This effect is well studied in its practical terms, and is often attributed to the model “offloading” probability mass to the early tokens to avoid their spurious allocation elsewhere. Recent works, like Softpick <d-cite key="softpick2025"></d-cite>, provide architectural choices that prevent sinks from forming. While this explanation may sound convincing at a first glance, my intuition is still bothered by it: what do you mean the model “offloads”? Of course it doesn’t explore that possibility intentionally, there must be some mechanism by which the attention sinks are either advantageous or a result of an intrinsic bias to the model. In this blogpost, we will argue that there is a significant bias in decoder-only transformers that may be to blame, at least partially, for this phenomenon. Moreover, this will also allow us to introduce a series of blogposts focused on analyzing transformers from the lens of message passing on graphs.</p> <h2 id="attention-as-message-passing">Attention as Message-Passing</h2> <p>Recent work by Chaitanya K. Joshi <d-cite key="joshi2025"></d-cite> has finally freed us from having to formalize independently a well known property of Transformers (and especially of attention layers): them being a special case of Graph Neural Networks (just like pretty much anything else, to be fair).</p> <p>As a setting to our discussion, though, we will go over another angle with which attention can be seen as message-passing on a graph.</p> <p>Most people are usually introduced to (multi-headed) self-attention directly via the original Transformer paper <d-cite key="vaswani2017attention"></d-cite>. Despite this being generally a good practice in my opinion, it generally directs attention to being interpreted as the simplest way of making tokens interact in a transformer, or as just a soft version of a dictionary lookup. While neither being wrong, such interpretations often drown out some interesting geometric details that lie in attention itself.</p> <p>Let’s start with regular, multiheaded attention.</p> <p>Say you have $n$ tokens, with an embedding dimension $d$.</p> <p>Let our input tokens be shaped as a matrix $X \in \mathbb{R}^{n \times d}$, we first process $X$ with three different linear projections, namely $W_q$, $W_k$ and $W_v$, and end up with the respective $Q \in \mathbb{R}^{n \times d_q}$, $K \in \mathbb{R}^{n \times d_k}$ and $V \in \mathbb{R}^{n \times d_v}$ matrices.</p> <p>We then perform the well-known attention operation</p> \[\text{attention}(X) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Let’s take a look at $\alpha = QK^T$.</p> <p>If we rewrite it component-wise we get</p> \[\alpha_{ij} = \sum_{l=1}^{d_k} Q_{il}(K^T)_{lj} = \sum_{l=1}^{d_k} Q_{il}K_{jl}\] <p>and if we note that $Q$ and $K$’s rows, respectively $q_i$ and $k_i$, we see that</p> \[\alpha_{ij} = q_i k_j^T = \langle q_i, k_j \rangle\] <p>The attention matrix $\alpha$’s entries are thus simply speaking the euclidean dot product between token embeddings, projected via the query and key matrices.</p> <p>This still falls within the classical presentation of attention, so nothing to see here as of yet.</p> <p>What if we could reinterpret these operations from a more geometric/topological perspective?</p> <p>Let’s take, for example</p> \[X \in \mathbb{R}^{n \times d}\] <p>And let’s treat the rows of $X$ as a <strong>point-cloud</strong>:</p> \[X = \begin{bmatrix} x_1^{\top}\\ x_2^{\top}\\ \vdots\\ x_n^{\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{n \times d_q}\] \[K = X W_k \in \mathbb{R}^{n \times d_q}\] \[V = X W_v \in \mathbb{R}^{n \times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle = q_i k_j^{\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^{\top} \in \mathbb{R}^{n \times n}.\] <p>The intuition is: the more points align in Query-Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp(\alpha_{ij}/\sqrt{d_k})}{\sum_{j'=1}^n \exp(\alpha_{ij'}/\sqrt{d_k})}, \qquad A = \text{softmax}\left(\frac{\alpha}{\sqrt{d_k}}\right)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\text{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong>.</p> <p>If we write it row-wise (hence focusing on each token, or node, at a time), we see that the updated function’s value associated with the node becomes</p> \[\text{attention}(x)_i = \sum_l A_{il} V_l\] <p>But what does multiplying a function defined on a graph by the adjacency mean? Let’s say we have a directed graph $\mathcal{G} = (V, E)$ with adjacency $A$, with a function $f: v \rightarrow \mathbb{R}$ and $v \in V$.</p> <p>Then, the multiplication $y = Af$ can be written, component-wise, as</p> \[y_i = \sum_{j} A_{ij} f_{j}\] <p>Remember that, for an adjacency matrix, elements of column $i$ represent incoming links from other nodes in the graph. This means that $y_i$, or the result of the adjacency-multiplied function $f$, is the weighted average of $f$ over incoming nodes to node $i$, where the weights are decided by the adjacency matrix’ entries. Intuitively, you can think of this process as a sort of <em>diffusion</em>: features are aggregates of their neighbours. This means that, if we start with a rather unequally spatially distributed function (say a very localized highly positive region, and the rest being zero), then nodes on the boundary of the highly positive region would “diffuse” the highly positive values towards neighbouring nodes. Of course the topology of the graph heavily influences the speed of this diffusion. Unsurprisingly, this ties back very well with the actual physical phenomenon of heat diffusion, as we will see in a future blogpost.</p> <h2 id="causal-transformers-and-attention-sinks">Causal Transformers and Attention Sinks</h2> <p>Note that the discussion so far has been agnostic of masking strategies applied to the attention score. While several uses of Transformer models employ attention bidirectionally, LLMs, our Large Model protagonists, are usually causally masking attention to leverage parallelism for their Next Token Prediction task.</p> <p>In our attention mechanism, this is done by substituting our $\alpha$ adjacency matrix with a masked, causal one, in the shape of $\alpha_m = \alpha \odot M$, with $M_{ij} = 1$ if $j \leq i$ and zero otherwise. Note that this gives our attention graph an even more interesting structure: our graph is now, by design, a <strong>Directed Acyclic Graph</strong> (<em>DAG</em>), meaning the graph contains no loops, and its adjacency matrix is nilpotent (meaning there exists $k$ such that $(A^k)_{ij} = 0$, $\forall i,j$).</p> <p>One interesting corollary of this observation is that adjacency-based diffusion over DAGs is bound to accumulate information in sinks, specifically, in the first tokens of a causal model. This can be made explicit by looking at the shape of powers of $A$:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig.1-4: We simulate an attention matrix being composed with itself several times, across three different configurations: bidirectional, masked pre-softmax, softmax. As we can see, the combination of masking (making the matrix nilpotent) and softmax (forcing row-wise mass to sum to one) rapidly recovers the familiar attention pattern we see for attention sinks. </div> <p>These plots (Fig.1-4) show exactly what we expect on a DAG: as we take powers of the (masked) attention matrix $A$ the mass moves “leftward” toward early tokens. In the strictly lower-triangular case (no self-loops) this is a nilpotent operator, so sufficiently high powers collapse entirely into the earliest positions.</p> <p>To connect this with learning dynamics, linearize one residual attention block (one head, for intuition; treat the MLP as a node-wise map) as</p> \[X^{\ell+1} \approx X^{\ell} + A^{\ell} X^{\ell} B^{\ell}, \qquad B^{\ell} = W_v^{\ell} W_o^{\ell}.\] <p>Stacking $L$ such blocks yields an end-to-end map that is a polynomial in the $A^{\ell}$’s:</p> \[X^{L} \approx \left(\prod_{\ell=1}^{L}(I + A^{\ell} B^{\ell})\right) X^{0} = X^{0} + \sum_{\ell} A^{\ell} B^{\ell} X^{0} + \sum_{\ell_2 &gt; \ell_1} A^{\ell_2} B^{\ell_2} A^{\ell_1} B^{\ell_1} X^{0} + \cdots\] <p>When the $A^{\ell}$ are geometrically similar across depth, dominant terms behave like <strong>powers of a causal $A$</strong>. That is the same “multi-hop diffusion” we saw in the previous figures, progressively concentrating influence onto the first columns (early tokens).</p> <p>But if that’s the case during a forward pass, what makes a model exhibit this bias across training, as it’s been noticed in the literature?</p> <p>As it turns out, backprop itself mirrors this geometry. Gradients w.r.t. hidden states propagate with Jacobian transposes along the value path:</p> \[g^{\ell} \approx (I + {B^{\ell+1}}^{\top} {A^{\ell+1}}^{\top}) \cdots (I + {B^{L}}^{\top} {A^{L}}^{\top}) g^{L}.\] <p>Hence token-wise gradients accumulate along <strong>column sums of products of $A$</strong> (or, equivalently, row sums of products of $A^{\top}$). In a causal DAG those column sums are largest for earlier positions, so both activations <strong>and</strong> gradients preferentially route through (and update) paths that point to early tokens.</p> <p>Practically, residual connections make the map a <strong>polynomial</strong> (not a single $A^k$), multi-head mixing and $B^{\ell}$ projections reshape directions, and layer-norm rescales signals. But the structural bias remains: deeper layers inherit updates that look like compositions of attention-diffusion steps, which, under causal masking, tend to be more and more “first-column concentrated”.</p> <p>Another corollary of our observation is that it would suggest that later layers are more subject to the attention sink phenomenon, while the very first layer should be much less impacted. This turns out to be true and well known when studying attention sinks, as is the case, for example, for LLaMA-2 <d-cite key="xiao2023streamingllm"></d-cite>, or in Sun et al. <d-cite key="sun2024massive"></d-cite> and Cancedda et al. <d-cite key="cancedda-2024-spectral"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Attention patterns across layers showing the accumulation effect in later layers. </div> <p>Note that, while this <strong>may not be the single effect responsible for attention sinks</strong>, this means we should expect any causal decoder-only transformer to exhibit a bias towards allocating attention to its first few tokens (and increasingly so to the first).</p> <p>This fundamentally clashes with many interpretations of sinks: several works characterize them as a useful feature that is learned by the model. If what we propose is true, it’s exactly the opposite: when sinks <strong>don’t</strong> show up, it means <strong>the message-passing mechanism of your transformer is fundamentally flawed</strong>, and hence it performs worse.</p> <p>The attention sinks become a signal of <strong>healthy communication</strong> of tokens in attention, being a bias that is <strong>intrinsic to the causal, decoder-only transformer</strong>.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>So, to recap, what does this mean? We individuated a possible mechanism that may bias Causal Transformers to accumulate attention on its first few tokens. Note that we showed the mechanism in a highly simplified setting, and are proposing the idea that, despite those simplifications, the underlying effect is still strong enough to accumulate across training steps of a large transformer, and eventually explain the existence of attention sinks as we know them. In the next blogposts, we will use the same graph-centric framing of attention to analyze the problem of long context in transformer models, connecting it to heat diffusion and the oversmoothing and oversquashing phenomena known in the GNN literature. Stay tuned!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-28-attention-sinks-graph-perspective.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/witness-problem/">The Witness Problem in Multi-Agent Cooperation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-ai-evaluations-need-error-bars/">Why AI Evaluations Need Error Bars</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/web-agent/">Computer Use Survey - A Visual Survey of Computer Use Agents</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>