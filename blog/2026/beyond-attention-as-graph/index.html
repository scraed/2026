<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Attention as a Graph | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We extend a graph-based perspective on attention to higher-order topological structures, exploring 2-simplicial attention and its implications for transformer depth and expressivity."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/beyond-attention-as-graph/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Beyond Attention as a Graph",
            "description": "We extend a graph-based perspective on attention to higher-order topological structures, exploring 2-simplicial attention and its implications for transformer depth and expressivity.",
            "published": "April 29, 2026",
            "authors": [
              
              {
                "author": "anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Beyond Attention as a Graph</h1> <p>We extend a graph-based perspective on attention to higher-order topological structures, exploring 2-simplicial attention and its implications for transformer depth and expressivity.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#motivation">Motivation</a> </div> <ul> <li> <a href="#depth-for-transformers">Depth for Transformers</a> </li> </ul> <div> <a href="#what-lies-beyond-graphs">What Lies Beyond Graphs</a> </div> <div> <a href="#2-simplicial-attention">2-Simplicial Attention</a> </div> <ul> <li> <a href="#rediscovering-simplicial-attention-from-the-topological-perspective">Rediscovering Simplicial Attention from the Topological Perspective</a> </li> <li> <a href="#what-does-n-simplicial-attention-mean-for-depth">What Does n-Simplicial Attention Mean for Depth</a> </li> </ul> <div> <a href="#wrapping-up">Wrapping Up</a> </div> <div> <a href="#acknowledgements">Acknowledgements</a> </div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/hero-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/hero-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/hero-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/hero.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Most attention variants have been designed to retain as much sample efficiency as possible, under the constraint of achieving subquadratic scaling with respect to sequence length.</p> <p>While this has clearly been a powerful research direction, recent changes in the pretraining paradigm have directed <em>attention</em> to architectures capable of increasing sample-efficiency.</p> <p>In my previous blogpost I had briefly introduced, as a tool to explain attention sinks, a simple way of viewing attention as a graph operation.</p> <p>We will use this same viewpoint to argue that regular transformers may be <strong>fundamentally limited</strong> in their message-passing capabilities, arguing in favor of higher-order attention methods, such as 2-simplicial Attention <d-cite key="roy2025fastsimplex"></d-cite> and provide a natural way of generalizing it to $n$-simplices, while explaining them from a <strong>topological perspective</strong>.</p> <p>Finally, we will also poke at the very mechanism that makes Machine Learning “deep”: <strong>layer composition</strong>.</p> <h2 id="motivation">Motivation</h2> <p>“Deep Learning” is named after the typical definition of Neural Network models as a set of subsequent, composed <em>Layers</em>.</p> <p>Layers represent atomic, parametric transformations between vector spaces, rendered non-linear by a selection of activation functions.</p> <p>In transformers, layers are organized in transformer blocks, and the two are often used interchangeably. Transformer blocks are nothing more than subsequent attention and MLP transformations operating on the residual stream.</p> <p>Intuitively, depth is easy to justify: while the Universal Approximation Theorem guarantees that a single, infinitely wide non-linear layer can approximate any continuous function arbitrarily well, it doesn’t mean that width scaling is practical.</p> <p>As it turns out, properly approximating functions becomes exponentially hard with respect to the dimension of the spaces the functions map between, which can be seen as another angle of the curse of dimensionality <d-cite key="poggio2017deepnotshallow_journal"></d-cite>.</p> <p>For this reason, it becomes convenient to instead “break down” the approximation problem by composing several parametric layers, one after the other.</p> <p>This allows the model to increase in expressivity without exploding in (latent) dimensionality.</p> <p>As for all worthwhile architectural choices in deep learning, this exposes us to a tradeoff: composing operations sequentially is <em>by definition</em> the least <strong>parallel (and hence fast) architectural choice we can make</strong>.</p> <h3 id="depth-for-transformers">Depth for Transformers</h3> <p>While the previous considerations apply in general for all Neural Network architectures, transformers in particular have their specific drawbacks when scaling depth: Transformers’ success has been greatly propelled by their natural parallelism during Next Token Prediction tasks, and, apart from inevitably increasing latency in both inference and training, depth exposes the network to further instability in gradients, as, depending on normalization, the model risks vanishing or exploding gradients.</p> <p>In sequence modelling, though, one key element justifies depth: attention is an operation that message-passes between pairs of tokens in a graph. This means that individual transformer blocks can only possibly encode interactions between pairs of tokens. <strong>Depth allows information to be passed beyond a single-hop</strong>: if we reframe the $AV$ multiplication as in the attention sinks blogpost (seeing as “diffusion” of $V$ on the graph), we can reconnect this intuition to regular graph theory by noticing how powers of the adjacency matrix of a graph, $A^k$, represent $k$-hop walks from each node, and therefore depth approximates this due to attention’s fully connected, yet sparse, input-dependent adjacency matrix.</p> <p>As a result, depth is a fundamental ingredient in transformers that allows them to effectively message-pass between <em>tuples</em> of tokens, and hence build complex and useful representations of tokens in sequences.</p> <p>But what if there existed a way to message-pass between tuples of tokens without resorting to depth?</p> <h2 id="what-lies-beyond-graphs">What Lies Beyond Graphs</h2> <p>As we know, the message-passing operation happening during attention can be conceptualized as a graph operation. This simple observation, while trivial, has a relevant practical consequence: an entire field of science has, since roughly 2017, been extensively studying Neural Networks as message-passing on graphs, and has developed a variety of theories and techniques to best represent information on topological objects. Of course, the field in question is Geometric Deep Learning, and its central contributions, Graph Neural Networks and Topological Deep Learning.</p> <p>Notably, one key element of that vast literature has been an expressivity bound on GNN architectures: if we define “expressivity” as the capability of distinguishing graphs that are different, then a GNN is only as expressive as the Weisfeiler-Lehman test <d-cite key="huang2022wl"></d-cite> (also referred to as the WL-test). I won’t go in the details of what the test is, and will gladly refer the interested reader to Federico Barbero’s excellent video explaining it.</p> <p>If you don’t have the time, here’s the gist of it: the WL-test is designed to understand when two graphs are isomorphic (the same graph), but it doesn’t always work. It can be shown that a GNN is at most as expressive at graph isomorphism as the WL-test itself <d-cite key="xu2018powerful"></d-cite>.</p> <p>If you’re anything like me, this sounds like bad news: what do you mean we have a theoretically bounded expressivity? Isn’t universal approximation the reason we like Neural Networks so much?</p> <p>Fortunately, not everything is lost. As it turns out, it’s possible to “break” the WL-test bound by inserting higher-order topological information.</p> <p>But what does it mean?</p> <p>As you know, a graph is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = {1, 2, \cdots, n}$ is a set of <em>nodes</em>, and $\mathcal{E}: \mathcal{V} \times \mathcal{V} \rightarrow {0,1}$ is a set of <em>links</em>, also called <em>edges</em> if $(i,j) \in \mathcal{E}$ also implies $(j,i) \in \mathcal{E}$.</p> <p>In other words, elements in $\mathcal{E}$ represent directed, pairwise relations between nodes in the graph.</p> <p>This can be naturally extended by considering a generalization of $\mathcal{E}$, say $\mathcal{E}^{(k)}$, with $k \in \mathbb{N}$, where</p> \[\mathcal{E}^{(k)} : \mathcal{V}^{k+1} \rightarrow \{0,1\}.\] <p>Intuitively, this represents <em>$k$-sized tuples</em> of nodes. For example, for $k=2$, this is equivalent to all <strong>directed triangles</strong> between nodes, while the case $k=1$ recovers the original graph with pairwise links. Note, how, intuitively, for $\mathcal{E}^{(k)}$, we would be effectively considering $k$-dimensional geometric objects: nodes would be 0-dimensional points, edges 1-dimensional lines, triangles 2-dimensional surfaces, and so on (of course this is just an intuition, for this to be true we would need to embed our nodes in a space and require relations to be undirected).</p> <p>Inserting higher-order information in message passing in GNNs can be shown to increase expressivity beyond the regular WL-test. More generally, it can be shown <d-cite key="bodnar2021weisfeiler"></d-cite> that the networks with <strong>order $k$ topological information are bounded by the $k$-WL test</strong>.</p> <p>While this is by no means a formal introduction to higher-order topological objects like Simplicial Complexes, it should be sufficient to paint an intuition about where we’re going: if we manage to message-pass also considering higher order topological objects, instead of just pairs of tokens, we may be able to capture more complex patterns in parallel, instead of having to rely on depth.</p> <h2 id="2-simplicial-attention">2-Simplicial Attention</h2> <p>The Higher-order Attention idea has been floating around for a while: its first implementation in a transformer architecture is dated to the 2019 work by Clift et al. <d-cite key="clift2020simplicialtransformer"></d-cite>, and further along has been reinvented/reinterpreted/tangentially rediscovered in a series of works, such as Representational Strengths and Limitations of Transformers <d-cite key="sanford2023representational"></d-cite>, Tensor attention <d-cite key="liang2024tensorattentiontraining"></d-cite>, The Cellular Transformer <d-cite key="ballester2024cellulartransformer"></d-cite>, AlphaFold 2 <d-cite key="jumper2021alphafold"></d-cite>, and TransNAR <d-cite key="bounsi2024transformersmeetnar"></d-cite>. Even I, since last year, have been obsessed with the idea, proposing it in public a couple of times.</p> <p>Apart from theoretical work, what this idea really needed was a step towards experimental validation under a modern paradigm. Fortunately, Aurko, Rohan and their colleagues delivered well beyond that: a novel implementation <d-cite key="roy2025fastsimplex"></d-cite> of a Higher-order Attention method was the first architectural change to seem to induce a change in the exponent in the scaling law of Large Language Models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Fig. 1: Scaling law results from the Fast and Simplex: 2-Simplicial Attention in Triton paper. </div> <h3 id="rediscovering-simplicial-attention-from-the-topological-perspective">Rediscovering Simplicial Attention from the Topological Perspective</h3> <p>So, how do we extend our graph-based perspective on attention, so that it naturally becomes a (potentially higher-order) topological perspective?</p> <p>Refreshing the graph case, let’s take, for example</p> \[X \in \mathbb{R}^{n \times d}\] <p>And let’s treat the rows of $X$ as a <strong>point-cloud</strong>:</p> \[X = \begin{bmatrix} x_1^{\top}\\ x_2^{\top}\\ \vdots\\ x_n^{\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{n \times d_q}\] \[K = X W_k \in \mathbb{R}^{n \times d_q}\] \[V = X W_v \in \mathbb{R}^{n \times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle = q_i k_j^{\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^{\top} \in \mathbb{R}^{n \times n}.\] <p>The intuition is: the more points align in Query-Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Fig. 2: An attention matrix encodes a graph. </div> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp(\alpha_{ij}/\sqrt{d_k})}{\sum_{j'=1}^n \exp(\alpha_{ij'}/\sqrt{d_k})}, \qquad A = \text{softmax}\left(\frac{\alpha}{\sqrt{d_k}}\right)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\text{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong> which is diffused from its neighbors to each node.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/message_passing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Fig. 3: Left: central node (i) weighs via attention neighboring nodes; Right: central node aggregates via attention weights the value function defined on neighboring nodes. </div> <p>But we already knew all of this from the previous blogpost. The key point to notice, here, is the operation we perform to extract a graph: we project $X$ into two distinct spaces via $W_Q$ and $W_K$, precisely because we need to perform a <em>bi</em>-linear form (the dot product) to extract a two-way relationship.</p> <p>What if we wanted to capture three-way relationships? Naturally, one could think of adding a second $K’$ matrix, resulting from a $W_{K’}$ projection, such that we would have a 3D tensor</p> \[T_{ijk} = \sum_{l} Q_{il} K_{jl} K'_{kl}\] <p>Which can also be seen as taking a multilinear product, if viewed per query:</p> \[T_{ijk} = \langle q_i, k_j, k'_s \rangle\] <p>Notice how, before, each attention score $A_{ij}$ represented the link weight going from node $i$ to node $j$. Now, each entry $T_{ijk}$ can instead be seen as the collective weight assigned to the triangle determined by the (directed) walk from node $i$, passing through node $j$, and ending up in node $k$.</p> <p>Such a triangle, in algebraic topology, may also be called a <em>2-simplex</em> (a node is a 0-simplex, an edge is a 1-simplex), explaining the naming of the attention mechanism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Fig. 4: 2-simplicial attention's tensor T, in each of its entries, represents a (directed) 2-simplex (triangle). </div> <p>Now that we’ve found a formulation to represent 2-simplices (or simplexes, one day I’ll have to decide which version of the plural I prefer), how do we transfer our regular sparsification mechanism (softmax) to it? And, moreover, what even is a neighborhood in this case?</p> <p>The intuitive extension of attention (also used in 2-simplicial attention) treats this by keeping the query token as central: instead of being a matrix, our attention score is now a 3D tensor. This simply means that, instead of rows, we now normalize over entire slices associated with query $i$.</p> <p>Meaning, our softmax operation becomes:</p> \[\alpha^{(2)}_{ijk} = \text{softmax}(T)^{(2)}_{ijk} = \frac{e^{T_{ijk}}}{\sum_{jk} e^{T_{ijk}}}\] <p>Intuitively, this is defining the node’s neighborhood as the <strong>triangles it’s included in</strong>. Hence, here, we’re squashing to zero triangles with low three-way similarity, and amplifying the signal from the more similar ones.</p> <p>This makes sense because our final goal will be to use this information to update the nodes’ embeddings. With that said, there exist more ways to define adjacency for higher order structures: an interesting idea could be to normalize over triangles sharing faces, instead.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Fig. 5: Left: message passing now happens between 2-simplices (oriented triangles). Each 2-simplex is weighed by an entry in tensor T. Right: each 2-simplex has an aggregated value vector that is used to update the node's representation. </div> <p>The last piece of the puzzle is the $V$ matrix of regular attention. As we discussed previously, it can be thought of as a vector-valued function defined on nodes, where individual vectors are rows $V_i$.</p> <p>So what about 2-simplicial attention? Naturally, $V$ would still have to be defined token-wise, but now we have to engineer it so that it can represent, for node $i$, the value associated with the neighbors in a triangle, just like in regular attention $V$ was being aggregated from neighbors in the graph. Furthermore, in order to express value of tokens with full degrees of freedom, we introduce a second value projection, $V’$, that we use analogously to $K’$.</p> <p>What we need is for all triangles $(i,j,k)$ to aggregate $V_j$ and $V_{k}^{\prime}$ with some function $f:\mathbb{R}^{h}\times \mathbb{R}^{h} \rightarrow \mathbb{R}^{h}$. such that we have, for each triangle, a resulting vector \(V^{(2)}_{ijk} = f(V_{k},V_{k}^{\prime})\). In the paper, f is just the product of the entries of $V$, which can be conveniently written as an element-wise product between $V$ and $V^{\prime}$: \(V^{(2)}_{ijk} = V_{ik}V_{jk}^{\prime}\) Apart from convenience, this choice can also be seen as combining value vectors using an “AND” operation, in the sense that large values will compound, and a single small value is sufficient to drop the magnitude of the vector. This is opposed, for example, to having the function be \(V^{(2)}_{ijk} = V_{ik}+ V_{jk}^{\prime}\) which would, instead, be analogous to an “OR” operation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Fig. 6: v and v' from each triangle are aggregated and used to update the central node's embedding. </div> <p>At last, we end up with $V^{(2)}$ being another 3D tensor. This allows us to perform the final operation of attention as a tensor contraction taking us back to our regular $\mathbb{R}^{n \times d}$ shape:</p> \[\text{attention}(x)_{il} = \sum_{jk} \frac{\alpha^{(2)}_{ijk} V^{(2)}_{jkl}}{\sqrt{d}}\] <p>Note how this operation can still be thought of as some kind of “diffusion”: we are aggregating value vectors from each triangle including node $i$, scaling them and summing them to update the vector in node $i$.</p> <p>Now, the extension to the n-simplicial case is trivial:</p> <p>For n-simplices, we just repeat the 2-simplicial recipe with $n$ Key projections. For an $(n+1)$-tuple $(i, j_1, \ldots, j_n)$ define the score tensor by a multilinear form</p> \[T_{i\,j_1 \cdots j_n} = \sum_{\ell} Q_{i\ell} \prod_{m=1}^n K^{(m)}_{j_m \ell} = \langle q_i, k^{(1)}_{j_1}, \ldots, k^{(n)}_{j_n} \rangle,\] <p>and normalize per-query over all $n$-tuples to get</p> \[\alpha^{(n)}_{i\,j_1 \cdots j_n} = \frac{\exp T_{i\,j_1 \cdots j_n}}{\sum_{(j_1, \ldots, j_n)} \exp T_{i\,j_1 \cdots j_n}}.\] <p>Values remain token-wise but are combined along each $n$-simplex via a symmetric $n$-ary reducer $f$; the simplest is the element-wise product “AND”</p> \[V^{(n)}_{i\,j_1 \cdots j_n} = \prod_{m=1}^{n} V^{[m]}_{j_m i},\] <p>though sum/mean (an “OR”) or MLP reducers are possible. The update is then a contraction over all $n$-tuples incident to $i$:</p> \[\text{attn}(X)_{i\ell} = \frac{1}{\sqrt{d}} \sum_{j_1, \ldots, j_n} \alpha^{(n)}_{i\,j_1 \cdots j_n} \left[V^{(n)}_{j_1 \cdots j_n}\right]_\ell\] <p>Topologically, we’re diffusing over the star of $i$ in the $n$-skeleton (cofaces incident to $i$), so higher-order interactions are captured in one hop.</p> <p>Naturally, an $n$-simplicial attention mechanism’s memory scales catastrophically quickly with sequence length, precisely with $O(L^{n+1})$. This means that we have to come up with ways of sparsifying this mechanism in order to make it practical.</p> <p>In the 2-simplicial attention paper, this is solved by performing Sliding Window Attention (SWA) with potentially different windows per dimension in the attention tensor.</p> <p>But is this the only way to tackle this? When I first started pondering these ideas, my first thought was instead to route tokens dynamically to a fixed size window. A very similar idea came recently with DeepSeek 3.2 <d-cite key="deepseek2025v32exp"></d-cite>, in the shape of DeepSeek Sparse Attention (DSA). The intuition is simple: why have a sliding window when you can hand-pick the tokens you want to use, with your preferred sparsity?</p> <p>DSA (DeepSeek Sparse Attention) replaces dense attention with a two-stage sparse mechanism: a <strong>lightweight indexer</strong> followed by <strong>top-k token selection</strong>.</p> <p>The indexer computes cheap similarity scores between each query and all past tokens. For each query token $i$ and indexer head $h$, it first computes</p> \[s_{ijh} = \text{ReLU}(\langle q^I_{ih}, k^I_{j} \rangle) \cdot w^I_{ih},\] <p>where $q^I_{ih}$ is the indexer’s query vector for token $i$ and head $h$, $k^I_j$ is the (shared) indexer key for token $j$, and $w^I_{ih}$ is a learned per-head weight. Summing over heads gives the final score</p> \[S_{ij} = \sum_{h=1}^{H_I} s_{ijh}.\] <p>For each query $i$, the top-k keys according to $S_{ij}$ are selected:</p> \[\mathcal{K}_i = \text{TopK}_j(S_{ij}, k),\] <p>and full attention is then computed <strong>only</strong> on this restricted set.</p> <p>This reduces the core attention complexity from $O(L^2)$ to $O(Lk)$, while preserving the most relevant interactions, making it particularly effective for long contexts.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Fig. 7: Intuitively representing full attention, SWA and DSA in the regular case. </div> <p>In our case, we use a modified version of DSA to substitute SWA: first, we notice that substituting ReLU with softmax performs better on our small experiments on a token-wise level. Furthermore, to avoid individual computation of $qk_1^T$ and $qk_2^T$ distinct pairs, we instead leverage existing $QK^T$ from the previous regular attention layers, and directly index based on those scores, obtaining the same exact top-k scorers for both $k_1$ and $k_2$.</p> <p>This yields a tiny speedup to our very small model / small token-horizon run, while keeping the same scaling as SWA, where we have $O(Lk^2)$ (with $k$ chosen to be equivalent to the window size of SWA) instead of the full sequence $O(L^3)$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 8: Losses for a 127M variant of nanogpt using a 3:1 regular to 2-simplicial attention ratio, with a block size of 512 and a top-k/SWA window of 128 tokens. In gray, is the SWA-sparsified version, in green the DSA-inspired technique we introduced. In orange, regular self-attention. Total token horizon is of around 60M tokens. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/speedup-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/speedup-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/speedup-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/speedup.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 9: speedup across steps of DSA and SWA vs baseline </div> <p>While in Fig. 8 we can see that baseline appears to have roughly the same acceleration as windowed simplicial attention, we notice how the 2-simplicial attention paper itself only notices gains against the transformer at a much larger parameter size, as seen in Fig. 10.</p> <p>Overall, though, our acceleration is of an average of <strong>0.76%</strong> (so barely noticeable) with respect to the Sliding Window Attention version.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Fig. 9: Reported performance comparison between transformers and 2-simplicial attention in the original paper. </div> <h3 id="what-does-n-simplicial-attention-mean-for-depth">What Does n-Simplicial Attention Mean for Depth</h3> <p>As we’ve discussed, one of the key elements of depth is <strong>multi-token representation-learning</strong>. Another way to view it, is that individual tokens are in a <strong>constant relay race</strong>: each token wants to get to a target representation, but needs crucial information from other tokens’ representations to do so. If the proper representation is very hard to find, the model eventually runs out of depth to message-pass. 2-simplicial attention goes in the direction of fixing this, because it <strong>combinatorially opens up surface area</strong> for the model to do message-passing, for each block. Of course, the present one is just its first, prototypal iteration, which will inevitably change in the future.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>We’ve explored a recent advance in attention architecture, and explained it using our previously established topologically-oriented angle. We’ve also outlined a trivial extension to n-simplices of the mechanism, as well as demonstrated tiny gains in expressivity by utilizing a DSA-like sparsification of 2-simplicial attention keys, substituting SWA. Given my obsession with the topic, you’re very likely to read something from me on the topic soon. In the meantime, let me know what you think!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-29-beyond-attention-as-graph.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-ai-evaluations-need-error-bars/">Why AI Evaluations Need Error Bars</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/web-agent/">Computer Use Survey - A Visual Survey of Computer Use Agents</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>