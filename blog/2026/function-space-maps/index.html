<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning Function Space Maps: A Red Herring? | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Much interest has been generated in the space of learning function space maps, such as in deep operator networks and neural operators. In this post, we explore whether viewing data in their underlying, infinite-dimensional form offers benefits in the manner professed or whether this is a fad."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/function-space-maps/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Learning Function Space Maps: A Red Herring?",
            "description": "Much interest has been generated in the space of learning function space maps, such as in deep operator networks and neural operators. In this post, we explore whether viewing data in their underlying, infinite-dimensional form offers benefits in the manner professed or whether this is a fad.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Learning Function Space Maps: A Red Herring?</h1> <p>Much interest has been generated in the space of learning function space maps, such as in deep operator networks and neural operators. In this post, we explore whether viewing data in their underlying, infinite-dimensional form offers benefits in the manner professed or whether this is a fad.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-prestine-world-of-mathematics">The Prestine World of Mathematics</a> </div> <div> <a href="#partial-differential-equations">Partial Differential Equations</a> </div> <div> <a href="#neural-operators">Neural Operators</a> </div> <div> <a href="#surrogates-for-engineering-design">Surrogates for Engineering Design</a> </div> <div> <a href="#the-utility-of-neural-operators">The Utility of Neural Operators</a> </div> <ul> <li> <a href="#call-to-action">Call to Action</a> </li> </ul> </nav> </d-contents> <h2 id="the-prestine-world-of-mathematics">The Prestine World of Mathematics</h2> <p>Arguments between the “discovery” and “invention” of math date back to the advent of the field. We believe, for instance, that math largely exists in the minds of people, existing in a platonic idealization of reality that lends itself to careful manipulation in an almost unprecedented manner that allows people to escape from their own intuitions. That is, by turning the crank of mathematics, people often discover surprising phenomena that they otherwise would never have predicted. This is not merely that humanity more generally are surprised by the findings uncovered by mathematical manipulation, but even the people who are <em>doing the cranking</em> are oftentimes themselves surprised.</p> <p>Mathematics, however, often gets sullied, in the eyes of mathematicians, when brought down to the messiness of reality. Perfect geometric shapes are replaced by curves with tolerances from manufacturing defects. Clean, analytic manipulations give way to numerical approximations. What, then, is the use of these mathematically pure objects if they eventually require such degradation?</p> <p>In particular, we focus this question on the recent rise of interest in using ML over function spaces to see whether this offers legitimate benefits over the traditional, discretized counterparts.</p> <h2 id="partial-differential-equations">Partial Differential Equations</h2> <p>Before diving into neural operators, let us first take a detour into the mature study of partial differential equations (PDEs). PDEs are a crowning achievement of physics, having produced remarkably predictive equations for many phenomena in nature. One notable example is the heat equation</p> \[\frac{\partial u(x,t)}{\partial t} = \nabla u(x,t)\] <p>In the heat equation, $u(x,t)$ is a spatiotemporal field, meaning that it maps $u : \mathbb{R}^{d}\times\mathbb{R}\to\mathbb{R}$, a position in the spatial domain and time in the temporal domain to a temperatue reading. “Solving” this PDE then means, given a specification of the <em>initial</em> state of the system, can we predict the evolution of the system state? That is, given the initial temperature distribution $u(\cdot, 0)$, can we predict the temperature field at a later time $u(\cdot, T)$?</p> <p>An interesting property of PDEs is that, while it may be feasible to analytically posit them, it is incredibly rare to find one that can be analytically solved. To circumvent this deficiency, we are often forced to approximate the dynamics and simulate the system evolution. Let us briefly switch to a simple ODE to understand this approach before returning to the PDE case.</p> <p>Suppose we has a basic ODE $\frac{dx}{dt} = f(x)$ and that we did not know how to solve this particular ODE. Recall that a derivative is just the limit of a difference:</p> \[\frac{dx}{dt} = \lim_{\Delta t\to0} \frac{x(t + \Delta t) - x(t)}{\Delta t}\] <p>An intuitive idea, therefore, is to simply replace the derivative with this approximation:</p> \[\frac{x(t + \Delta t) - x(t)}{\Delta t} = f(x(t))\] <p>From this, we can now take any initial state $x(0)$ and evolve to a time $x(\Delta t)\approx f(x(0)) \Delta t + x(0)$. We can then use this to get $x(2 \Delta t)\approx f(x(\Delta t)) \Delta t + x(\Delta t)$ and so on. Naturally, there is a deep literature of numerical methods to solve ODEs. These approaches, however, fundamentally all propose a conceptually similar approach to that discussed above: discretizing the differential operator.</p> <p>Returning to the space of PDEs, a similar approximation exists, in which the differential operators are replaced by their numerical counterparts and iterated to arrive at the answer. The time derivatives are treated analogously to in the ODE setting. To approximate spatial derivatives, we then need an analogous discretization to the $\Delta t$ used in the time domain. In a 1D spatial domain, a natural choice is to use an interval length $\Delta x$, in which case the $\nabla = \frac{\partial^2}{\partial x^2}$ becomes</p> \[\nabla u(x, t) \approx \frac{u(x + \Delta x, t) - u(x - \Delta x, t)}{2 \Delta x}.\] <p>To characterize the full evolution of $u(\cdot, t)$, the intuitive strategy is to then alternate between solving for the spatial field at a fixed time step $t$ and evolving this field to the next time step.</p> <h2 id="neural-operators">Neural Operators</h2> <p>A core issue with numerical solvers is that any new initial condition $u(\cdot, 0)$ or problem specification must be solved anew. This, however, is a failing of the method rather than something fundamentally true of PDE solutions. After all, analytic solutions are such a method that allows for the “instantaneous” solving of newly specified problems; if we have an analytic solution of a PDE $\mathcal{G}$, any new initial condition $u(\cdot, 0)$ can be plugged in to arrive at the solution $\mathcal{G}(u(\cdot, 0)) = u(\cdot, T)$. Given the success of, for example, AlphaFold in protein structure prediction in cases where we could not hand-specify an analytic folding function <d-cite key="abramson2024accurate"></d-cite>, a natural question to ask is whether such PDE solution maps can be learned from data.</p> <p>In this vein, a slew of works have arisen, such as <d-cite key="li2020fourier,bonev2023spherical,liu2025difffno,lu2019deeponet,wang2021learning"></d-cite>. These maps that approximate the solutions of PDEs are termed “neural operators.” An “operator” in math is simply a map between two function spaces $\mathcal{G} : \mathcal{U}\to\mathcal{V}$. Often, these function spaces are taken to be Hilbert spaces over a class of sufficiently smooth functions. <d-footnote>We will ignore the technical details of smoothness in this discussion, but for those interested, Sobolev spaces are typically employed to ensure the differential operators are well defined.</d-footnote> A “neural” operator, therefore, is simply a learned approximation of this mapping $\mathcal{G}_{\theta}$.</p> <p>In the above example, we were interested in learning the mapping between initial and final conditions. A natural setup, therefore, would be to have a dataset $\mathcal{D} = {(u^{(i)}(\cdot, 0), u^{(i)}(\cdot, T))}_{i=1}^{N}$ pairs, where $u^{(i)}(\cdot, T) = \mathcal{G}(u^{(i)}(\cdot, 0))$ for the true operator $\mathcal{G}$, and to then train this neural operator in a typical MSE fashion:</p> \[\theta^* := \mathrm{arg}\min_{\theta} \sum_{i=1}^{N} || \mathcal{G}_{\theta}(u^{(i)}(\cdot, 0)) - u^{(i)}(\cdot, T) ||^{2}_{\mathcal{U}}\] <p>However, therein lies the problem: unlike in typical machine learning tasks, the data here are fundamentally unobservable. This is in the sense we described earlier: a function only exists in the pristine world of mathematics. We, therefore, cannot construct ever truly produce such a dataset $\mathcal{D}$.</p> <p>Instead, datasets consist of solved PDEs, in the manner described previously. That is, the dataset consists of <em>discretized</em> fields. What, then, does it mean for neural operators to treat these observations as functions if we only observe them as discretized fields? At their core, neural operators assume the discretized observations arise from an underlying field, thus allowing the discretization to be nonuniform across the dataset. In particular, if each datapoint is discretized with some grid $\mathcal{X}_{i}\,$ the function loss function is then given by the finite-dimensional norm induced on $\mathcal{U}$. For instance, in the common case of $\mathcal{U} = \mathcal{L}^{2}(\mathcal{X})$, this loss becomes</p> \[\sum_{i=1}^{N} \sum_{x\in\mathcal{X}_i} || \mathcal{G}_{\theta}(u^{(i)}(x, 0)) - u^{(i)}(x, T) ||^{2}\] <p>The reason to leverage neural operators, therefore, is to take advantage of datasets with datasets with non-uniform grids. Returning to the original focus, is this property worthy of further investigation or is the field over-indexing on this property?</p> <h2 id="surrogates-for-engineering-design">Surrogates for Engineering Design</h2> <p>To answer this question, it is worthwhile investigating the use of neural operators. As mentioned, neural operators are intended to learn the solution maps of PDEs to allow for rapidly evaluation of different initial conditions to circumvent our inability to produce such a mapping analytically. Critically, this differs in a fundamental sense from the AlphaFold application. In the case of AlphaFold, we previously had <em>no</em> method to map from amino acid sequences to 3D structures. In contrast, we <em>do</em> have solution maps in the case of PDEs: numerical solvers. The only problem with numerical solvers is that they are incredibly costly, making solutions over collections of initial conditions slow.</p> <p>This makes the value proposition of neural operators fundamentally different from AlphaFold: the question is, are there settings where the rapid solution of PDEs justifies a potential loss in accuracy in using this learned surrogate in place of the numerical solver? A common application is in the rapid evalution of engineering designs. For instance, in the design of aircraft or vehicles, an engineer will propose a potential shape of the aircraft wing or car chassis, for instance, which then gets evaluated by running a computational fluid dynamics (CFD) solver to estimate the drag expected of the proposed design.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-function-space-maps/cfd-480.webp 480w,/2026/assets/img/2026-04-27-function-space-maps/cfd-800.webp 800w,/2026/assets/img/2026-04-27-function-space-maps/cfd-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-function-space-maps/cfd.png" class="img-cfd" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p class="figcaption"> CFD simulation of flow around an airfoil. <d-cite key="krishnamurthy2018meshCFD"></d-cite> </p> <p>Naturally, such simulations will often reveal deficiencies in the proposed design, revealing regions of the original design producing especially high drag. With such insights, engineers can then improve the design, iteratively producing more and more efficient or cost-effective designs. This, however, brings us back to the major deficiency of numerical methods: since the cost of running a solver both high and is not amortized across runs, iterating over designs becomes slow. In contrast, neural operators allow for rapid estimation of the quality of a design, in turn allowing engineers to far more rapidly iterate on designs or even do so in an automated fashion.</p> <p>For this reason, much of the practical use of such neural operators has focused on its surrogate use in design optimization, such as in materials <d-cite key="michaloglou2025physics,wang2026micrometer,jin2025characterization"></d-cite> and airfoil shape <d-cite key="shukla2023deep,shukla2024deep"></d-cite>. Formally, these surrogate models enable us to more efficiently solve a “PDE-constrained optimization” problem,</p> \[\min_{\theta} J(u_{\theta}) \qquad \mathrm{s.t.} \quad D_{\theta} u = f_{\theta}\] <p>where $D_{\theta} u = f_{\theta}$ is the PDE that the field must satisfy (i.e., the Navier-Stokes equations in the case of the fluid flow around a car) and $J(u_{\theta})$ is the functional of such a field (i.e., the drag resulting from such a flow field).</p> <h2 id="the-utility-of-neural-operators">The Utility of Neural Operators</h2> <p>Notably, the above discussion regarding neural surrogates makes no explicit reference to discretization invariance. In other words, any surrogate model, learned or hand-crafted, could serve equally well for the above purpose. In that case, are discretization-invariant surrogate models useful for design optimization? It depends.</p> <p>Even though we specifically discussed the map from initial to final conditions in the discussions above, neural operators are not limited to this application. For instance, the Poisson equation can be used to model the stationary temperature distribution with a heat source function $f(x)$ as</p> \[\nabla u(x) = -f(x)\] <p>In this case, there is no temporal dependency and one may wish instead to learn the map $\mathcal{G} : f\to u$. Such an approximation can be useful if, for instance, one has control over the source heat distribution (i.e., with local thermostats) and wishes to achieves a target thermal profile over a room. A discretization-invariant surrogate would then be useful if, across different design choices $f$, the ideal discretization would vary. While seemingly artificial, this phenomenon naturally arises when using numerical solvers.</p> <p>Suppose one has a spatial domain where a field is rapidly varying: using a fixed discretization $\Delta x$ for such regions as well as those that vary more gradually would result in poor approximations of the spatial gradient. For this reason, the spatial discretization is oftentimes non-uniform and adjusted to closely follow where it is expected more rapidly fluctuations of the solution field will exist, such as around sharp curves of a car or wing design in a fluids simulation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-function-space-maps/mesh-480.webp 480w,/2026/assets/img/2026-04-27-function-space-maps/mesh-800.webp 800w,/2026/assets/img/2026-04-27-function-space-maps/mesh-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-function-space-maps/mesh.jpg" class="img-mesh" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p class="figcaption"> Non-uniform discretization is often necessary to capture high frequency variation in the fields over the spatial domain. <d-cite key="centaur2025MeshRefinement"></d-cite> </p> <p>The discretization invariance of the surrogate, therefore, is useful in settings where the meshing will need to adapt as a result of the design iteration. This is necessarily true in settings of shape optimization, where the domain shape is precisely the variable of optimiation, and even in many other cases of source property optimization, as such cases often require variable meshes to align well with each design iterate.</p> <h3 id="call-to-action">Call to Action</h3> <p>While the above section highlights cases where a discretization-invariant surrogate could be useful, little work has yet concentrated on mesh adaptation concurrent to design iteration with neural operators. That is to say, little direct empirical evidence exists for the advantages afforded by neural operators for design iteration over fixed resolution surrogate maps. For this reason, a necessary step in the development of this field is to empirically validate whether this finding truly holds up to empirical scrutiny.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-function-space-maps.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-ai-evaluations-need-error-bars/">Why AI Evaluations Need Error Bars</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>