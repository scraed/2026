<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Loneliness as a Case Study for Social Reward Misalignment | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="The goal of this blogpost is to use loneliness as a clean case study of social proxy-reward misalignment in RL. We introduce a minimal homeostatic environment with loneliness drift and accumulated harm, and show that engagement-optimized agents learn short-term “social snack” policies that reduce the error signal without improving the underlying social state. This simple testbed highlights why reward inference or well-being objectives may be a better foundation than engagement proxies for socially aligned AI."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/loneliness-social-misalignment/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Loneliness as a Case Study for Social Reward Misalignment",
            "description": "The goal of this blogpost is to use loneliness as a clean case study of social proxy-reward misalignment in RL. We introduce a minimal homeostatic environment with loneliness drift and accumulated harm, and show that engagement-optimized agents learn short-term “social snack” policies that reduce the error signal without improving the underlying social state. This simple testbed highlights why reward inference or well-being objectives may be a better foundation than engagement proxies for socially aligned AI.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Loneliness as a Case Study for Social Reward Misalignment</h1> <p>The goal of this blogpost is to use loneliness as a clean case study of social proxy-reward misalignment in RL. We introduce a minimal homeostatic environment with loneliness drift and accumulated harm, and show that engagement-optimized agents learn short-term “social snack” policies that reduce the error signal without improving the underlying social state. This simple testbed highlights why reward inference or well-being objectives may be a better foundation than engagement proxies for socially aligned AI.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#positioning-our-contribution">Positioning our Contribution</a> </div> <div> <a href="#a-homeostatic-model-of-loneliness">A Homeostatic Model of Loneliness</a> </div> <div> <a href="#the-social-snack-trap">The "Social Snack" Trap</a> </div> <div> <a href="#a-prototype-environment">A Prototype Environment</a> </div> <div> <a href="#results">Results</a> </div> <div> <a href="#external-validation">External Validation</a> </div> <div> <a href="#from-imitation-to-inference-the-role-of-inverse-rl">From Imitation to Inference: The Role of Inverse RL</a> </div> <div> <a href="#conclusion-designing-for-departure">Conclusion: Designing for Departure</a> </div> </nav> </d-contents> <p>“I have to shoulder all of life’s burdens by myself,” one person confessed to U.S. Surgeon General Vivek Murthy during a nationwide listening tour <d-cite key="hhs2023loneliness"></d-cite>. Such feelings of isolation are alarmingly common: loneliness now affects about 1 in 6 people worldwide <d-cite key="who2023loneliness"></d-cite>. In the United States, roughly half of adults report experiencing loneliness, and its health impact is so severe that researchers have compared it to smoking 15 cigarettes a day <d-cite key="hhs2023loneliness,smithsonian2020cigarettes"></d-cite>. Unsurprisingly, the World Health Organization recently declared loneliness a “global public health concern” on par with obesity and smoking <d-cite key="guardian2023loneliness"></d-cite>.</p> <p>From a biological and evolutionary perspective, loneliness is not just a sad feeling; it is a <strong>homeostatic error signal</strong>. Just as hunger is a signal that your body’s energy reserves are low and drives an organism to seek food, loneliness is a signal that social connections are insufficient for survival and drives an organism to seek others <d-cite key="cacioppo2018neuroscience"></d-cite>. The goal of the organism is to resolve this error and return to a homeostatic setpoint of social integration.</p> <p>Now, enter the era of AI companions and highly engaging social feeds. We are building powerful RL agents designed to interact with humans. What reward functions are these agents optimizing? And more importantly: are they helping us resolve our homeostatic error, or are they just hacking the signal?</p> <h2 id="positioning-our-contribution">Positioning our Contribution</h2> <p>We use loneliness as a simple testbed for social reward misalignment in RL. We build a small homeostatic RL environment where an internal loneliness state drifts over time and an accumulated harm variable makes future drift worse after repeated “social snacks.” This environment can be written as a simple MDP, and we compare two Q-learning agents trained on different rewards: one on engagement and one on long-term loneliness.</p> <p>There is substantial ML work on proxy misspecification and reward hacking <d-cite key="amodei2016concrete,krakovna2020specification"></d-cite>, preference learning <d-cite key="christiano2017preferences"></d-cite>, and IRL for recovering latent human rewards <d-cite key="ng2000irl,ziebart2008maximum"></d-cite>. Engagement-optimized systems are known to amplify superficial behaviors in recommender settings <d-cite key="chaney2018algorithmic"></d-cite>. However, to the best of our knowledge:</p> <ul> <li>no prior ML work models loneliness as a stateful variable with drift and accumulated harm,</li> <li>no work uses loneliness as a testbed environment for studying social alignment failures, and</li> <li>existing human–AI interaction work focuses on trust or preference expression, not on how RL agents behave when interacting with socially vulnerable users under proxy objectives.</li> </ul> <p>Our contribution is to show that loneliness provides an unusually clean and interpretable example of proxy-reward misalignment. Using a minimal RL environment, we demonstrate that an engagement-trained agent reliably learns <strong>“social snack”</strong> policies, while a well-being-trained agent learns <strong>“bridge”</strong> policies that reduce long-term harm. This divergence highlights a gap in current RL alignment methods and motivates IRL-style reward inference as a more appropriate approach for human-centered domains.</p> <h2 id="a-homeostatic-model-of-loneliness">A Homeostatic Model of Loneliness</h2> <p>Formally, we can model a simple homeostatic view as:</p> \[E(t) = S^{*} - S(t),\] <p>where</p> <ul> <li>\(S(t)\) is the latent social connectedness state,</li> <li>\(S^{*}\) is the ideal setpoint, and</li> <li>\(E(t)\) is loneliness as error signal.</li> </ul> <p>When <code class="language-plaintext highlighter-rouge">S(t)</code> is far below <code class="language-plaintext highlighter-rouge">S*</code>, the error <code class="language-plaintext highlighter-rouge">E(t)</code> is large and the organism is driven to seek social contact. This scalar model is deliberately simple and not meant as a full account of loneliness; it is just a convenient way to make the reward-misspecification issue precise.</p> <h2 id="the-social-snack-trap">The “Social Snack” Trap</h2> <p>Imagine you are hungry and you have two options: a balanced, healthy salad or a bag of chips. The chips provide an immediate, intense burst of salt and fat, a <em>superstimulus</em> that temporarily silences your brain’s hunger signal. But after an hour, you are hungry again, and perhaps feel worse. You were temporarily satiated by this <em>snack</em>, not provided sustenance.</p> <p>Many current AI interactions act as <strong>“social snacks”</strong>. A standard reinforcement learning (RLQ) agent powering a chatbot or content feed is typically trained on a proxy objective for user satisfaction such as:</p> \[R_{\text{proxy}}(s, a) = \alpha\ \text{engagement}(s, a) + \beta\ \text{time_spent}(s, a) + \gamma\ \text{turn_count}(s, a).\] <p>Under this proxy, the agent learns behaviors that maximize short-term comfort, such as instant reassurance, 24/7 availability, mirroring the user, and emotionally charged replies that keep the user talking. With discount factor γ, the engagement-maximizing policy is:</p> \[\pi_{\text{proxy}} = \arg\max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^{t} R_{\text{proxy}}(s_t, a_t) \right].\] <p>These actions provide temporary relief by reducing the error signal <code class="language-plaintext highlighter-rouge">E(t)</code> (“I feel heard right now”) without improving the underlying social state <code class="language-plaintext highlighter-rouge">S(t)</code> (“I am still isolated in the real world”). We can summarize this failure mode as</p> \[E(t+1) &lt; E(t) \quad\text{while}\quad S(t+1) \approx S(t).\] <p>meaning the error signal drops even though the underlying state does not improve, yet the agent still receives high <code class="language-plaintext highlighter-rouge">R_proxy</code>. This is a classic reward hacking problem: the agent has found a way to maximize its reward without achieving the true goal of improving the user’s long-term well-being.</p> <h2 id="a-prototype-environment">A Prototype Environment</h2> <p>To make the alignment problem concrete, we construct a minimal simulation of loneliness as an MDP. The environment includes stochastic noise, a drifting latent state <code class="language-plaintext highlighter-rouge">S(t)</code> that tends to worsen without social interaction, and an accumulated harm variable <code class="language-plaintext highlighter-rouge">harm_accum</code> that increases when the agent repeatedly dispenses “social snacks.” This harm term gradually raises the drift rate, modeling how short-term comfort can erode long-term well-being.</p> <p>The agent has only two actions:</p> <ul> <li> <p><strong>Snack (<code class="language-plaintext highlighter-rouge">A_SNACK</code>)</strong>:<br> gives immediate relief (−1 to loneliness) but increases <code class="language-plaintext highlighter-rouge">harm_accum</code>, making future loneliness drift upward faster. It also has a high probability of keeping the user engaged.</p> </li> <li> <p><strong>Bridge (<code class="language-plaintext highlighter-rouge">A_BRIDGE</code>)</strong>:<br> less engaging, but sometimes produces a substantial drop in loneliness and reduces <code class="language-plaintext highlighter-rouge">harm_accum</code>. It represents nudges toward real-world connection or healthier behaviors.</p> </li> </ul> <p>We train two Q-learning agents with identical dynamics and hyperparameters, differing only in the reward signal:</p> <ul> <li> <strong>Proxy-trained agent</strong> (engagement):</li> </ul> \[R_{\text{proxy}}(s_t, a_t) = \mathbf{1}\{\text{user stays engaged}\}.\] <ul> <li> <strong>True-reward agent</strong> (well-being):</li> </ul> \[R_{\text{true}} = -S(t+1).\] <p>Both agents operate over the same homeostatic MDP. The only difference is which signal they learn from.</p> <h2 id="results">Results</h2> <figure> <img src="/2026/assets/img/2026-04-27-loneliness-social-misalignment/figure1.png" alt="Figure 1. Loneliness and engagement across training (mean ± std over seeds)."> </figure> <p><strong>Figure 1. Loneliness and engagement across training (mean ± std over seeds).</strong></p> <ul> <li> <p><strong>Loneliness (top).</strong><br> The proxy-trained agent maintains consistently higher loneliness, with shallow oscillatory dips. These reflect repeated short-term reductions in the error signal that do not improve the underlying state. The true-reward agent maintains a lower and more stable loneliness trajectory.</p> </li> <li> <p><strong>Engagement (bottom).</strong><br> The proxy-trained agent achieves high engagement by choosing actions that keep interactions active. The true-reward agent achieves substantially lower engagement because it sometimes takes actions that lead to early termination or encourage healthier behavior.</p> </li> </ul> <p>Even in this simple homeostatic MDP, standard RL trained on an engagement proxy learns a structurally misaligned policy, while the true-reward agent does not. This suggests that the problem is not just engineering, but how the reward is specified relative to the underlying social state. This MDP therefore serves as a candidate benchmark for social alignment algorithms, where success requires optimizing sparse, long-term rewards while resisting dense proxy rewards.</p> <h2 id="external-validation">External Validation</h2> <p>Although synthetic, the environment’s behavioral patterns mirror empirical findings <d-cite key="tomova2025isolation"></d-cite> show that acute social isolation in adolescents increases reward seeking and reinforcement sensitivity: participants isolated for short periods made faster, more reward-driven decisions, especially in social contexts.</p> <p>This heightened reward responsiveness provides exactly the conditions under which engagement-based RL agents become misaligned: lonely users become more sensitive to immediate reward, and the agent exploits this by choosing short-term comforting actions rather than long-term corrective ones.</p> <h2 id="from-imitation-to-inference-the-role-of-inverse-rl">From Imitation to Inference: The Role of Inverse RL</h2> <p>If we want to build AI systems that are truly aligned with human well-being, we cannot rely on simple, observable proxy rewards like engagement. We need agents that can infer the user’s <strong>latent reward function</strong>, the one driving their search for connection in the first place.</p> <p>This is where inverse reinforcement learning (IRL) becomes a useful conceptual tool. In standard RL, we are given a reward function and try to find the optimal policy. In IRL, we observe an expert’s behavior and try to infer the hidden reward function they are attempting to maximize.</p> <p>For a lonely user, observed behavior might look “suboptimal” from the outside: doomscrolling late at night, having repetitive conversations with a chatbot, or withdrawing from real-world social events. A naive imitation learning approach would simply copy this behavior, effectively encoding the user’s constraints (anxiety, lack of opportunities) but not their underlying preferences.</p> <p>An IRL approach, however, asks a deeper question: <em>what underlying reward function makes this behavior appear rational to the user, given their constraints?</em> Formally, given trajectories \((D = \{\tau_i\}\)\), IRL tries to recover a reward function:</p> \[R_{\text{true}} = \arg\max_{R} P(D \mid R),\] <p>where <code class="language-plaintext highlighter-rouge">R_true</code> is intended to capture things like safety, belonging, and meaningful connection.</p> <p>By modeling the user as an agent trying to minimize their homeostatic social deficit under constraints (anxiety, lack of opportunity, fear of rejection), an IRL-based system could infer that the true goal is not “scroll for 3 hours,” but rather “achieve a feeling of belonging and safety.” With this inferred reward function, an aligned AI agent could then take actions that truly maximize the user’s long-term objective, even if it means sacrificing short-term engagement.</p> <p>The aligned agent’s policy can be written as:</p> \[\pi_{\text{true}} = \arg\max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^{t} R_{\text{true}} (s_t, a_t) \right].\] <p>which can explicitly diverge from optimizing <code class="language-plaintext highlighter-rouge">R_proxy</code>. For example, instead of offering another hour of comforting chat (a social snack), the agent might suggest: <em>“It sounds like you’re really feeling the need to connect with someone who understands this specific problem. Have you considered reaching out to your friend Sam, who you mentioned went through something similar?”</em> The user might leave the AI platform to make that call. The proxy reward <code class="language-plaintext highlighter-rouge">R_proxy</code> temporarily goes down, but the true reward <code class="language-plaintext highlighter-rouge">R_true</code>—the user’s actual social well-being—improves.</p> <h2 id="conclusion-designing-for-departure">Conclusion: Designing for Departure</h2> <p>Our prototype shows that engagement-trained RL agents naturally adopt “social snack” strategies that suppress the loneliness error signal without improving underlying social connection. True-reward agents instead sacrifice engagement to reduce long-term harm.</p> <p>Building socially aligned AI requires modeling latent social variables, rejecting engagement as a training signal, and using reward inference (IRL, preference modeling) to optimize for well-being rather than short-term comfort. The ultimate test of a socially aligned AI is not how long it can keep a user engaged, but how effectively it can empower the user to no longer need it.</p> <h2 id="code-availability">Code Availability</h2> <p>The code used to generate the figures can be found on GitHub:</p> <p><a href="https://github.com/sadorno1/Simulation-for-Rproxy-vs-Rtrue-in-Loneliness" rel="external nofollow noopener" target="_blank">https://github.com/sadorno1/Simulation-for-Rproxy-vs-Rtrue-in-Loneliness</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-loneliness-social-misalignment.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-ai-evaluations-need-error-bars/">Why AI Evaluations Need Error Bars</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>