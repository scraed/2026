<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Trade-offs in LLM Compute for Reasoning-Intensive Information Retrieval | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Large Language Models have become essential for reasoning-intensive information retrieval, but their computational cost raises a critical question: where should compute be allocated for maximum effectiveness? Using the BRIGHT benchmark and the Gemini 2.5 model family, we systematically evaluate trade-offs across model strength, inference-time thinking depth, and reranking depth. Our controlled experiments reveal the marginal gains of investing compute in query expansion versus reranking, providing practical guidance for optimizing cost-performance in LLM-augmented retrieval pipelines."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Trade-offs in LLM Compute for Reasoning-Intensive Information Retrieval",
            "description": "Large Language Models have become essential for reasoning-intensive information retrieval, but their computational cost raises a critical question: where should compute be allocated for maximum effectiveness? Using the BRIGHT benchmark and the Gemini 2.5 model family, we systematically evaluate trade-offs across model strength, inference-time thinking depth, and reranking depth. Our controlled experiments reveal the marginal gains of investing compute in query expansion versus reranking, providing practical guidance for optimizing cost-performance in LLM-augmented retrieval pipelines.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Trade-offs in LLM Compute for Reasoning-Intensive Information Retrieval</h1> <p>Large Language Models have become essential for reasoning-intensive information retrieval, but their computational cost raises a critical question: where should compute be allocated for maximum effectiveness? Using the BRIGHT benchmark and the Gemini 2.5 model family, we systematically evaluate trade-offs across model strength, inference-time thinking depth, and reranking depth. Our controlled experiments reveal the marginal gains of investing compute in query expansion versus reranking, providing practical guidance for optimizing cost-performance in LLM-augmented retrieval pipelines.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#reasoning-intensive-retrieval-and-bright">Reasoning-Intensive Retrieval and BRIGHT</a> </li> <li> <a href="#the-compute-levers-in-neural-ir">The Compute Levers in Neural IR</a> </li> <li> <a href="#the-thinking-dimension">The "Thinking" Dimension</a> </li> </ul> <div> <a href="#experimental-setup">Experimental Setup</a> </div> <ul> <li> <a href="#model-suite">Model Suite</a> </li> <li> <a href="#evaluation-metrics">Evaluation Metrics</a> </li> <li> <a href="#experiment-design">Experiment Design</a> </li> </ul> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>The paradigm of Information Retrieval (IR) is undergoing a fundamental shift. While traditional IR focused on semantic similarity and keyword matching, modern applications increasingly demand Reasoning-Intensive Information Retrieval (RIIR). In these scenarios, as shown by the recent ICLR 2025 Spotlight paper BRIGHT - a system cannot simply find a document that looks like the query; it must understand complex logic, synthesize constraints, and deduce relationships to identify the correct evidence.</p> <p>The BRIGHT benchmark established that standard retrieval methods (both sparse and dense) struggle significantly with these tasks. However, it also highlighted a promising path forward: the integration of Large Language Models (LLMs) into the retrieval pipeline. Specifically, the paper demonstrated that Query Expansion (QE) and LLM-based Reranking (RR) are critical for boosting performance.</p> <p>Yet, this introduces a complex resource allocation problem. <strong>If we view LLM compute as a finite resource (quantifiable by inference cost or latency), where is it best spent?</strong> Should we use a stronger, more expensive model to formulate a better search query (QE), or should we reserve that compute to carefully check the retrieved documents (during RR)? Furthermore, with the advent of “thinking” models (models capable of dynamic chain-of-thought generation during inference), we now have a third lever: the depth of reasoning per token.</p> <p>In this blogpost, we perform a controlled ablation study to study the tradeoffs of LLM compute in RIIR. Using the Gemini 2.5 family of models, we systematically evaluate the cost-performance trade-offs across three dimensions:</p> <ul> <li> <strong>Model Strength</strong>: From lightweight models (Flash-Lite) to reasoning-heavy models (Pro).</li> <li> <strong>Thinking Depth</strong>: Comparing standard inference against dynamic “thinking” modes.</li> <li> <strong>Reranking Depth</strong>: Analyzing the impact of increasing the reranking pool size (k).</li> </ul> <p>Our goal is to answer a practical question for system designers:</p> <blockquote> <p>In a reasoning-intensive IR pipeline, where does an additional unit of compute yield the highest marginal gain in retrieval accuracy?</p> </blockquote> <h2 id="background">Background</h2> <h3 id="reasoning-intensive-retrieval-and-bright">Reasoning-Intensive Retrieval and BRIGHT</h3> <p>Standard retrieval benchmarks (e.g., BEIR) often rely on lexical overlap or semantic proximity. In contrast, the BRIGHT benchmark consists of queries where the answer requires multi-hop reasoning or domain-specific logic that is not explicitly present in the query text. For example, a query might ask about a specific chemical property that implies a class of materials, requiring the retriever to identify documents discussing those materials without the explicit class name being present.</p> <p>More formally, let $\mathcal{C}$ be a large corpus of documents and $q$ be a user query. In standard IR, relevance is often approximated by lexical overlap or semantic similarity. In RIIR, relevance is determined by a latent logic or reasoning requirement.</p> <p>We define a binary relevance function $Rel(q, d) \in {0, 1}$ provided by the dataset annotations. The objective is to retrieve the subset of relevant documents $\mathcal{D}^* = {d \in \mathcal{C} \mid Rel(q, d) = 1}$ and rank them at the top of the result list. Unlike factoid retrieval where a single document might suffice, RIIR tasks in BRIGHT often involve multiple relevant documents that must be identified based on implicit characteristics derived from $q$.</p> <h3 id="the-ir-pipeline-for-riir">The IR Pipeline for RIIR</h3> <p>To address the complexity of RIIR, we utilize a multi-stage pipeline. The process generally follows a “retrieve-then-rerank” architecture augmented by LLMs.</p> <h4 id="step-1-query-expansion-qe">Step 1: Query Expansion (QE)</h4> <p>The raw query $q$ is often underspecified or requires domain knowledge to map to relevant terms in $\mathcal{C}$. An LLM is used to generate an expanded query $q_{exp}$:</p> \[q_{exp} = \text{LLM}_{\theta}(q)\] <p>This expansion adds context, uncovers implicit constraints, and generates keywords that are statistically likely to appear in $\mathcal{D}^*$.</p> <h4 id="step-2-initial-retrieval-bm25">Step 2: Initial Retrieval (BM25)</h4> <p>We use the BM25 (Best Matching 25) algorithm for the initial retrieval stage. BM25 is a probabilistic retrieval framework based on TF-IDF (Term Frequency-Inverse Document Frequency). It scores documents based on the frequency of query terms in the document relative to their frequency across the entire corpus, with normalization for document length. Given $q_{exp}$, BM25 retrieves an initial candidate list $\mathcal{L}_{init} = {d_1, d_2, …, d_N}$ sorted by lexical relevance.</p> <h4 id="step-3-llm-based-reranking-rr">Step 3: LLM-based Reranking (RR)</h4> <p>The top-$k$ documents from $\mathcal{L}_{init}$ are passed to an LLM for re-ordering. In our setup, we employ a list-wise reranking approach rather than a point-wise scoring function. The LLM receives the query and the concatenated text of the top-$k$ candidates as a single prompt. It is instructed to reason over the set and output the identifiers of the top 10 most relevant documents in descending order:</p> \[\pi_{top10} = \text{LLM}_{\phi}(q, \{d_1, ..., d_k\})\] <p>This approach allows the model to compare candidates directly against one another within its context window.</p> <h3 id="the-thinking-dimension">The “Thinking” Dimension</h3> <p>We leverage the Gemini 2.5 family’s ability to perform inference-time compute scaling. “Thinking” models generate internal Chain-of-Thought (CoT) traces before producing the final output. In the context of QE, this allows the model to plan the search strategy. In RR, it allows the model to explicitly reason through the connection between the query constraints and the candidate document content before assigning a rank.</p> <h2 id="experimental-setup">Experimental Setup</h2> <p>To isolate the impact of compute allocation, we fix our retrieval algorithm to BM25 (using the Pyserini implementation) and vary the LLM components used for Query Expansion and Reranking.</p> <h4 id="model-suite">Model Suite</h4> <p>We utilize the Google Gemini 2.5 family to represent a spectrum of cost and capability. We categorize them as follows:</p> <ul> <li>Gemini-2.5-Flash-Lite: A highly efficient, low-latency model (No-Thinking mode only).</li> <li>Gemini-2.5-Flash (No-Think): A standard mid-sized model (Thinking features disabled).</li> <li>Gemini-2.5-Flash (Think): The same mid-sized model with dynamic thinking enabled, allowing for extended reasoning tokens.</li> <li>Gemini-2.5-Pro: A large, high-capacity model (Thinking mode enabled).</li> </ul> <h4 id="evaluation-metrics">Evaluation Metrics</h4> <ul> <li>Quality: We report NDCG@10 for ranking quality and Recall@100 to assess the retrieval ceiling.</li> <li>Performance: We measure Cost per Sample ($) and Time per Sample (ms) to visualize the trade-offs.</li> </ul> <h2 id="experiments">Experiments</h2> <p>We structure our analysis into two primary phases:</p> <h3 id="scaling-compute-in-query-expansion-qe">Scaling Compute in Query Expansion (QE)</h3> <p>We first evaluate the impact of the generator’s strength on the initial retrieval stage.</p> <ul> <li>Protocol: For every query q in the BRIGHT subsets, we generate q_{exp} using the four model variants (Flash-Lite, Flash-No-Think, Flash-Think, Pro).</li> <li>Retrieval: We perform BM25 retrieval using q_{exp}.</li> <li>Objective: To determine if “smarter” queries (generated by more expensive models) lead to higher Recall@100, providing a better candidate set for the reranker.</li> </ul> <div class="l-page"> <iframe id="ndcg-iframe" src="/2026/assets/html/2026-04-27-trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval/cost_vs_ndcg.html" frameborder="0" scrolling="no" width="100%" style="border: 0; overflow: hidden; display: block; height: 1px;"></iframe> </div> <h3 id="scaling-compute-in-reranking-rr">Scaling Compute in Reranking (RR)</h3> <p>We investigate three specific mechanisms for increasing compute during the reranking stage. For these experiments, we take the retrieval outputs from Phase 1 and apply varying reranking strategies.</p> <h4 id="increasing-reranking-depth-top-k">Increasing Reranking Depth (Top-k)</h4> <p>Hypothesis: Reranking more documents improves recall at the top of the list but increases cost linearly. Setup:</p> <ul> <li>Retrieval Basis: We use the candidate lists generated by all four QE settings from Phase 1.</li> <li>Reranker: Fixed to Gemini-2.5-Flash (Think).</li> <li>Variable: We rerank the top-k documents, where \(k \in \{10, 20, 50, 100\}\).</li> </ul> <h4 id="impact-of-thinking-in-reranking">Impact of “Thinking” in Reranking</h4> <p>Hypothesis: Enabling dynamic thinking tokens allows the model to resolve harder logical dependencies in document-query pairs. Setup:</p> <ul> <li>Retrieval Basis: All four QE settings from Phase 1.</li> <li>Reranking Depth: Fixed at k=100.</li> <li>Comparison: We measure the performance delta between Gemini-2.5-Flash (Think) and Gemini-2.5-Flash (No-Think). <h4 id="impact-of-model-strength-in-reranking">Impact of Model Strength in Reranking</h4> </li> </ul> <p>Hypothesis: Stronger base models (Pro) outperform optimized smaller models (Flash), even when the smaller models utilize thinking. Setup:</p> <ul> <li>Retrieval Basis: Fixed to the high-performing baseline of BM25 + QE (Gemini-2.5-Flash-Think).</li> <li>Reranking Depth: Fixed at k=100.</li> <li>Variable: We compare the four ranking models: Flash-Lite, Flash-No-Think, Flash-Think, and Pro. [Placeholder: We will insert a diagram here illustrating the matrix of experiments: QE variations on the X-axis vs. RR variations on the Y-axis.]</li> </ul> <script>
(function() {
  function resizeIframe() {
    var iframe = document.getElementById('ndcg-iframe');
    if (!iframe) return;
    try {
      var doc = iframe.contentDocument || (iframe.contentWindow && iframe.contentWindow.document);
      if (!doc) return;
      var newHeight = Math.max(
        doc.body ? doc.body.scrollHeight : 0,
        doc.documentElement ? doc.documentElement.scrollHeight : 0
      );
      if (newHeight && newHeight !== parseInt(iframe.style.height, 10)) {
        iframe.style.height = newHeight + 'px';
      }
    } catch (e) {
      // cross-origin guard (should not happen for same-site embeds)
    }
  }
  // Resize on key lifecycle events
  window.addEventListener('load', resizeIframe);
  window.addEventListener('resize', function() { setTimeout(resizeIframe, 100); });
  document.addEventListener('DOMContentLoaded', resizeIframe);
  var el = document.getElementById('ndcg-iframe');
  if (el) el.addEventListener('load', resizeIframe);
})();
</script> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-trade-offs-in-llm-compute-for-reasoning-intensive-information-retrieval.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-ai-evaluations-need-error-bars/">Why AI Evaluations Need Error Bars</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>