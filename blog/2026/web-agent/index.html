<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Computer Use Survey - A Visual Survey of Computer Use Agents | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="In recent years, AI systems operating on the web and in computer environments have become a major topic of interest for both academia and industry. The goal of this blog is to provide an interesting and interactive survey of historical and recent works on computer use agents. We define key terms used in the literature, catalogue the expansive list of environments and datasets, discuss the evolution of the methodologies, and assess both today’s landscape and possible paths forward."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/web-agent/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Computer Use Survey - A Visual Survey of Computer Use Agents",
            "description": "In recent years, AI systems operating on the web and in computer environments have become a major topic of interest for both academia and industry. The goal of this blog is to provide an interesting and interactive survey of historical and recent works on computer use agents. We define key terms used in the literature, catalogue the expansive list of environments and datasets, discuss the evolution of the methodologies, and assess both today’s landscape and possible paths forward.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Anonymous",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Computer Use Survey - A Visual Survey of Computer Use Agents</h1> <p>In recent years, AI systems operating on the web and in computer environments have become a major topic of interest for both academia and industry. The goal of this blog is to provide an interesting and interactive survey of historical and recent works on computer use agents. We define key terms used in the literature, catalogue the expansive list of environments and datasets, discuss the evolution of the methodologies, and assess both today’s landscape and possible paths forward.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#what-is-computer-use">What is Computer Use</a> </div> <div> <a href="#environments-and-datasets">Environments and Datasets</a> </div> <div> <a href="#models-and-methods">Models and Methods</a> </div> <ul> <li> <a href="#pre-llm-work">Pre-LLM Work</a> </li> <li> <a href="#computer-use-lm-agents">Computer Use LM Agents</a> </li> <li> <a href="#base-models-for-llm-agents">Base Models for LLM Agents</a> </li> <li> <a href="#multi-task-agent-foundation-models">Multi-task Agent Foundation Models</a> </li> <li> <a href="#firmament">Firmament</a> </li> <li> <a href="#proprietary-systems">Proprietary Systems</a> </li> </ul> <div> <a href="#discussion">Discussion</a> </div> <ul> <li> <a href="#areas-for-improvement">Areas for Improvement</a> </li> <li> <a href="#safety-ethical-concerns">Safety/Ethical Concerns</a> </li> <li> <a href="#agentic-web">Agentic Web</a> </li> <li> <a href="#but-why-computer-use-agents">But Why Computer Use Agents?</a> </li> </ul> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In the last few years, and particularly in the most recent past year, AI systems which operate on the web and in computer environments have become a significant topic of study, for both academia and industry. If readers skim the citations in this review, they will likely notice that many of the works are extremely recent, most within the past year.</p> <p>The goal of this survey is to organize much of the historical and recent work on computer use agents. First, we define exactly what we mean by “Computer Use” in <a href="#what-is-computer-use">What is Computer Use</a>. We then define and categorize different environments, datasets and evaluations for computer use agents in <a href="#environments-and-datasets">Environments and Datasets</a>. Next, we discuss the methodological work in this area in <a href="#models-and-methods">Models and Methods</a> focusing in particular on the recent trend of “LLM Agents” and provide an accessible explanation of this class of systems and trends within this research. Finally, we discuss ongoing trends, areas for improvement and possible safety and ethical concerns brought about by this research in <a href="#discussion">Discussion</a>.</p> <p>So what has shaped this sudden interest in computer use Agents in the first place? What goals are academic and industry researchers trying to serve? The first explanation is that the advent of extremely capable language and vision language models (LLMs / VLMs) has made the goal of autonomous computer use agents suddenly quite plausible. These models, trained now on trillions of tokens scraped from the web, are incredibly powerful few- and zero-shot learners <d-cite key="brown2020language,kojima2022large"></d-cite> able, with little or no new training data, to be generally proficient at many language tasks, making them adaptable to many different problems.</p> <p>Computer use is not only difficult in terms of being a long-horizon sequential decision-making problem, but also linguistically challenging and knowledge-laden. Before these large-scale models, language problems required models to be trained from scratch, language understanding was often limited and brittle and few-shot reasoning was almost unheard of. This made it hard to do these computer use tasks. Take a popular web use task example: booking an airline ticket. To solve the problem, you first have to be able to precisely understand a user’s request: what airline they prefer, when they want to travel, etc. The agent also must have quite a bit of both commonsense and specific knowledge about airline ticket purchasing to accomplish this. The agent needs to not only understand the request and know how to navigate the airline website, it also needs to understand things like “I should not book a $10k ticket.” With these capable LLMs, we not only have a backbone which has general capability that can start to make progress on the task, but also have these essential commonsense capabilities to make computer use agents possible.</p> <p>While the precise timing of this work can be explained by the arrival of these capable models, we should also ask what people are trying to accomplish with computer use agents. Many papers point to automating routine, boring or time consuming tasks as a motivation. Many works list automation of tedious tasks or enhancing user experiences as a motivation. The other common motivations are accessibility; making computers and the Internet more generally accessible to those with disabilities. We elaborate on this further in Discussion.</p> <hr> <h2 id="what-is-computer-use">What is Computer Use</h2> <p>First, we define what we mean here by a “Computer Use” AI system or computer use agent. By this, we mean an agent that interacts dynamically with a computer system or web interface. In the MDP formulation, an agent is a decision maker that observes an environment and takes actions <d-cite key="sutton2018reinforcement"></d-cite>.</p> <p>As we discuss later on in <a href="#environments-and-datasets">Environments and Datasets</a>, we can define a computer use environment in a number of different ways, from an environment which is a simulation or Virtual Machine instance of an operating system, an environment which contains the text of a website(s) which can be navigated, an environment which emulates a browser to navigate and act on a live or cached version of the web or any portion or simulation of a computer that can be operated like an environment. These environments allow for any number of tasks from navigation to question answering. This definition suggests that the thing we care about is looking at works which create agents or environments with the goal of creating AI which can operate on computer systems in a manner similar to the ways humans do. This can actually incorporate many kinds of AI systems, but the most recent version of these which we discuss in detail in Models and Methods is the LLM or VLM agent on computer use environments.</p> <iframe class="l-page" src="/2026/assets/html/2026-04-27-web-agent/definitions.html" width="100%" height="480" frameborder="0" style="border-radius: 12px; margin: 1.5rem 0;"></iframe> <hr> <h2 id="environments-and-datasets">Environments and Datasets</h2> <p>The landscape of computer use research spans diverse environments, from desktop operating systems to web browsers and mobile platforms. Each environment presents unique challenges and opportunities for developing autonomous agents. Below, we present an interactive exploration of the major datasets and benchmarks that have shaped this field.</p> <p>We divide the datasets into several broad categories. Computer and OS Control describes environments which give more or less full simulated access to a computer or operating system. Web Control and Navigation describes environments where access is mostly to the web (live or statically defined pages). Text-Only Web Environments describes specifically text-only versions of web environments. Web QA and Classification are datasets which ask static questions or classifications on web pages as opposed to controllable environments. Finally we include Coding and Assistant Tasks for coding tasks and the broad category of assistant tasks using computer use elements.</p> <p>We realize that the category “web browsing” is somewhat vague. It can include tasks involved with changing the settings of a browser such as Chrome and tasks requiring finding information on the web <d-cite key="bonatti2024windows"></d-cite>. Similarly, even what is meant by the web can be very different. This can mean access to a few websites such as in VisualWebArena <d-cite key="koh2024visualwebarena"></d-cite> or access to the entire web This list is by no means complete and likely by the time we hit publish there will be many more datasets and environments.</p> <iframe class="l-page" src="/2026/assets/html/2026-04-27-web-agent/observatory.html" width="100%" height="750" frameborder="0" style="border-radius: 12px; margin: 1.5rem 0;"></iframe> <p>While going through all of these environments and datasets, we had a number of observations we thought were important to discuss.</p> <p>First, unlike with many large-scale datasets in other fields such as ImageNet <d-cite key="imagenet2009"></d-cite>, these datasets are often entirely annotated by the researchers themselves (usually graduate students). This often makes scaling these datasets quite challenging and puts more burden on students to create these datasets. Specialized tools such as AgentNet <d-cite key="agentnetdocs"></d-cite> have been developed to try to aid in the annotation process, but the work is still highly specialized.</p> <p>Second, we found it actually quite difficult to count the number of tasks versus number of instances. In many datasets, there is no real distinction made: each query to a web agent is considered a task and there is exactly one instance of that task. Some of these tasks can be similar or overlap, but there is no categorization of similar tasks or instances. In general, we try to report the size of the dataset in terms of the number of queries unless the dataset has a firm distinction between a task and instance, in which case we report both.</p> <p>Finally, we note that, especially recently, there are an enormous number of datasets in this area, each with their own data, methodology of collection and input and action spaces. One challenge the field will have going forward is having consistent evaluations and deciding how we are making progress in the field given this. Several papers now have discussed various issues with evaluation <d-cite key="zhu2025establishing,kapoor2025holistic,kapoor2024ai,xue2025illusion,wang2025computer"></d-cite> with suggestions such as evaluating cost and eliminating agent shortcuts <d-cite key="kapoor2024ai"></d-cite>, standardized evaluation harnesses <d-cite key="kapoor2025holistic"></d-cite>, or by making an arena to directly compare computer agents with humans<d-cite key="wang2025computer"></d-cite>.</p> <hr> <h2 id="models-and-methods">Models and Methods</h2> <p>In this section we discuss the models and methodologies used in the field, from the pre-LLM era (Pre-LLM Work) to Computer Use LM Agents (Computer Use LM Agents) where we discuss Base Models (Base Models) and the wrappers or “Firmament” around them (Firmament), and proprietary systems and products (Proprietary Systems).</p> <h3 id="pre-llm-work">Pre-LLM Work</h3> <p>Inherent to this problem is taking repeated actions over time on some representation of a computer interface or website. Naturally, then, works have framed their methodology through the agent/environment paradigm discussed in What is Computer Use. Let’s journey through time to see how these approaches evolved:</p> <iframe src="/2026/assets/html/2026-04-27-web-agent/timemachine.html" width="100%" height="580" frameborder="0" style="border-radius: 12px; margin: 1.5rem 0;"></iframe> <p>Other papers explored various ideas such as curriculum approaches <d-cite key="gur2021environment"></d-cite>, curiosity RL <d-cite key="zheng2021synergistic"></d-cite>, hierarchical RL <d-cite key="gur2018learning"></d-cite>.</p> <h3 id="computer-use-lm-agents">Computer Use LM Agents</h3> <p>As LLMs and VLMs began to scale on massive amounts of web data and thus became far more capable on language and vision tasks <d-cite key="achiam2023gpt,kojima2022large,alayrac2022flamingo,brown2020language,hoffmann2022training"></d-cite>, approaches directly using these models as a backbone began to dominate. This line of work is characterized by frameworks which operate by utilizing LLM’s ability to call tools <d-cite key="schick2023toolformer"></d-cite> to have the LLM call external tools and capabilities (such as OCR modules, episodic memory modules) and critically to call itself as a tool.</p> <p>These papers are often called “agents” or “agentic” to mean that the system can take repeated actions over time by creating a finite state machine construction in such a way that the system is able to repeat a cycle of reading the task query, planning, interpreting the current input and taking actions in the environment. This bears some resemblance to the sense-think-act cycle in robotics <d-cite key="siegel2003sense"></d-cite>. To see this more clearly, see the figure below which shows a typical control flow diagram for an LLM agent on computer or web control tasks.</p> <div class="pdf-figure l-page" style="margin: 1.25rem 0; text-align: center;"> <img src="/2026/assets/img/2026-04-27-web-agent/ComputerUse.png" alt="LLM Agent Control Flow Diagram showing control passing between differently prompted versions of the LLM/VLM and external components" style="max-width: 100%; width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);" loading="lazy"> <div style="margin-top: 0.5rem; color: #6c757d; font-size: 0.95rem;"> Typical control flow diagram for an LLM agent showing how control passes between differently prompted versions of the LLM/VLM and external components. </div> </div> <p>We can think of the diagram as showing how control passes between differently prompted versions of the LLM or VLM (a planner or step planner, an input interpreter, an action decider as well as other kinds of modules such as a reflect module) and external/non LLM components. The user query (i.e. a question or goal the agent is asked to perform) is taken as input to an LLM prompted to act as a planning module. After an initial plan, control is passed to the LLM prompted to plan each step, which calls other modules to load and interpret the input and decide the action.</p> <p>The input can either be a direct image of the environment, or some combination of the image observation, OCR outputs, parsed HTML code or DOM elements or other representations from the environment. Then the action is decided (or no action is taken and the step is replanned) and passed to a module which interprets the LLM output string as an environment action. This can either be a direct API call (e.g. click(X, Y) or type(str)) or a custom action space which is then parsed and interpreted into a corresponding environment action. This loop continues until the planner decides that the agent has fulfilled the user query and action terminates. We call such agents <strong>Computer Use LM Agents</strong>.</p> <p>We can say that what all of these have in common are:</p> <ol> <li>An LLM or VLM backbone as well as any software that handles API calls if using an external service.</li> <li>What we will here call the <strong>firmament</strong> which is the finite state machine that decides the control flow and the prompting used for different LLM components.</li> <li>External tools or modules used.</li> </ol> <p>The differences then between different methods which use this framework can be differentiated in several ways including:</p> <ol> <li>Differences in base models including whether it is a VLM or LLM using only text observations, and whether and how the base model is trained.</li> <li>Differences in the firmament in the exact control flow used, which external modules are called, how the input and output spaces are represented.</li> <li>Whether there is any test-time expansion of the base model and how that is handled.</li> <li>The input can be a direct image, OCR outputs, parsed HTML, DOM elements, or other representations.</li> <li>The action is decided and passed to a module which interprets the LLM output string as an environment action.</li> <li>This loop continues until the planner decides the agent has fulfilled the user query.</li> </ol> <p>These modules are not all necessary or exhaustive (some methods include reflect operations and others have a simplified control flow). Different methods will add unique modules such as episodic memory <d-cite key="murty2024bagel,sarch2024ical"></d-cite>, or web knowledge <d-cite key="agashe2024agent"></d-cite>, or even a code interpreter <d-cite key="wang2023cogvlm"></d-cite>. The thing that all of these agents have in common however is the finite state machine which allows the LLM to be called in a loop with different prompts for each step, the invoking of tools, and the interaction with the web environment through some representation of the input and actions.</p> <h3 id="base-models-for-llm-agents">Base Models for LLM Agents</h3> <p>Possibly the most important design decision that can be made is the choice of base model and whether it is fine-tuned for the task. This is generally decided by the general ability of the model as well as practical considerations such as cost or ability to run or train on the researcher’s available hardware.</p> <h4 id="fixed-base-models">Fixed Base Models</h4> <p>The first category of work does not train the LLM backbone, but relies on innovations in the firmament components or adding new modules such as OCR. Much of the early LLM agent work on web agents uses prompted only GPT-4(V) <d-cite key="achiam2023gpt"></d-cite> (e.g <d-cite key="sarch2024ical,zhenggpt"></d-cite>) as at the time of its release it was one of the most capable models, was easily callable through an API and had visual input. Later much of the non-training work switched over to GPT-4o <d-cite key="hurst2024gpt"></d-cite> due to greater affordability and performance <d-cite key="agashe2024agent"></d-cite>. Claude-3 and Claude-3.5 <d-cite key="claude3"></d-cite> (e.g. in <d-cite key="agashe2024agent"></d-cite>) and GPT-4-Turbo (in <d-cite key="abuelsaad2024agent"></d-cite>) are other popular choices.</p> <h4 id="supervised-finetuning">Supervised Finetuning</h4> <p>Because base VLMs and LLMs are trained on typical web images and text, certain text and images may be out of distribution and models will struggle on these tasks <d-cite key="rahmanzadehgervi2024vision"></d-cite>. This is often the case for web tasks as well, so several works have tried to train models for this task (e.g xLAM <d-cite key="zhang2024xlam"></d-cite>). CogAgent <d-cite key="hong2024cogagent"></d-cite> for example adds a high-resolution image stack to CogVLM <d-cite key="wang2023cogvlm"></d-cite> and finetunes on a large-scale dataset of GUI and OCR tasks. Similarly, OS-ATLAS <d-cite key="wu2024atlas"></d-cite> creates a large GUI grounding corpus and finetune Qwen2-VL <d-cite key="wang2024qwen2"></d-cite> and InternVL-2 <d-cite key="chen2024far"></d-cite> to better perform on these tasks.</p> <h4 id="rlexploration-finetuning">RL/Exploration Finetuning</h4> <p>Similarly, there have been many works looking at automatic exploration or web and OS environments for finetuning <d-cite key="ou2024synatra,patel2024large,trabucco2025insta,gandhi2025go,yangself,ma2025autodata,xiao2025ui"></d-cite>. These methods interact with the OS or Web environment directly to then update the model. In <d-cite key="patel2024large"></d-cite>, traces in the environment are created synthetically by either using the dataset training set or prompting an LLM for a task, using the base LLM to generate actions in the environment, then using another LLM to self-critique to determine if the task was accomplished successfully. Similarly <d-cite key="murty2024nnetscape"></d-cite> generates synthetic traces by first starting with an environment trace and then labeling the task in hindsight. In all of these works, the synthetic datasets are used for supervised finetuning of a base LLM model for the task.</p> <p>WEBRL <d-cite key="qi2024webrl"></d-cite> adopted a full RL training loop for exploring and fine-tuning base models for web agents. Like other works, it uses a base LLM to generate instructions, executes using its current model, and uses a critic LLM to determine success or failure which is then used as a reward for reinforcement learning. It also employs KL smoothing and a replay buffer only keeping successful trajectories to avoid catastrophic forgetting. For datasets with direct environment rewards, other papers are able to directly finetune agents with RL<d-cite key="lai2025computerrl,vattikonda2025train"></d-cite>.</p> <h3 id="multi-task-agent-foundation-models">Multi-task Agent Foundation Models</h3> <p>Most recently, a very popular approach has been to build new foundation models which include web agent data <d-cite key="szot2025multimodal,yang2025magma,liuvisualagentbench"></d-cite>. In <d-cite key="szot2025multimodal"></d-cite> they jointly train on a number of tasks including computer use, simulated robotics and games. This work finetunes a multimodal language model on embodied tasks, both with supervised fine-tuning and RL, making use of a multi-embodiment action tokenizer. Similarly in Magma<d-cite key="yang2025magma"></d-cite> they train a foundation model on a variety of multimodal environments including computer use which is specifically trained for planning in embodied settings. Others have trained foundation models exclusively for GUI agents by training across many different GUI environments <d-cite key="huang2025spiritsight"></d-cite>.</p> <h3 id="firmament">Firmament</h3> <p>Another critical part of web agent architectures is the control flow state machines and prompting techniques (which we here call the <strong>firmament</strong> but others might call scaffolding). This incorporates things such as prompting strategies such as prompting agents to give responses in code <d-cite key="wangexecutable"></d-cite>, adding modules such as self reflection, episodic memory of related examples or other information synthesized from past experience <d-cite key="lutz2024wilburadaptiveincontextlearning,murty2024bagel,sarch2024ical,wang2024agentworkflowmemory,limobileuse,wang2025inducing"></d-cite>, hierarchical planning modules, online web search and narrative planning <d-cite key="agashe2024agent"></d-cite>.</p> <p>One interesting area is in incorporating test-time search into the agent <d-cite key="putta2024agent,Yu2024ExACTTA"></d-cite>. <d-cite key="koh2024tree"></d-cite> does this by doing an A*-like exploration of web actions, picking a state to expand at each step and considering the possible next states the agent could land in. Similarly, ExACT <d-cite key="Yu2024ExACTTA"></d-cite> incorporates Reflective Monte Carlo Tree Search (R-MCTS), a variation on MCTS at test time, using reflection to evaluate to estimate state values. Similarly WebPilot <d-cite key="zhang2024webpilotversatileautonomousmultiagent"></d-cite> adopts an MCTS-like approach, using multiple LLMs acting in different capacities as Explorer, Verifier, Appraiser, and Controller. <d-cite key="gu2024your"></d-cite> does this with model-based planning rather than a full search in the environment. Other works such as <d-cite key="hu2025owl"></d-cite> will similarly employ explicit multi-agent systems to break down and execute different parts of a task.</p> <p>Another important aspect of all of these frameworks is how they specifically interact with the Web or OS environment itself. For the representation of the input, there has been great experimentation ranging from set of marks <d-cite key="yang2023set"></d-cite>, where an id is overlaid on the UI element of interest <d-cite key="bonatti2024windows,xie2024osworld"></d-cite>, accessibility trees, built in features in operating systems and browsers which tag UI elements with text <d-cite key="cao2024spider2,koh2024visualwebarena,pan2024autonomous,pan2024webcanvas"></d-cite>, HTML <d-cite key="cao2024spider2,qi2024webrl"></d-cite> often reduced or filtered in some way such as by using a Document Object Model (DOM) tree <d-cite key="deng2024mind2web,kapoor2024omniact,koh2024visualwebarena"></d-cite> or even the raw screenshots <d-cite key="gou2024navigating,hong2024cogagent,xie2024osworld"></d-cite>. Often several of these input types will be combined together.</p> <p>The wide variety of inputs used by different models can often create issues with evaluations and benchmarking models. As shown in Table 5 of OSWorld <d-cite key="xie2024osworld"></d-cite> (reproduced below), the same backbone model prompted differently and using different representations of the same input can have vastly different levels of performance. Ideally, comparisons would be made between models using the same assumptions about the input space.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-web-agent/inputtype-480.webp 480w,/2026/assets/img/2026-04-27-web-agent/inputtype-800.webp 800w,/2026/assets/img/2026-04-27-web-agent/inputtype-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-web-agent/inputtype.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="Table 5 from OSWorld showing different performance based on input type" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Table 5 from OSWorld showing greatly different performance based on input type.</figcaption> </figure> <p>Similarly, methods have a wide variety of ways of dealing with the action spaces in the environment. In the earliest works, the action space was at the lowest level, made up of atomic mouse (x, y) positions and click actions <d-cite key="pmlr-v70-shi17a"></d-cite>, or click and typing actions <d-cite key="liu2018reinforcement"></d-cite>. Some works such as UGround <d-cite key="gou2024navigating"></d-cite> still use these action spaces, while other works have tried to stay grounded to the visual space without coordinates<d-cite key="wu2025gui"></d-cite>. For the most part however, when language model agents became the more dominant paradigm, the action space often changed to better match with the language output of LLMs. Works such as AutoWebGLM <d-cite key="lai2024autowebglm"></d-cite> or Agent S <d-cite key="agashe2024agent"></d-cite> or many others <d-cite key="abuelsaad2024agent,koh2024visualwebarena,Yu2024ExACTTA"></d-cite> allow for the language model to specify actions in templated language. In other words, it treats the computer action space as a tool <d-cite key="schick2023toolformer"></d-cite>. It still allows for clicks and typing, but rather than specify an x,y coordinate, it specifies click(id) with the id of the UI element to click on. These works also define a set of pre-defined web actions such as jump_to(url, newtab) which navigates a particular tab to a specified url so that some actions can be taken without specifying every mouse and keyboard action required.</p> <p>With these custom action spaces, prompts are used to tell the LLM the set of available actions and some kind of parser or interpreter is used to translate these language-specified actions to the environment. The input and action spaces are often tightly linked. For instance in works such as <d-cite key="zhenggpt"></d-cite>, the UI elements are labeled with ids in a way similar to set-of-marks and those same ids are then used in the action space so the agent can interact with those same elements.</p> <h3 id="proprietary-systems">Proprietary Systems</h3> <p>It is also worth mentioning that Web and OS control agents are not only an academic topic, but an emerging product area for AI companies. Google DeepMind has announced Project Mariner <d-cite key="projectmariner"></d-cite> and later Gemini 3.0 with web browsing<d-cite key="pichai2025gemini3"></d-cite> , OpenAI with Operator <d-cite key="computeruseagent"></d-cite> and Anthropic with Computer Use <d-cite key="anthropicwebblog"></d-cite> and several others <d-cite key="browseruse2024,yutori2025navigator,cohere2025agentstudio"></d-cite>. Unfortunately, some of these models are in limited release, either behind trusted user groups or premium subscriptions, making evaluation difficult.</p> <p>Some of these projects have reported numbers on some popular computer use benchmarks, but none of these projects have released papers or detailed technical reports, so details about how the agents work and how the evaluations were conducted are not known. Based on the limited information released, it seems apparent that these agents correspond with the line of work described in Computer Use LM Agents, but other details such as how the base models are fine-tuned, data used for training etc are not known. Ultimately, these black-box releases are very relevant to the interest in computer use agents, but cannot be easily benchmarked or relied upon to contribute to the academic literature. This will likely continue to be a tension going forward, as it has been in the broader literature of LLMs.</p> <hr> <h2 id="discussion">Discussion</h2> <h3 id="areas-for-improvement">Areas for Improvement</h3> <p>In the next table, we (non-exhaustively) look at the performance of different methods on two popular computer/web agent tasks, WebArena <d-cite key="zhouwebarena"></d-cite> and OSWorld <d-cite key="xie2024osworld"></d-cite>. (Note, we do not include proprietary systems with unpublished methodologies).</p> <table> <thead> <tr> <th>Base Models</th> <th>WebArena-Lite</th> <th>WebArena</th> <th>OSWorld</th> </tr> </thead> <tbody> <tr> <td>GPT-3.5 Turbo <d-cite key="brown2020language"></d-cite> </td> <td>–</td> <td>6.2</td> <td>–</td> </tr> <tr> <td>GPT-4 <d-cite key="achiam2023gpt"></d-cite> </td> <td>–</td> <td>14.4</td> <td>12.24</td> </tr> <tr> <td>GPT-4o <d-cite key="hurst2024gpt"></d-cite> </td> <td>13.9</td> <td>13.1</td> <td>11.36</td> </tr> <tr> <td>LLAMA2-7B <d-cite key="touvron2023llama"></d-cite> </td> <td>–</td> <td>1.2</td> <td>–</td> </tr> <tr> <td>LLAMA2-70B <d-cite key="touvron2023llama"></d-cite> </td> <td>–</td> <td>0.6</td> <td>–</td> </tr> <tr> <td>Llama3.1-Instruct <d-cite key="grattafiori2024llama"></d-cite> </td> <td>4.8</td> <td>–</td> <td>–</td> </tr> <tr> <td>ScribeAgent + GPT-4o <d-cite key="shen2024scribeagent"></d-cite> </td> <td>53</td> <td>–</td> <td>–</td> </tr> <tr> <td>AgentSymbiotic <d-cite key="zhang2025symbiotic"></d-cite> </td> <td>52.1</td> <td>–</td> <td>–</td> </tr> <tr> <td>WebRL <d-cite key="qi2024webrl"></d-cite> </td> <td>49.1</td> <td>–</td> <td>–</td> </tr> <tr> <td>Learn-by-Interact <d-cite key="su2025learn"></d-cite> </td> <td>48</td> <td>–</td> <td>–</td> </tr> <tr> <td>AgentOccam-Judge <d-cite key="yang2024agentoccam"></d-cite> </td> <td>45.7</td> <td>–</td> <td>–</td> </tr> <tr> <td>NNetscape navigator <d-cite key="murty2024nnetscape"></d-cite> </td> <td>–</td> <td>7.2</td> <td>–</td> </tr> <tr> <td>AutoWebGLM <d-cite key="lai2024autowebglm"></d-cite> </td> <td>–</td> <td>18.2</td> <td>–</td> </tr> <tr> <td>AWM <d-cite key="wang2024agentworkflowmemory"></d-cite> </td> <td>–</td> <td>35.5</td> <td>–</td> </tr> <tr> <td>WebPilot (GPT-4o) <d-cite key="zhang2024webpilotversatileautonomousmultiagent"></d-cite> </td> <td>–</td> <td>37.2</td> <td>–</td> </tr> <tr> <td>ExACT <d-cite key="Yu2024ExACTTA"></d-cite> </td> <td>–</td> <td>–</td> <td>19.39</td> </tr> <tr> <td>Agent S <d-cite key="agashe2024agent"></d-cite> </td> <td>–</td> <td>–</td> <td>20.58</td> </tr> <tr> <td>OpenCUA <d-cite key="opencua2025"></d-cite> </td> <td>–</td> <td>–</td> <td>34.8</td> </tr> <tr> <td>Seed1.5-VL <d-cite key="guo2024seed"></d-cite> </td> <td>–</td> <td>–</td> <td>40</td> </tr> </tbody> </table> <p><em>Progress on popular benchmarks WebArena <d-cite key="zhouwebarena"></d-cite> and OSWorld <d-cite key="xie2024osworld"></d-cite>. Scores represent accuracy percentages.</em></p> <p>One observation to make here is that great improvements have been made on all of these tasks, but that accuracies on these tasks are still quite low, all below 60%, and many of the higher numbers are quite recent! And more recent benchmarks such as <d-cite key="xu2024theagentcompany"></d-cite> are even more difficult. So what things might be holding models back?</p> <h4 id="planning">Planning</h4> <p>One area where computer use agents often get stuck is planning. A common issue is that LLM agents will sometimes get stuck in a state and repeatedly try to perform some action unsuccessfully or being distracted by an irrelevant web element. An analysis in Figure 3 of WebRL <d-cite key="qi2024webrl"></d-cite> for instance shows that this is one of the most common issues for many baseline agents.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-web-agent/errors-480.webp 480w,/2026/assets/img/2026-04-27-web-agent/errors-800.webp 800w,/2026/assets/img/2026-04-27-web-agent/errors-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-web-agent/errors.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="Figure 3 from WebRL showing breakdown of error types in web agents" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3 from WebRL showing the most common error types for baseline agents, with planning-related issues being predominant.</figcaption> </figure> <p>As mentioned earlier, many works try to solve this issue with either using an LLM as a “planner”, relying on the LLM itself to recognize that it is stuck, or using some test-time search to escape loops <d-cite key="Yu2024ExACTTA,koh2024tree"></d-cite>.</p> <h4 id="inputoutput-representation">Input/Output Representation</h4> <p>Another issue is environment grounding, which can take many forms. For agents which use the raw pixel input of the environment, this is a problem of visual grounding (see <d-cite key="xiao2024towards"></d-cite> for a general survey): literally, can the model understand what is in an image and where. And while many methods use non-visual representations of the current web or computer state (see Firmament), they still have the same grounding problem, but in an input space of HTML (or other non-visual space).</p> <p>A common qualitative issue mentioned in many papers is important UI elements not being recognized by the system, either due to a specific failure in the tool representing the input (e.g. failures of the non-visual tools used to represent the input such as Set-of-Marks <d-cite key="yang2023set"></d-cite>). Sometimes accessibility trees or DOMs for instance, contain errors or missing labels <d-cite key="gou2024navigating"></d-cite>. Works which specifically use image inputs often have the problem that the VLMs were not adequately trained on web images <d-cite key="hong2024cogagent"></d-cite> and a well-known issue in VLMs is out-of-domain image understanding <d-cite key="rahmanzadehgervi2024vision"></d-cite>. Ultimately, these issues may come down to basic issues of visual recognition, which some have used to suggest specific fine-tuning for better visual recognition of GUIs <d-cite key="hong2024cogagent,gou2024navigating,he2024webvoyager,qinghong2024showui,gilley2025bitterlesson"></d-cite>.</p> <h4 id="lack-of-training-data">Lack of Training Data</h4> <p>One obvious issue (which is implicit in bad grounding and planning) may be that many base LLMs or VLMs are simply not well aligned to the task because the base models are inadequately trained on this distribution. Attempts have been made to create larger datasets <d-cite key="deng2024mind2web"></d-cite>, for training, especially for visual finetuning <d-cite key="hong2024cogagent,liu2024harnessing,liu2024visualwebbench,wu2024atlas"></d-cite>. However, one major issue, which can be generally seen in long-horizon RL problems is that it can require a lot of data, especially for long trajectories, and that long trajectories can cause policies to suffer a distribution shift from the training trajectories, leading to poor performance <d-cite key="ross2011reduction"></d-cite>. As discussed in Base Models, newer methods have often incorporated explicit exploration using RL or other exploration methods to gather more training trajectories. In particular, WebRL <d-cite key="qi2024webrl"></d-cite> has great success directly finetuning through reinforcement learning on the live environment with policy gradients.</p> <h4 id="long-horizon-problems-are-hard">Long-horizon Problems are Hard</h4> <p>Another answer is simply that many web or computer tasks are difficult because they are fundamentally a long-horizon action problem. We have a description of some task or problem in language and we expect agents to take many consecutive steps before being able to resolve the query. In the Reinforcement Learning literature, there is a well-known issue of having “sparse rewards” facing agents which train on these rewards which makes it difficult for agents to explore properly to reliably find the rewards <d-cite key="hare2019dealing"></d-cite>. Another related problem is the Credit Assignment problem which is well known in the RL literature and was described by Marvin Minsky: “In applying such methods to complex problems, one encounters a serious difficulty-in distributing credit for success of a complex strategy among the many decisions that were involved” <d-cite key="minsky1961steps"></d-cite>. In a greater sense, to ask why complex multi-step web or computer tasks are difficult is asking the same questions that practitioners of AI have been asking since the founding of the field.</p> <h3 id="safetyethical-concerns">Safety/Ethical Concerns</h3> <p>The emergence of more and more capable computer use agents has raised a number of new ethical and safety issues. One prominent issue is the possibility of Web Agents for malicious use. Hard-coded web agents (often colloquially called “bots”) have already been cited as a major issue with so-called “Scalper bots” being used to buy limited-quantity items and resold at massive markups <d-cite key="michigan2021sneaker"></d-cite>. With the possibility of even more sophisticated bots, these concerns are even more acute. For instance, more adaptable and intelligent bots could be able to override mechanisms meant to deter these behaviors such as CAPTCHAs <d-cite key="rawles2024androidinthewild,xie2024osworld"></d-cite>. And as web agents become more capable, they could help automate more web activities used for fraudulent or malicious purposes.</p> <p>Another concern is privacy and security. Computer use agents acting on behalf of users have access to extremely sensitive information such as users’ profiles, passwords, financial transactions, social media messages and many other kinds of data <d-cite key="pan2024autonomous,zhenggpt"></d-cite>. This information could accidentally or even maliciously be accessed by the agent and sent to others. Mistakes in actions could, for instance, cause web agents to accidentally send un-encrypted passwords to others by email or chat websites. Web agents could also be deliberately designed to steal such information. For a more thorough discussion of the ethical issues involved in agents, the blog post “AI Agents Are Here. What Now?” <d-cite key="kiela2024ai"></d-cite> provides a detailed breakdown.</p> <h3 id="agentic-web">Agentic Web</h3> <p>A recurring discussion point in the philosophy of computer use agents is that current interfaces are built for humans, not autonomous agents. This has motivated arguments for rearchitecting the web for agents rather than forcing agents to reverse-engineer the human-centric interfaces. Stemming from this view, the <em>Agentic Web</em> <d-cite key="yang2025agenticwebweavingweb"></d-cite> is framed as a new phase of the internet, in which autonomous agents transact, coordinate, and negotiate on behalf of users over infrastructures explicitly designed for agentic activities.</p> <p>On a similar note, Agentic Web Interfaces (AWI) <d-cite key="lu2025buildwebagentsagents"></d-cite> discusses an interaction layer optimized for perception and action with the web interface. Designing AWIs around principles of safety, efficiency, and standardization reframes core challenges of web agents, <em>e.g.</em>, DOM parsing and UI understanding, as shared responsibilities of web developers, standards bodies, and agent designers. Yet the argument for building web agents still remains in the era of agentic web, as we may need web agents to navigate through the older pre-agentic websites.</p> <h3 id="but-why-computer-use-agents">But Why Computer Use Agents?</h3> <p>As discussed in the Introduction, the two most common goals for Computer Use Agents are related to <strong>automation</strong> and <strong>accessibility</strong>. The automation motivation is compelling - as described in WebGPT <d-cite key="nakano2021webgpt"></d-cite>, they envision: “Picture this scenario: You type in a task description, then relax and enjoy a cup of coffee while watching tasks like booking tickets online, conducting web searches, managing files, and creating PowerPoint presentations get completed automatically.” This vision of seamless task automation drives much of the current research effort.</p> <p>The other motivation is accessibility <d-cite key="li2024effects,bonatti2024windows"></d-cite>, with researchers stating that this line of work will “make digital devices more accessible” <d-cite key="pan2024autonomous"></d-cite>. Several potential issues emerge however. One concern is that many current agents rely implicitly or explicitly on accessibility features already built into operating systems or the web to function <d-cite key="cao2024spider2,koh2024visualwebarena,pan2024autonomous,pan2024webcanvas"></d-cite>. Unfortunately, as mentioned in <d-cite key="gou2024navigating"></d-cite>, 95.9% of webpages contain errors in accessibility including missing alt text in images or missing form input labels at around 57 errors per page <d-cite key="webaim"></d-cite>. Relying on web accessibility to be correct is an unfortunate issue for those who use these features directly and makes it more difficult for web agents to help these same people if they rely on the same features being correct.</p> <p>There can often be a large disconnect between the use cases, datasets and models thought up by researchers (often mostly sighted people) and those actually important and useful to blind and visually impaired people. As noted in <d-cite key="gurari2018vizwiz"></d-cite> who studied this in the context of Visual Question Answering, the kinds of questions actually asked by blind users varied significantly from prior VQA datasets. Similarly, we might expect that the use cases of the visually impaired for online agents or assistants might be significantly different from those imagined by current lines of research. Future work which seeks to improve accessibility must similarly to <d-cite key="gurari2018vizwiz"></d-cite> use the actual data and requirements from the blind to be genuinely helpful as the overall capabilities of these systems improves.</p> <p>Nevertheless, the potential certainly exists in the future for these agents to be useful for accessibility, not just by those with visual impairment, but people with severe neuropathy or motor control symptoms as well as people with cognitive or memory issues which might make common web tasks difficult. Computer use agents will first have to drastically improve in performance and researchers will have to work with health researchers, providers and affected people to develop truly useful accessibility features using this technology.</p> <iframe class="l-page" src="/2026/assets/html/2026-04-27-web-agent/carousel.html" width="100%" height="520" frameborder="0" style="border-radius: 12px; margin: 1rem 0;"></iframe> <h2 id="acknowledgments">Acknowledgments</h2> <p>This survey was adapted from the original interactive web version created by the authors. We tried to take advantage of the medium of the web specifically to make the survey a bit different and more interactive for readers. In addition, certain parts of the survey, such as the list of datasets would simply not have worked in a normal linear blog. Claude Sonnet 4 was used to help build the website.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-web-agent.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/witness-problem/">The Witness Problem in Multi-Agent Cooperation</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>