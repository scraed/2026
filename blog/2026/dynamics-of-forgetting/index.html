<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dynamics of Forgetting | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We analyze catastrophic forgetting through spectral decompositions of weights and updates, revealing when optimization refines existing circuits versus builds interfering new ones. Leveraging this, we design spectral techniques that suppress destructive update components while preserving structure."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/dynamics-of-forgetting/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Dynamics of Forgetting",
            "description": "We analyze catastrophic forgetting through spectral decompositions of weights and updates, revealing when optimization refines existing circuits versus builds interfering new ones. Leveraging this, we design spectral techniques that suppress destructive update components while preserving structure.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Dynamics of Forgetting</h1> <p>We analyze catastrophic forgetting through spectral decompositions of weights and updates, revealing when optimization refines existing circuits versus builds interfering new ones. Leveraging this, we design spectral techniques that suppress destructive update components while preserving structure.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#why-do-models-forget">Why Do Models Forget</a> </div> <div> <a href="#setting-the-stage">Setting the Stage</a> </div> <div> <a href="#observing-spectral-training-dynamics">Observing Spectral Training Dynamics</a> </div> <div> <a href="#two-types-of-forgetting">Two Types of Forgetting</a> </div> <div> <a href="#towards-spectrally-graceful-updates">Towards (Spectrally) Graceful Updates</a> </div> <div> <a href="#going-deeper">Going Deeper</a> </div> <div> <a href="#experimental-results">Experimental Results</a> </div> <ul> <li> <a href="#toy-model-mlp-on-mnist">Toy Model - MLP on MNIST</a> </li> <li> <a href="#llms-sequential-sft-on-q-a-tasks">LLMs - Sequential SFT on Q&amp;A Tasks</a> </li> </ul> <div> <a href="#final-notes">Final Notes</a> </div> <div> <a href="#appendix">Appendix</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover-480.webp 480w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover-800.webp 800w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Most of our understanding of the world is built through a continuous interaction with our surroundings.</p> <p>This marks a very distinct difference between Artificial and Human intelligence: while modern AI models are stuck in a training / inference dichotomy, human intelligence (and, more generally, intelligence found in nature) blurs the line between the two, integrating a subconscious process of accumulation and processing of new information throughout our interaction with the world.</p> <p>This makes continual learning, to us, the most pressing problem in modern Artificial Intelligence.</p> <p>Most existent continual learning methods (<d-cite key="kirkpatrick_overcoming_2017"></d-cite><d-cite key="saha_gradient_2020"></d-cite><d-cite key="wang_training_2021"></d-cite><d-cite key="zenke_continual_2017"></d-cite><d-cite key="aljundi_memory_2018"></d-cite><d-cite key="farajtabar_orthogonal_2020"></d-cite><d-cite key="lopez_paz_gradient_2017"></d-cite><d-cite key="chaudhry_efficient_2019"></d-cite><d-cite key="riemer_learning_2019"></d-cite><d-cite key="shin_continual_2017"></d-cite>) revolve around the idea of making a specific subset of the training data <em>harder to forget.</em> Through either replay buffers, reduced learning rates on the parameters that are more important to interpret <em>a specific</em> subset of data, or orthogonalization of the updates to task-specific directions, the model is strongly penalized whenever relevant information is forgotten.</p> <p>All of these methods might work in specific multi-task scenarios, but the interactions that we, as humans, experience with the surrounding world are rarely ever categorizable in separate tasks.</p> <p>To begin pondering the possibility of human-like lifelong learning, we can’t rely on manually “saving” a small amount of specific subsets of data from catastrophic forgetting: we need a learning paradigm that allows to simply <em>not forget anything,</em> and learn new things without damaging previous knowledge.</p> <p>This blogpost will propose a strongly personal point of view on:</p> <ul> <li>Why do models forget, and whether we can distinguish between different types of forgetting;</li> <li>How does the core phenomenon of catastrophic forgetting look like from a spectral perspective;</li> <li>A possible approach towards more graceful optimization, to preserve learned structure.</li> </ul> <h2 id="why-do-models-forget">Why Do Models Forget</h2> <p>Catastrophic forgetting stems as an intrinsic consequence of how models are optimized. Gradient descent would like to minimize a loss function over a dataset of samples. Since this is usually computationally unfeasible, <em>Stochastic</em> Gradient Descent only concerns itself with a tiny, random portion of the data, and tunes the parameters to minimize the objective function on that subset of data. This means that each iteration has only one goal: optimize the parameters to predict the current batch as accurately as possible, as if no other batch ever existed.</p> <p>This leads to a constant flow of <em>construction</em> and <em>destruction</em> of structure in the weight matrices, that hopefully balances itself over enough training steps with large enough batches.</p> <p>To mitigate the damage of this oversimplification of the learning paradigm, two main practices stuck around over the years:</p> <ul> <li>Learning rate schedulers apply a very simple yet effective heuristic: if the model has been updated a lot of times, then probably it has built a lot of structure in the parameters that is worth preserving. Therefore, it makes sense to lower the magnitude of the updates to avoid destroying that structure.</li> <li>Better optimizers, like Adam, add two improvements: i) stabilize the trajectory of the updates, by accelerating descent through consistent paths, and decelerating through inconsistent ones; ii) optimize more strongly parameters that have been optimized fewer times.</li> </ul> <h2 id="setting-the-stage">Setting the Stage</h2> <p>This kind of techniques can be seen, from a continual learning point of view, as heuristics that try to balance the ability of a model to keep learning from new examples, while mitigating the amount of destruction that a new batch induces on the information that was previously learned. We like to think of this balance as a trade-off between <em>stability</em> and <em>plasticity</em> (<d-cite key="kim_achieving_2023"></d-cite><d-cite key="chen_stabilityplasticity_2023"></d-cite><d-cite key="jung_new_2023"></d-cite><d-cite key="lu_rethinking_2025"></d-cite>). The former represents the ability of a model to preserve learned structure (i.e: circuits, patterns…), the latter stands for the potential to learn new structures when needed (for example, when a change in the distribution of training samples occurs).</p> <p>In the traditional optimization scenario, this trade-off looks like a zero-sum game: stronger optimization (higher learning rate), increases plasticity at the cost of heavier forgetting; more delicate optimization (lower learning rate), increases stability by reducing the model’s ability to learn new information.</p> <p>We think that, to better navigate this tradeoff, a change in the optimization paradigm is needed. We like to think of this in terms of <em>gracefulness</em> of the updates. A graceful update is one that improves performance on the current batch without overwriting or degrading previously accumulated structure in the weights. There is a very fine line that keeps this from being an intrinsically contradictory statement: a good update should, by definition, update the model’s perception of the world (the training data) in a generalizable way; so how could it work without altering and refining previously learned connections and circuits?</p> <h2 id="observing-spectral-training-dynamics">Observing Spectral Training Dynamics</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover2-480.webp 480w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover2-800.webp 800w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>To answer, we need to go deeper into the dynamics of learning and refining knowledge. Each optimization step computes update directions that are entirely specific to the current batch. Intuitively, we can think of an update as a mixture of two macro-types of operations:</p> <ul> <li> <em>Refining present structure</em>: existing information encoded in the weights is updated according to its performance on the current batch;</li> <li> <em>Building new structure</em>: a new encoding is built ad-hoc for the data in this batch.</li> </ul> <p>The first targets encoded structure that is strongly learned and optimized and tries to tweak it to make it work on the current batch. The second takes a weak, raw part of the network and begins to shape and refine it using the data in the current batch.</p> <p>We can try to observe these behaviors by decomposing with SVD both a weight matrix</p> \[W = U_w \Sigma_w V^T_w\] <p>and the update applied to it at each optimization step,</p> \[\Delta = U_\Delta \Sigma_\Delta V^T_\Delta\] <p>and plot the alignment between the $U_w$ vectors of the weight and the $U_\Delta$ vectors of the update.</p> \[M_{i, j} = \frac{|U_w^i \cdot U_\Delta^j|}{\left\lVert U_w^i \right\rVert \|U_\Delta^j\|}\] <p>An alignment between strong update directions and strong weight directions shows destructive interference of an already highly optimized structure in the weights. Alignment between top update directions and weak weight directions represents the second type of update, in which new structure is being built to encode new information.</p> <p>Let’s now observe an EMA of this matrix while training a 2-layer MLP on MNIST with Adam.</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-dynamics-of-forgetting/video1.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <div class="caption"> Left, top: singular values of the weight matrix; Left, bottom: singular values of the update proposed by Adam for the same matrix; Center: heatmap of alignment between left singular vectors of the update and left singular vectors of the weight; Right: Validation Accuracy on MNIST. The high values concentrated on the top left of the heatmap show strong alignment between the most important spectral components of the update and the weight. In other words, the optimizer focuses mostly on updating structures which are already very strong. </div> <p>We clearly see that, after an initial stage of randomness, the training quickly converges to always updating a few directions in weight space.</p> <p>This behavior is even more visible when we reduce the hidden dimension of the linear layer analyzed to 2. The training clearly switches from a <em>structure building</em> state, in which updates are not necessarily aligned with the top directions in the weight matrix, to a <em>refining</em> state, in which the update assigns its focus proportionately to the importance of each direction. This switch also synchronized with the moment in which the validation accuracy’s derivative drops below 1 and the model enters a training phase of much slower improvement.</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-dynamics-of-forgetting/video2.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <h2 id="two-types-of-forgetting">Two Types of Forgetting</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover3-480.webp 480w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover3-800.webp 800w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-dynamics-of-forgetting/cover3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>So what causes forgetting, exactly?</p> <p>Our previous distinction between two macro-types of updates leads us to think about forgetting in terms of two easily distinguishable patterns. As an example, let’s think of the case of training a simple MNIST classifier.</p> <ol> <li> <p><em>The current sample destroys structure that was learned on different data.</em></p> <p>In our example, this could look like the model encountering a new digit, and applying to it a series of weights built to recognize other digits, leading to a wrong prediction. The weight update based on this new sample will interfere with the existing circuits built from the previous digits, overwriting weights that were needed to recognize them.</p> </li> <li> <p><em>The model learns entirely new structure on the current batch of data.</em></p> <p>Here, the model might see a new digit and build ad-hoc structure to encode / recognize it. These new paths might now incorrectly fire for other digits as well, interfering with their prediction.</p> </li> </ol> <p>In both cases, the problem consists in the model learning something on the current data which doesn’t apply to different data, but the approach to mitigate these two phenomena is drastically different. The first can be solved by applying updates that preserve the current structure. The second cannot.</p> <p>Most continual learning methods, as well as the rest of this blogpost, are only concerned with the first <em>(destructive)</em> type of forgetting, which is much easier to address.</p> <p>We believe that analyzing and determining how much of each of these types of forgetting actually occurs at a given time in a continual learning procedure is one of the most interesting open problems in continual learning.</p> <p>To observe spectral behavior in presence of strong forgetting, we can look at the alignment matrix when switching training tasks from MNIST to Fashion-MNIST.</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-dynamics-of-forgetting/video3.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <p>It seems that, when switching to the new task of Fashion-MNIST:</p> <ul> <li>The top 3 components of the update switch from targeting already strong structures to building weaker ones (in the first 3 columns of the matrix, the “heat” disappears from the top and shifts downward). This would represent type 2, or <em>constructive</em> forgetting.</li> <li>The remaining important components of the update keep targeting already strong structure (a strong heat spot remains visible in the top-left sector of the matrix). This would seem to indicate type 1, or <em>destructive</em> forgetting.</li> </ul> <p>So, are we able to do something to at least mitigate the <em>destructive</em> forgetting that happens at each optimization step?</p> <h2 id="towards-spectrally-graceful-updates">Towards (Spectrally) Graceful Updates</h2> <p>Given this (very raw and certainly incomplete) analysis, we can try to intervene by observing the spectral characteristic of each weight matrix, and manually tweaking each update to make it more graceful. Ultimately, we want to <em>refuse</em> components of the update that wrongfully penalize weights that were optimized for very different data.</p> <p>Let’s start with an oversimplification and divide the spectral decomposition of a matrix into low-rank (or strong) components and high-rank (or weak) ones. Let us also assume that a low-rank update on a direction changes it strongly, while a high-rank update only refines it slightly.</p> <p>Now we can define a policy that decides which components of an update can be allowed, and which ones need to be refused as potentially destructive, based on their alignment with the corresponding singular vectors of the weight matrix.</p> <p>A very simple example policy could look as follows:</p> <ul> <li>Strong updates on strong directions are clearly destructive of previously accumulated knowledge: we want to refuse them;</li> <li>Strong updates on weak directions are exactly what we want: they build new structure for the new data without destroying old ones;</li> <li>Weak updates on strong directions are also probably ok, as they only slightly refine strongly optimized weights;</li> <li>Weak updates on weak structures are hard to characterize: they might be noise, or very low-importance updates in general. For this example, we refuse them.</li> </ul> <p>We can implement this policy and immediately see the structure we are enforcing in the alignment matrix: our policy divides the matrix into four quadrants and only allows updates belonging to two of the four possible combinations (top-right and bottom-left).</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-dynamics-of-forgetting/video4.html" frameborder="0" scrolling="no" height="400px" width="100%"> </iframe> </div> <h2 id="going-deeper">Going Deeper</h2> <p>A matrix-wise intervention on modern deep learning models can be extremely impactful, with the <a href="https://kellerjordan.github.io/posts/muon/" rel="external nofollow noopener" target="_blank">Muon optimizer</a> being the prime example. Yet, we can’t but notice that models will inevitably encode information in the composition of their layers, their <em>circuitry</em>, rather than just atomically in the weights of each. This exposes a clear weak point in our technique so far: it only tries to identify and preserve structure in each matrix individually, but it never looks at their composition.</p> <p>While we’re still far from having a practical solution to this, we have some initial approaches that seem to bring improvements over the previous update conditioning function (or <em>post-conditioner</em>, as we like to call it).</p> <p>Our deeper approach shares the same core idea as the previous one, but it does so taking into account a key element: cross-layer alignment.</p> <p>Let</p> \[W^{(l)} = U^{(l)}\Sigma^{(l)}V^{(l)T} , \qquad \Delta^{(l)} = U_\Delta^{(l)}\Sigma_\Delta^{(l)}V_\Delta^{(l)T}\] <p>be a weight matrix for layer $l$ in a model and the corresponding update proposed by a chosen optimizer, such as AdamW or Muon</p> <p>In a matrix-wise post-conditioner, we use some function $f(W^{(l)},\Delta^{(l)})$ to re-scale the singular sub-spaces spanned by $\Delta^{(l)}$, so that the update is not destructive.</p> <p>For this post-conditioner, we instead re-scale them based on a function</p> \[f_{comp}(W^{(l-1)},\Delta^{(l-1)}, W^{(l)},\Delta^{(l)}, W^{(l+1)},\Delta^{(l+1)}).\] <p>Of course, using $l+1$ and $l-1$ implies a notion of ordering of the weight matrices, which is definitely not trivial to define in a more complex network, such as an LLM.</p> <p>The intuition of $f_{comp}$ is that the subspaces of $\Delta^{(l)}$ are now also rescaled based on the alignment matrices</p> \[T^{(l+1)} = V^{(l+1)T}U^{(l)} , \qquad T^{(l-1)} = V^{(l)T}U^{(l-1)}.\] <p>The core insight is as follows: if incoming and outgoing spectral spaces align, then there is sign of structure in the weights, and we don’t want to be updating it too aggressively, so we reduce the magnitude of the update based on how aligned they are.</p> <p>So, can these style of approaches actually mitigate destructive updates and help in the continual learning setting?</p> <h2 id="experimental-results">Experimental Results</h2> <p>Truth is, we don’t really know yet. In some specific scenarios, our preliminary results seemed extremely encouraging; in others, lackluster.</p> <p>We present here a small collection of results, to be taken with a nice pinch of salt, which highlight the intrinsic difficulty of a proper navigation of the Stability-Plasticity trade-off.</p> <p>We experiment with our post-conditioning technique across two distinct scenarios:</p> <ul> <li> <p><strong>Toy model: 2-layer MLP trained on MNIST</strong></p> <p>We train a tiny 2-layer MLP from scratch on two subsequent classification tasks. This lets us study how our method’s hyperparameters behave across settings when training from scratch.</p> </li> <li> <p><strong>LLMs: sequential SFT across 3 datasets</strong></p> <p>We fine-tune a Transformer-based Large Language Model with Supervised Fine-Tuning on three different datasets in sequence. We evaluate accuracy on all three datasets throughout training, and measure forgetting and learning metrics for all checkpoints. This setting lets us monitor how the technique scales and how it interacts with sparsity in large models.</p> </li> </ul> <p>In our experiments, we call <em>softmask</em> the matrix-wise post-conditioner, and <em>compsoft</em> its circuit-aware counterpart.</p> <h3 id="toy-model---mlp-on-mnist">Toy Model - MLP on MNIST</h3> <p>We train on subsets of MNIST, where each task is binary classification over two digits. The first task is classifying digits $1$ vs. $2$, and the second task is classifying digits $3$ vs. $4$. Once accuracy reaches $0.95$ on the first task, the second task is activated and the model is trained on the new digit pair. We train with a constant learning rate.</p> <p>Here we report <strong>AccΣ := Acc1 + Acc2</strong> , representing the sum of the accuracies of the two tasks at the end of the training steps.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig1-480.webp 480w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig1-800.webp 800w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="llms---sequential-sft-on-qa-tasks">LLMs - Sequential SFT on Q&amp;A Tasks</h3> <p>We perform Supervised Fine-Tuning on <strong>Qwen3 0.6B</strong> (<d-cite key="yang2025qwen3"></d-cite>) and <strong>Llama 3.2 1B</strong> (<d-cite key="grattafiori2024llama"></d-cite>) on a sequence of three distinct datasets. As training progresses, we evaluate the model’s capabilities on all tasks for every checkpoint. The resulting accuracies let us gauge how well training navigates the robustness–plasticity tradeoff: an ideal model learns strongly while retaining high accuracy on previous tasks.</p> <p>We train sequentially on three Q&amp;A datasets (inspired by <d-cite key="shenfeld2025rl"></d-cite>):</p> <ul> <li>Open Reasoner-zero (<d-cite key="hu2025open"></d-cite>)</li> <li>SciKnowEval (<d-cite key="feng2024sciknoweval"></d-cite>)</li> <li>HellaSwag (<d-cite key="zellers2019hellaswag"></d-cite>)</li> </ul> <p>Here, we plot the validation accuracy of the model on each of the three datasets throughout the training, when using a fixed learning rate of $4 \times 10^{-5}$:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-480.webp 480w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-800.webp 800w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig2a-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig2a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>And the same with a learning rate of $1 \times 10^{-5}$:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-480.webp 480w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-800.webp 800w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig2b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig2b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>It is crucial to notice how the post-conditioning method strongly outperforms Adam, when used with a higher learning rate, but it starts losing when the learning rate is tuned to a level that is optimal for the task.</p> <p>We can get an idea of the model’s ability to navigate the stability-plasticity tradeoff by plotting the last task’s accuracy (plasticity) vs an average of the accuracies for the previous tasks (stability) at the end of the training.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-480.webp 480w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-800.webp 800w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig3a-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig3a.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-480.webp 480w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-800.webp 800w,/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig3b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-dynamics-of-forgetting/fig3b.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="final-notes">Final Notes</h2> <p>We are publishing this blogpost with the main intent of conveying our point of view on continual learning: how it is a core distinguishing factor between human and artificial intelligence, how catastrophic forgetting plays a central role in the optimization procedure of a model, even outside of traditional sequential multi-task scenarios, and how we believe that forgetting is made of two very different dynamics, one of which is commonly addressed by most CL methods, the other still largely unexplored.</p> <p>We argue that modern continual learning methods cannot rely on knowing what knowledge must be retained, and should instead be capable of preserving existing structure in the weights.</p> <p>We test a couple early implementations in two distinct settings, where we find some mixed results, which we hope will stimulate discussion in the community and help us develop on this first step towards producing gracefully updating, continually learning models. Overall, we think the point stands: we must find a new way to build non-destructive updates that respect the model’s learned structure. We can’t wait to know what you think about it.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-dynamics-of-forgetting.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/witness-problem/">The Witness Problem in Multi-Agent Cooperation</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>