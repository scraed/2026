<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Extracting Model Precision from 20 Logprobs | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We demonstrate that the internal floating-point precision of language models can be inferred from API-exposed logprobs."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/precision-extraction/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Extracting Model Precision from 20 Logprobs",
            "description": "We demonstrate that the internal floating-point precision of language models can be inferred from API-exposed logprobs.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Anonymous",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Extracting Model Precision from 20 Logprobs</h1> <p>We demonstrate that the internal floating-point precision of language models can be inferred from API-exposed logprobs.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#preliminaries">Preliminaries</a> </div> <ul> <li> <a href="#floating-point-formats">Floating-Point Formats</a> </li> <li> <a href="#how-are-logprobs-actually-computed">How are Logprobs Actually Computed?</a> </li> </ul> <div> <a href="#the-precision-extraction-attack">The Precision Extraction Attack</a> </div> <ul> <li> <a href="#attack-1-brute-force-search">Attack 1: Brute Force Search</a> </li> <li> <a href="#attack-2-inverted-search">Attack 2: Inverted Search</a> </li> <li> <a href="#handling-fp32-rounding-errors">Handling FP32 Rounding Errors</a> </li> </ul> <div> <a href="#experiments">Experiments</a> </div> <ul> <li> <a href="#validation-results-with-known-ground-truth">Validation Results with Known Ground Truth</a> </li> <li> <a href="#precision-collision-why-fp8-e4m3-gets-misclassified-as-e5m2">Precision Collision: Why FP8 E4M3 Gets Misclassified as E5M2</a> </li> <li> <a href="#identifying-the-precision-of-openai-and-gemini-models">Identifying the Precision of OpenAI and Gemini Models</a> </li> </ul> <div> <a href="#discussion">Discussion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>While today we mostly think of language models as text <em>generators</em>, the log-probabilites they output at each token position have long been used outside of generation contexts, for example to perform classification tasks <d-cite key="classification"></d-cite>, to visualize differences between human-written and AI-generated text <d-cite key="gltr"></d-cite>, and to detect memorized training data <d-cite key="memorization"></d-cite>. For this reason, most early LLM APIs exposed <strong>logprobs</strong>, allowing users to easily request these per-token scores when querying models like GPT-3 or Gemini 1.</p> <p>Recent work has shown that logprobs expose surprising amounts of information about the proprietary models underlying these APIs. For example, they can be used to extract hidden dimensions and embedding projection matrices from production models <d-cite key="carlini2024stealing"></d-cite>. In this blog post, we demonstrate another vulnerability: with access to just 20 token logprobs, we can <em>infer the floating-point precision of the underlying model</em>.</p> <p>While the APIs usually return the logprobs as 32-bit floats, internally, models often store their weights and do computations in lower precision (FP16, BF16, FP8) for improved efficiency. We were curious to answer the question: is it possible to infer the internal precision of the proprietary model underlying an LLM API using the returned logprob values alone? We show the answer is <strong>yes</strong>. Our key insight is that the log-softmax computation shifts all logits by a constant, and we can search for shift values that map logprobs back to representable values in a given precision.</p> <p>Our technique seems to suggest that older OpenAI models (GPT-3.5, GPT-4) use FP32 logits, while newer models (GPT-4o, GPT-4.1) use BF16. This transition likely reflects the adoption of different base architectures or training methods.</p> <p><strong>Why does this matter?</strong> Precision of the logits reveals architectural and inference details that providers typically keep secret such as quantization level and model precision. For competitive reasons, companies rarely disclose whether they trained or served quantized models. Our attack provides a measurement technique that can track these changes over time, enabling researchers to study the evolution of deployed systems and validate provider claims about model quality.</p> <p>However, simple defenses (such as adding random noise to the logprobs before the API returns them to the user) can entirely defend against this class of attacks. Moreover, LLM deployers have been moving away from exposing logprobs in their APIs, so while we think our attack is pretty neat, we don’t expect it to stick around for newer APIs and models.</p> <h2 id="preliminaries">Preliminaries</h2> <h3 id="floating-point-formats">Floating-Point Formats</h3> <p>So, what is a floating point number? In the IEEE 754 standard, a floating-point number consists of three parts: the sign, exponent, and mantissa. The <strong>sign</strong> is always a single bit, but the remaining bits are divided up between the exponent and the mantissa. The <strong>exponent</strong> determines the magnitude (in powers of 2), while the <strong>mantissa</strong> determines the precision within that magnitude. More mantissa bits mean finer granularity between representable values. Here are some common floating point number formats:</p> <table> <thead> <tr> <th>Format</th> <th>Total Bits</th> <th>Exponent</th> <th>Mantissa</th> <th>Min Step (near 1.0)</th> </tr> </thead> <tbody> <tr> <td>FP32</td> <td>32</td> <td>8</td> <td>23</td> <td>~1.2×10⁻⁷</td> </tr> <tr> <td>FP16</td> <td>16</td> <td>5</td> <td>10</td> <td>~9.8×10⁻⁴</td> </tr> <tr> <td>BF16</td> <td>16</td> <td>8</td> <td>7</td> <td>~7.8×10⁻³</td> </tr> <tr> <td>FP8 E5M2</td> <td>8</td> <td>5</td> <td>2</td> <td>~0.25</td> </tr> </tbody> </table> <p>“Min Step” indicates the smallest difference in values that can be represented in this format. BF16 is particularly interesting: it has the same exponent range as FP32 (so it can represent the same magnitude) but with much coarser precision. This makes it a common choice for neural network training and inference where dynamic range matters more than fine precision in avoiding numerical issues.</p> <p>When a lower-precision value is converted to the higher-precision FP32, the extra mantissa bits are filled with zeros:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BF16 mantissa:  1.0101010
                    ↓ convert to FP32
FP32 mantissa:  1.0101010_0000000000000000
                        └─ 16 trailing zeros
</code></pre></div></div> <p>A BF16 value has 7 mantissa bits; when viewed as FP32, it has <strong>16 trailing zeros</strong> (23 - 7 = 16). When a FP32 number ends with a suspicious number of trailing zeros, we can infer that the original number was probably a BF16.</p> <h3 id="how-are-logprobs-actually-computed">How are Logprobs Actually Computed?</h3> <p>Logprobs aren’t the raw model outputs. Models output logits, unnormalized scores for each item in the model’s vocabulary. Logprobs are computed from the logits by taking a log-softmax function over them. For example, here’s the implementation in the vLLM (<a href="https://github.com/vllm-project/vllm/blob/d44a63c6d6e1a545aff270b3b85cf231ef779dab/vllm/v1/sample/sampler.py#L207" rel="external nofollow noopener" target="_blank">sampler.py L207</a>) library:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">logprobs</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span></code></pre></figure> <p>Let’s look at how log_softmax is actually implemented. Here’s the PyTorch CUDA kernel (<a href="https://github.com/pytorch/pytorch/blob/04bd7e6850e8efec77994963ffee87549555b9c3/aten/src/ATen/native/cuda/SoftMax.cu#L37-L47" rel="external nofollow noopener" target="_blank">SoftMax.cu L37-47</a>):</p> <figure class="highlight"><pre><code class="language-cpp" data-lang="cpp"><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span> <span class="nc">T</span><span class="p">,</span> <span class="k">typename</span> <span class="nc">AccumT</span><span class="p">,</span> <span class="k">typename</span> <span class="nc">OutT</span><span class="p">&gt;</span>
<span class="k">struct</span> <span class="nc">LogSoftMaxForwardEpilogue</span> <span class="p">{</span>
  <span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">LogSoftMaxForwardEpilogue</span><span class="p">(</span><span class="n">AccumT</span> <span class="n">max_input</span><span class="p">,</span> <span class="n">AccumT</span> <span class="n">sum</span><span class="p">)</span>
    <span class="o">:</span> <span class="n">max_input</span><span class="p">(</span><span class="n">max_input</span><span class="p">),</span> <span class="n">logsum</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">log</span><span class="p">(</span><span class="n">sum</span><span class="p">))</span> <span class="p">{}</span>

  <span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">OutT</span> <span class="nf">operator</span><span class="p">()(</span><span class="n">T</span> <span class="n">input</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">OutT</span><span class="o">&gt;</span><span class="p">(</span><span class="n">input</span> <span class="o">-</span> <span class="n">max_input</span> <span class="o">-</span> <span class="n">logsum</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">const</span> <span class="n">AccumT</span> <span class="n">max_input</span><span class="p">;</span>
  <span class="k">const</span> <span class="n">AccumT</span> <span class="n">logsum</span><span class="p">;</span>
<span class="p">};</span></code></pre></figure> <p>The kernel is templated on three types: <code class="language-plaintext highlighter-rouge">T</code> (input type, e.g., BF16), <code class="language-plaintext highlighter-rouge">AccumT</code> (accumulation type, usually FP32), and <code class="language-plaintext highlighter-rouge">OutT</code> (output type). Ignoring the <code class="language-plaintext highlighter-rouge">max_input</code> shift (which is there for numerical stability), we have:</p> \[\text{logprob}_i = z_i - \underbrace{\log\left(\sum_j \exp(z_j)\right)}_{w}\] <p>where $z_i$ are the <strong>logits</strong> (raw model outputs) and $w$ is the log-sum-exp normalization constant. The key line in the kernel is <code class="language-plaintext highlighter-rouge">input - max_input - logsum</code>: the BF16 <code class="language-plaintext highlighter-rouge">input</code> is implicitly promoted to FP32 for the subtraction. When promoted, the BF16 value has trailing zeros in its mantissa, but the subtraction of <code class="language-plaintext highlighter-rouge">logsum</code> destroys this pattern.</p> <h2 id="the-precision-extraction-attack">The Precision Extraction Attack</h2> <p>All logprobs from a single prediction share the same normalization constant $w$, which is computed in FP32. When you subtract this FP32 value from a BF16 logit, the trailing zeros fingerprint gets destroyed. So without knowing the value for $w$, we can’t just look at logprobs and count trailing zeros. But if we could recover $w$, we could add it back to get the original logits and check their precision.</p> <h3 id="attack-1-brute-force-search">Attack 1: Brute Force Search</h3> <p>The most straightforward approach is to iterate through all $2^{32}$ possible FP32 values of $w$. For each candidate, compute $z_i + w$ for all logprobs and count trailing zeros. The $w$ that maximizes total trailing zeros is the true normalization constant.</p> <p>Here’s the algorithm:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for each possible FP32 value w:
    score = sum of trailing_zeros(logprob[i] + w) for all i
    keep w with highest score
</code></pre></div></div> <p>This works, but iterating through $2^{32}$ candidates takes about half an hour per set of logprobs. We can do better.</p> <h3 id="attack-2-inverted-search">Attack 2: Inverted Search</h3> <p>Instead of searching over all possible $w$, we exploit a constraint: the recovered logits $\text{logprob}_i + w$ must be representable in the target precision. For BF16, there are only 65,536 representable values. The first logprob gives us $\text{logprob}_0 + w = z_0$, so $w = z_0 - \text{logprob}_0$. Since $z_0$ must be one of 65,536 BF16 values, we have at most 65,536 candidates for $w$.</p> <p>Each remaining logprob filters this set: we keep only candidates where $\text{logprob}_i + w$ is also representable. If any candidate survives, we’ve found a valid $w$ and confirmed the precision. We test precisions from most restrictive to least (all FP8 values are representable in BF16, all BF16 values in FP16, etc.); if none match, we conclude FP32.</p> <p>Here’s the algorithm:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for precision in [FP8_E5M2, FP8_E4M3, BF16, FP16]:
    candidates = {z - logprob[0] : z in all representable values}
    for each logprob[i]:
        candidates = {w : w in candidates and (logprob[i] + w) is representable}
    if candidates is non-empty:
        return precision
return FP32
</code></pre></div></div> <p>This reduces worst-case complexity from $O(2^{32})$ to $O(65536 \times N)$ where $N$ is the number of logprobs - from hours to milliseconds.</p> <h3 id="handling-fp32-rounding-errors">Handling FP32 Rounding Errors</h3> <p>There’s a subtle issue: checking whether <code class="language-plaintext highlighter-rouge">logprob[i] + w</code> is “representable” requires care. FP32 addition is not perfectly invertible: if we compute <code class="language-plaintext highlighter-rouge">logprob = logit - w</code>, then <code class="language-plaintext highlighter-rouge">logprob + w</code> may not exactly equal <code class="language-plaintext highlighter-rouge">logit</code> due to rounding.</p> <p>Consider this concrete example:</p> <table> <thead> <tr> <th>Token</th> <th>Logprob (FP32)</th> <th>Quantized Logit (BF16)</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">"\n"</code></td> <td>-1.012399435043335</td> <td>2.125</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">" I"</code></td> <td>-2.371774435043335</td> <td>0.765625</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">" "</code></td> <td>-2.828805685043335</td> <td>0.30859375</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">" You"</code></td> <td>-3.793649435043335</td> <td>-0.65625</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">" It"</code></td> <td>-4.059274673461914</td> <td>-0.921875</td> </tr> </tbody> </table> <p>The normalization constant is <code class="language-plaintext highlighter-rouge">w = 3.137399435043335</code>. When we compute <code class="language-plaintext highlighter-rouge">logprob[4] + w</code> to recover the last logit, we get <code class="language-plaintext highlighter-rouge">-0.9218752384185791</code> instead of exactly <code class="language-plaintext highlighter-rouge">-0.921875</code>. The difference (~2×10⁻⁷) is vanishingly small, but it completely destroys the trailing-zeros pattern.</p> <p>A naive implementation checking for exact representation in BF16 would fail to match and reject BF16. The fix is to check whether <code class="language-plaintext highlighter-rouge">logprob[i] + w</code> is <em>close to</em> a representable value:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>is_representable(x, precision, tolerance=1e-5):
    rounded = round_to_nearest(x, precision)
    return |x - rounded| &lt; tolerance
</code></pre></div></div> <p>With this tolerance, we correctly identify BF16 even when FP32 arithmetic introduces small rounding errors.</p> <h2 id="experiments">Experiments</h2> <p>For both experiments below, we use the same methodology: 100 prompts with 20 logprobs each. Prompts are simple templates (“Count from 1 to N”, “What is A + B?”, “Name N colors”, etc.), and we request <code class="language-plaintext highlighter-rouge">max_tokens=1</code> and collect the top-20 logprobs at that position. Since the attack operates on the numerical properties of logprobs, the prompts themselves are not important. Each additional logprob adds another constraint that the candidate $w$ must satisfy, making it less likely for a coarser precision to “get lucky” and pass all checks.</p> <h3 id="validation-results-with-known-ground-truth">Validation Results with Known Ground Truth</h3> <p>We validate our detection algorithm using <em>simulated quantization</em>: we start with a known FP32 model (GPT-Neo-125M), quantize its logits to various precisions, compute log-softmax, and verify detection accuracy. Note that in most cases, we only need one set of logprobs to detect the precision, but we repeat the experiment on 100 prompts to see if the attack is robust to variation in prompts.</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "title": "Detection Accuracy by Precision",
  "width": 350,
  "height": 280,
  "data": {
    "values": [
      {"actual": "FP32", "detected": "FP32", "count": 100},
      {"actual": "BF16", "detected": "BF16", "count": 100},
      {"actual": "FP16", "detected": "FP16", "count": 100},
      {"actual": "FP8 E4M3", "detected": "FP8 E4M3", "count": 89},
      {"actual": "FP8 E4M3", "detected": "FP8 E5M2", "count": 11},
      {"actual": "FP8 E5M2", "detected": "FP8 E5M2", "count": 100}
    ]
  },
  "encoding": {
    "x": {"field": "detected", "type": "nominal", "title": "Detected", "sort": ["FP8 E5M2", "FP8 E4M3", "BF16", "FP16", "FP32"]},
    "y": {"field": "actual", "type": "nominal", "title": "Actual", "sort": ["FP8 E5M2", "FP8 E4M3", "BF16", "FP16", "FP32"]}
  },
  "layer": [
    {
      "mark": "rect",
      "encoding": {
        "color": {"field": "count", "type": "quantitative", "scale": {"scheme": "blues"}, "title": "Count"},
        "tooltip": [
          {"field": "actual", "type": "nominal", "title": "Actual Precision"},
          {"field": "detected", "type": "nominal", "title": "Detected As"},
          {"field": "count", "type": "quantitative", "title": "Count"}
        ]
      }
    },
    {
      "mark": {"type": "text", "fontSize": 12},
      "encoding": {
        "text": {"field": "count", "type": "quantitative"},
        "color": {"condition": {"test": "datum.count &gt; 50", "value": "white"}, "value": "black"}
      }
    }
  ],
  "config": {"axis": {"labelFontSize": 11, "titleFontSize": 12}},
  "usermeta": {"embedOptions": {"actions": false}}
}
</code></pre> <h3 id="precision-collision-why-fp8-e4m3-gets-misclassified-as-e5m2">Precision Collision: Why FP8 E4M3 Gets Misclassified as E5M2</h3> <p>The 11% misclassification rate for FP8 E4M3 reveals an ambiguity in our attack. The algorithm finds $w$ by testing whether all recovered logits are representable in a given format. But sometimes, a <em>different</em> $w$ exists that maps the same logprobs to valid values in a coarser format.</p> <h4 id="what-does-a-collision-look-like">What does a collision look like?</h4> <p>Here is a concrete example with E4M3-quantized logits:</p> <table> <thead> <tr> <th> </th> <th>True (E4M3)</th> <th>Algorithm finds (E5M2)</th> </tr> </thead> <tbody> <tr> <td>$w$</td> <td>2.938695</td> <td>4.313695</td> </tr> <tr> <td>logit[0]</td> <td>1.625</td> <td>3.0</td> </tr> <tr> <td>logit[1]</td> <td>0.625</td> <td>2.0</td> </tr> <tr> <td>logit[2]</td> <td>0.625</td> <td>2.0</td> </tr> <tr> <td>logit[3]</td> <td>-0.75</td> <td>0.625</td> </tr> <tr> <td>logit[4]</td> <td>-1.0</td> <td>0.375</td> </tr> </tbody> </table> <p>Both interpretations are mathematically valid given the logprobs. The algorithm tests E5M2 first (more restrictive) and finds a valid $w$, so it returns E5M2.</p> <h4 id="why-does-this-happen">Why does this happen?</h4> <p>Floating-point step sizes depend on magnitude:</p> <table> <thead> <tr> <th>Magnitude</th> <th>E4M3 step</th> <th>E5M2 step</th> </tr> </thead> <tbody> <tr> <td>[0.5, 1)</td> <td>0.0625</td> <td>0.125</td> </tr> <tr> <td>[1, 2)</td> <td>0.125</td> <td>0.25</td> </tr> <tr> <td>[2, 4)</td> <td>0.25</td> <td>0.5</td> </tr> </tbody> </table> <p>In the example above, the shifted logits (3.0, 2.0, 2.0, 0.625, 0.375) have larger magnitudes, placing them on a coarser grid that happens to also be representable in E5M2. This ambiguity is fundamental: from logprobs alone, we cannot distinguish between the two interpretations. That said, multiple samples usually resolve the uncertainty: true E4M3 logits will eventually hit values that E5M2 cannot represent.</p> <h4 id="why-doesnt-this-happen-for-higher-precision-formats">Why doesn’t this happen for higher-precision formats?</h4> <p>Higher precision means exponentially finer grids. For a BF16 value to accidentally land on a coarser grid after shifting by a different $w$, it would need to hit an increasingly sparse set of valid values, or logits would need to have high magnitudes. The E4M3/E5M2 overlap is a coincidence of their similar precision; the gap between BF16 and FP8 is wide enough that such collisions become rare.</p> <h3 id="identifying-the-precision-of-openai-and-gemini-models">Identifying the Precision of OpenAI and Gemini Models</h3> <p>We apply our method to OpenAI and Gemini models with logprobs access.</p> <table> <thead> <tr> <th>Model</th> <th>Detected Precision</th> <th>Agreement</th> </tr> </thead> <tbody> <tr> <td>gpt-3.5-turbo</td> <td>FP32</td> <td>100%</td> </tr> <tr> <td>gpt-4</td> <td>FP32</td> <td>100%</td> </tr> <tr> <td>gpt-4-turbo</td> <td>FP32</td> <td>100%</td> </tr> <tr> <td>gpt-4o</td> <td>BF16</td> <td>97%</td> </tr> <tr> <td>gpt-4o-mini</td> <td>BF16</td> <td>100%</td> </tr> <tr> <td>gpt-4.1</td> <td>BF16</td> <td>100%</td> </tr> <tr> <td>gpt-4.1-mini</td> <td>BF16</td> <td>100%</td> </tr> <tr> <td>gpt-4.1-nano</td> <td>BF16</td> <td>98%</td> </tr> <tr> <td>gemini-2.0-flash</td> <td>FP32</td> <td>100%</td> </tr> <tr> <td>gemini-2.0-flash-lite</td> <td>FP32</td> <td>100%</td> </tr> </tbody> </table> <p>For OpenAI, the pattern appears clear: older models (GPT-3.5, GPT-4) use FP32 logits, while newer models (GPT-4o onwards) use BF16. Gemini 2.0 models use FP32. We note that we measure <em>logit</em> precision specifically, not overall model precision - the model could use mixed precision with different formats for different layers.</p> <p>The imperfect agreement on GPT-4o (3% FP8 E4M3) and GPT-4.1-nano (2% FP16) comes from edge cases at extreme logit magnitudes. For GPT-4o, prompts like “Say the word ‘apple’” produce extremely confident predictions (logit ≈ 14), where BF16’s step size happens to align with FP8 E4M3’s grid.</p> <h2 id="discussion">Discussion</h2> <p><strong>Impact.</strong> When model deployers are choosing which information to expose in their API, they’re juggling a tradeoff between improving utility for users and decreasing the chance that proprietary details get inadvertently leaked. As far as leaks go, the precision of the underlying model is a relatively minor one.</p> <p>Knowing a model’s precision reveals a small amount of detail about its inference infrastructure. The FP32-to-BF16 transition in newer OpenAI models may reflect adoption of lower-precision training and inference pipelines on modern hardware.</p> <p><strong>Limitations.</strong> Our detection method has several limitations. Notably, it requires multiple logprobs from the same forward pass (we use 20). Some APIs expose logprobs but in limited forms: Cohere only provides logprobs for generated tokens, not top-k alternatives at each position, making our attack inapplicable. Our method cannot distinguish between float types that have the same number of mantissa bits, such as FP16 and TF32, though this is the only such collision among standard ML formats. Finally, we detect <em>logit</em> precision, not overall model precision. For example, mixed-precision inference could use different formats for different layers.</p> <p><strong>Responsible disclosure.</strong> We considered the risks before making this attack public. An attacker gains little from knowing a model’s precision alone. More importantly, due to other known vulnerabilities from exposing logprobs, none of the current frontier models from OpenAI, Google, or Anthropic expose logprobs anymore. Given the limited utility to attackers and the trend toward restricted API outputs, we believe publishing this technique poses minimal risk.</p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-precision-extraction.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/witness-problem/">The Witness Problem in Multi-Agent Cooperation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-ai-evaluations-need-error-bars/">Why AI Evaluations Need Error Bars</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>