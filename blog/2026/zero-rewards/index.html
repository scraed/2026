<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> What Can You Do When You Have Zero Rewards During RL? | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="If your base model has zero success rates, performing RL with outcome rewards won't do anything. What can you do then? ü§î&lt;br&gt;TL;DR: simply adding easy samples to your training dataset can unlock RL training!"> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/zero-rewards/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "What Can You Do When You Have Zero Rewards During RL?",
            "description": "If your base model has zero success rates, performing RL with outcome rewards won't do anything. What can you do then? ü§î<br>TL;DR: simply adding easy samples to your training dataset can unlock RL training!",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>What Can You Do When You Have Zero Rewards During RL?</h1> <p>If your base model has zero success rates, performing RL with outcome rewards won't do anything. What can you do then? ü§î<br>TL;DR: simply adding easy samples to your training dataset can unlock RL training!</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#key-takeaways">üìå Key Takeaways</a> </div> <div> <a href="#what-can-you-do-when-you-have-zero-rewards">What can you do when you have zero rewards? ü§î</a> </div> <div> <a href="#naive-rl-fails">Naive RL fails</a> </div> <div> <a href="#contemporary-baselines-don-t-really-work-in-the-zero-reward-scenario">Contemporary baselines don‚Äôt really work in the zero-reward scenario</a> </div> <div> <a href="#a-simple-intervention-helps-add-easy-samples-in-your-dataset">A simple intervention helps ‚Äî add easy samples in your dataset</a> </div> <div> <a href="#not-all-easy-samples-work-well">Not all easy samples work well</a> </div> <div> <a href="#just-mix-everything">Just mix everything!</a> </div> <div> <a href="#why-does-this-work">Why does this work?</a> </div> <div> <a href="#conclusions-and-future-work">Conclusions and Future Work</a> </div> <div> <a href="#limitations">Limitations</a> </div> <div> <a href="#appendix">Appendix</a> </div> <ul> <li> <a href="#why-do-the-baselines-fail-case-by-case-analysis">Why do the baselines fail? Case by case analysis</a> </li> </ul> </nav> </d-contents> <blockquote> <p>We recommend reading this blog with the <strong>white</strong> theme/background ‚ö™Ô∏è for the best experience üôÇ</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure1-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure1-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure1.png" class="img-fluid" width="80%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 1:</b> Approaches such as na√Øve RL (<span style="color: #2196F3;">Dr. GRPO</span>), reward densification (<span style="color: #FF9800;">Progress-Reward</span>), credit assignment (<span style="color: #4CAF50;">VinePPO</span>), and diversity incentives (<span style="color: #9C27B0;">BoN Aware Finetuning</span>) fail to solve a hard task.<br><b>A simple intervention of <span style="color: #8BC34A;">mixing easier samples</span> helps unlock RL training!</b></figcaption> </figure> <p>Before we dive into the blog, here are the key takeaways and a quick disclaimer.</p> <h2 id="-key-takeaways">üìå Key Takeaways</h2> <ul> <li>When the base model can‚Äôt solve a task at all (i.e., outcome rewards are always zero during RL training), we show that a <strong>simple data-centric intervention</strong> of adding <em>easier</em> instances of the same task in the training set works surprisingly well!</li> <li>Choice of <em>easy</em> instances that you add matters! Adding only <em>very</em> easy examples doesn‚Äôt help. However, you don‚Äôt need to hunt for the <em>perfect difficulty.</em> Mixing all the easier instances you have works!</li> <li>We benchmark methods that incorporate desirable components to tackle zero outcome rewards such as dense rewards, diversity incentives, and improved credit assignment, and <strong>find none of these to be effective in our zero reward settings</strong>. Since there was no official code for these baselines, we‚Äôre releasing (single-file, hackable) code for our implementation: <a href="https://github.com/anon-zero-rewards/zero-rewards-rl" rel="external nofollow noopener" target="_blank">rl-baselines</a>. Hopefully you‚Äôll find it useful in your own experiments üôÇ</li> <li>We conclude with a simple and practical recipe for RL practitioners: add all available <em>easier</em> instances of the task that one can get their hands on! We also connect our findings to ideas in skill learning and related prior work.</li> </ul> <h3 id="Ô∏è-disclaimer">‚ö†Ô∏è Disclaimer</h3> <p>We focus our experiments on the graph search problem introduced in <d-cite key="bachmann2024pitfalls"></d-cite>. While we haven‚Äôt yet explored other task types, <strong>we believe the insights here are interesting and worth discussing!</strong></p> <p>Now, without further ado, let‚Äôs get started!</p> <h2 id="what-can-you-do-when-you-have-zero-rewards-">What can you do when you have zero rewards? ü§î</h2> <p>The community has spent enormous compute training LLMs with RL on tasks with verifiable rewards, like math problems and code generation. Surprisingly, this works quite well, even though the reward signals are extremely sparse (outcome based only). Much of this success stems from the strong capabilities that base LLMs already possess due to large-scale pre-training.</p> <p>However, if a model is unable to solve a task even after thousands of attempts, performing RL will have no effect, since the gradients will be zero (there is nothing to <em>reinforce</em>).</p> <p>This highlights an inherent assumption when applying RL to language models: the model must have a reasonable probability of solving the task.</p> <p>This begs the question:</p> <blockquote> <p>What can one do if there are zero rewards due to no successful rollouts being sampled by the base model?</p> </blockquote> <p>To address the above problem, one could:</p> <ul> <li> <strong>Option 1: Supervised Finetuning (SFT) on successful traces:</strong> One approach is to perform SFT on successful trajectories from a stronger model, giving the base model non-zero success rates before applying RL. While this works well in practice (we tried it on the graph search problem, where it proved helpful), it can be restrictive, since the RL phase is then biased toward the SFT dataset distribution. Still, due to its simplicity and effectiveness, it is widely used! <strong>For now, let‚Äôs assume we can‚Äôt do this.</strong> </li> <li> <strong>Option 2: Densifying Rewards:</strong> Alternatively, one could apply reward shaping <d-cite key="setlur2024rewarding"></d-cite> to obtain dense rewards that provide a learning signal based on the quality of intermediate steps, even when outcome rewards are zero. One could also explore approaches that improve credit assignment <d-cite key="kazemnejad2024VinePPO"></d-cite> for intermediate steps in reasoning.</li> <li> <strong>Option 3: Encouraging Diversity:</strong> One could also modify the objective to incentivize the model to sample diverse responses during RL finetuning <d-cite key="chow2024inference"></d-cite>, with the hope that at least one of these responses gets a non-zero reward and thereby kick-starts RL training. <blockquote> <p>However, <strong>both Options 2 and 3 fail</strong> on this simple graph task in the zero-outcome-reward scenario which we will talk about below. ‚òπÔ∏è</p> </blockquote> </li> <li> <strong>Option 4: Use a stronger base model:</strong> Of course, you could just start with a bigger or better model that already generates some successful traces, which RL can then build on. But that‚Äôs boring üòÖ, <strong>so we‚Äôll assume we can‚Äôt do that either.</strong> </li> </ul> <p>Outside of these, there aren‚Äôt many obvious ways to address the problem. Let us know if you think otherwise <strong>üôÇ</strong>.</p> <p>Before we move onto our solution, we describe the task and show that both naive RL and reward densification are ineffective.</p> <p>Let‚Äôs start unpacking!</p> <h2 id="whats-the-task">What‚Äôs the task?</h2> <p>The task involves finding a path from the source node to the target in a star-shaped graph <d-cite key="bachmann2024pitfalls"></d-cite>. The source node is always the center of the star, and the target node is one of the leaf nodes on any branch. Note that this task is difficult for a transformer to solve by directly outputting the path (<d-cite key="bachmann2024pitfalls"></d-cite>, <d-cite key="saparov2025transformers"></d-cite>). However, performing reasoning first and then outputting the path helps the transformer search in context for the path from the source to the target. Refer Fig. 2 for an illustration of the task.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure2-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure2-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure2.png" class="img-fluid" width="80%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2: A Degree-3-Path-3 graph in which the center node has degree 3, and each outgoing path contains 3 nodes. In this example, node 4 is the source and node 7 is the destination, and the model is expected to output the path ‚Äò4, 2, 7.‚Äô This figure is adapted from Bachmann et al. (2024).</figcaption> </figure> <h2 id="naive-rl-fails">Naive RL fails</h2> <p>For our experiments, we focus on a Degree-10-Path-10 graph ‚Äî i.e., a graph where the center node has degree 10, and each outgoing path has 10 nodes.</p> <p>Before diving into the main ideas, let us first examine na√Øve RL using GRPO on this task. As shown in Fig. 1, rewards remain at zero across 1,000 iterations of RL training, resulting in zero gradients.</p> <p>All experiments were conducted on four NVIDIA H100 GPUs using <code class="language-plaintext highlighter-rouge">Qwen2.5-1.5B-Instruct</code> as the base model, with a maximum wall-clock time of 24 hours or 1,000 RL iterations, whichever occurred first.</p> <h2 id="contemporary-baselines-dont-really-work-in-the-zero-reward-scenario">Contemporary baselines don‚Äôt really work in the zero-reward scenario</h2> <p><strong>1. Rewarding Progress</strong></p> <p>Here, we briefly show dense rewards in the form of a recently proposed method Progress Rewards <d-cite key="setlur2024rewarding"></d-cite> fails to solve the task.</p> <p>Briefly, Progress Rewards <d-cite key="setlur2024rewarding"></d-cite> aims to reward <em>partial</em> reasoning traces based on the <em>progress</em> they make toward the final answer (akin to intermediate rewards). This essentially means the model being finetuned could be potentially <em>rewarded</em> even when the terminal/outcome reward is zero.</p> <p>Although the method performs well in the original paper, it is less effective on the Degree-10-Path-10 graph, as shown in Fig. 1. The main reason is the base model‚Äôs inability to sample successful trajectories: intermediate rewards are non-zero only when the state value changes before and after a step, but the difficulty of the task makes such changes rare or non-existent.</p> <p><strong>2. Encouraging Exploration and Diversity</strong></p> <p>Orthogonal to dense rewards, a recent approach modifies the RL objective to better align with inference-time requirements (Best-of-N aware finetuning) <d-cite key="chow2024inference"></d-cite>. Since the goal is often to obtain a single correct generation out of N attempts, this method encourages the model to produce at least one correct answer among the N generations, promoting sample diversity. The idea is that any one of these diverse samples can receive a non-zero reward, enabling RL training. However, as shown in Fig. 1, success rates remain flat throughout training. As noted earlier, a key reason is again the base model‚Äôs inability to sample successful trajectories across multiple attempts.</p> <p>In trying these different baselines out, we implemented our version of these since there was no official code. To rule out any implementation issues, we also test on simpler variants of the task where we find all baselines to perform well.</p> <blockquote> <p>Check out the detailed analysis of why the above methods fail in <a href="#appendix">Appendix</a>.</p> </blockquote> <h2 id="a-simple-intervention-helps--add-easy-samples-in-your-dataset">A simple intervention helps ‚Äî add easy samples in your dataset</h2> <p>Starting with a Degree-10-Path-10 dataset, we added samples from an <em>easier</em> task, such as a Degree-5-Path-5 task, and ran RL training using only outcome rewards with Dr. GRPO. Note that we mixed both datasets in equal proportion. This helped the model to solve the original Degree-10-Path-10 task (refer Fig. 3b) without any other modifications to the algorithm.</p> <p>Note that we did <strong>not</strong> provide the model with any instructions on how to solve the hard task. Simply adding easy samples to the dataset helped the model figure out how to solve the hard task.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure3a-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure3a-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure3a-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure3a.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3a: Rewards during RL training on a dataset containing an equal mixture of Degree-5-Path-5 and Degree-10-Path-10 examples. With training, the model successfully solves both types of problems.</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure3b-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure3b-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure3b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure3b.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3b: Success rates on a held-out test set of Degree-10-Path-10 problems show that Degree-5-Path-5 examples help the model solve harder instances from the Degree-10-Path-10 dataset.</figcaption> </figure> </div> </div> <h2 id="not-all-easy-samples-work-well">Not all easy samples work well</h2> <p>Here, we consider mixing two other types of easy datasets in equal proportion with the original Degree-10-Path-10 task. The first easy graph is a Degree-2-Path-5 graph, where the center node has a degree of 2, and each of the outgoing paths has 5 nodes. The second easy graph is a Degree-5-Path-2 graph, where the center node has a degree of 5, and each of the outgoing paths has only 2 nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure4a-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure4a-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure4a-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure4a.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 4a:</b> Rewards during RL training on datasets containing equal mixtures of (i) <span style="color: #FF9800;">Degree-5-Path-2</span> with <span style="color: #FF9800;">Degree-10-Path-10</span> and (ii) <span style="color: #2196F3;">Degree-2-Path-5</span> with <span style="color: #2196F3;">Degree-10-Path-10</span>. With RL training, the model learns to solve only the easier examples in the mixture, resulting in a training reward of ~0.5.</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure4b-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure4b-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure4b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure4b.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 4b:</b> Success Rate on a held out test set of Degree-10-Path-10 examples. In this case both the easier datasets (<span style="color: #FF9800;">Degree-5-Path-2</span> and <span style="color: #2196F3;">Degree-2-Path-5</span>) fail to help transfer to Degree-10-Path-10 task.</figcaption> </figure> </div> </div> <p>On looking at the chain-of-thought traces closely, we observed a few things that could explain the results in Fig. 4.</p> <ul> <li>In the case where <span style="color: #FF9800;">Degree-5-Path-2 mixed with Degree-10-Path-10</span>, most Degree-5-Path-2 instances were solved using just a single lookup in the edge list, without any search. While this strategy worked for the simpler Degree-5-Path-2 instances, it was not helpful for Degree-10-Path-10 instances, where searching down multiple paths was necessary to arrive at the correct answer. The traces resembled those observed when training a model directly on a Degree-10-Path-10 dataset (see the naive RL fails section <a href="#naive-rl-fails">above</a>).</li> <li>In the case where <span style="color: #2196F3;">Degree-2-Path-5 is mixed with Degree-10-Path-10</span>, there were instances where, given a Degree-10-Path-10 problem, the model explored only two branches and then ‚Äòforced‚Äô itself to output the answer path. We believe the model may have learned a strategy that produces an answer after a maximum of two backtracks, which works for Degree-2-Path-5 but is ineffective for Degree-10-Path-10 problems.</li> </ul> <p>Thus, <strong>not all easy samples are effective</strong>, and it is apriori <em>unclear</em> what a model will learn from a particular dataset. One needs to collect samples of the <em>right</em> difficulty, meaning samples that the model can solve correctly and that encourage behavior that transfers to the original task. These requirements make this approach cumbersome üôÅ.</p> <p>Another point to note is that, although it might be easier to guess which tasks have solutions that will not generalize to larger instances in a simple graph problem like this (e.g., reading off the edge list works for Degree-5-Path-2 examples but does not generalize to Degree-10-Path-10), in general, for any arbitrary problem, it is difficult to predict what the model will learn and which solutions will generalize to harder instances.</p> <p>But is all lost?</p> <h2 id="just-mix-everything">Just mix everything!</h2> <p>If we add not only Degree-2-Path-5 (or Degree-5-Path-2) but also Degree-5-Path-5 to the Degree-10-Path-10 dataset, we observe from Fig. 5b that the model once again begins to solve the Degree-10-Path-10 task!</p> <p>This means that instead of trying to choose the right version of the <em>easy</em> sample for the original task, one should add all possible versions of easy samples available. Hopefully, <em>at least one of those instances</em> will be of the <em>right</em> difficulty.</p> <p><strong>This actually makes it simpler. The final approach could be just described as:</strong></p> <blockquote> <p><em>add all available easy samples for your task in the original dataset</em></p> </blockquote> <p>This provides a very <strong>practical recipe for an RL practitioner.</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure5a-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure5a-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure5a-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure5a.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 5a:</b> Rewards during RL training on an equal mixture of <span style="color: #2196F3;">Degree-2-Path-5, Degree-5-Path-2, Degree-5-Path-5, and Degree-10-Path-10</span> samples. With training, the model begins to solve both the easy and hard examples from the mixture.</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure5b-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure5b-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure5b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure5b.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 5b:</b> Success Rate on a held out test set of <span style="color: #2196F3;">Degree-10-Path-10 examples</span>. With training, the model begins to solve harder examples.</figcaption> </figure> </div> </div> <h2 id="why-does-this-work">Why does this work?</h2> <p>One thing to note is that this is not a free lunch. There is an underlying assumption: we rely on the availability of easier samples for the task. While these may be difficult to obtain in general, they seem reasonable for most real-world tasks.</p> <p>Now we talk about why this works.</p> <p>We think there could be an interesting perspective of <em>skill learning</em> here. <em>Easier</em> samples help the model learn a <em>skill</em> (correlated action) from outcome rewards only. These <em>skills</em> (or correlated actions) help solve a difficult task that the base model originally could not have solved (i.e. these correlated actions transfer to downstream tasks).</p> <p>Another similar perspective is: these correlated actions (or skills) help make the search problem during RL easier ‚Äì since now the search happens in the space of <em>skills</em> (correlated actions) and not in the space of raw tokens. In other words: the action space is reduced, making search easier.</p> <p>To be concrete, one correlated action or skill that is useful for this task is: <strong>going down a branch to a leaf node without hallucinating and then back-tracking.</strong> Another could be to: <strong>explore all branches systematically one by one.</strong> That being said, these are anthropomorphized skills; what a model considers a skill is up to the optimization process during RL üôÇ.</p> <p>For more details about skill learning, we highly recommend this insightful talk by Ben Eysenbach <a href="https://ben-eysenbach.github.io/self-supervised-rl/" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="conclusions-and-future-work">Conclusions and Future Work</h2> <p>We now leave you with a discussion of how this connects to recent results in the community and to the role of datasets in reasoning, where algorithmic innovations have become the trend.</p> <ul> <li>Stojanovski et al. <d-cite key="stojanovski2025reasoninggymreasoningenvironments"></d-cite> talked about faster convergence when using an easy-to-hard curriculum. Setlur et al. <d-cite key="setlur2025e3"></d-cite> also talks about using such a curriculum to help the model gradually extrapolate its in-context search abilities. However, none of these works discuss the scenario when the base model has zero success rates initially.</li> <li>Liu et al. <d-cite key="liu2025prorl"></d-cite> discusses making specific changes to the RL algorithm and training on a much larger dataset mixture. As highlighted in the paper, we believe the dataset mixture, particularly the presence of both easy and hard examples, is a crucial ingredient in enabling RL to scale well beyond its usual number of steps. For instance, in the context of solving Olympiad math problems, a helpful skill, or correlated action, could come from an easy or medium algebra task in Reasoning Gym.</li> <li>Before writing this blog, the dataset used for RL was not very important to us. However, after these experiments, it has become clear just how important data is in RL and in learning the right strategy to solve a task.</li> </ul> <p>This gives us some hints for future work.</p> <ul> <li>If a model cannot solve a difficult task on its own, can it generate easier subtasks for itself, along with a verifier, so that it can first learn to solve the easier subtask and then ultimately tackle the original difficult task? This idea is similar to the recent work by Chen et al. <d-cite key="chen2025self"></d-cite>.</li> <li>Although all our experiments used a uniform mixture of easy and hard samples, varying the proportion of each could influence the convergence speed of different methods. We leave a systematic exploration of this factor for future work.</li> <li>We believe that when assessing the effectiveness of RL algorithms, evaluations should include harder tasks that the base model cannot solve. Demonstrating progress on such challenging tasks would provide concrete evidence of improvements in the models‚Äô exploration capabilities.</li> </ul> <h2 id="limitations">Limitations</h2> <ul> <li>In this study our experiments were only focussed on the graph search problem which is a toy task, Extending such an analysis to more realistic settings would be an interesting follow up.</li> <li>Even though we mix samples from all difficulty levels and found this approach to work in our setup, the interaction of the mixture with RL optimization could mean that, even if examples of the right difficulty are present in the dataset, the model may still fail to learn the correct solution. We leave further exploration and analysis of this dataset mixture with RL optimization for future work.</li> <li>There is again an inherent requirement for a good pre-trained base model, one that achieves non-zero success rates on easier tasks. In our setting, this translates to the assumption of have non-zero success rates on Degree-5-Path-5 instances, which allows us to bootstrap RL training on Degree-10-Path-10.</li> </ul> <h2 id="appendix">Appendix</h2> <h3 id="why-do-the-baselines-fail-case-by-case-analysis">Why do the baselines fail? Case by case analysis</h3> <p>As we discussed above, <strong>a key reason for the failure of some baselines on the Degree-10-Path-10 task is the base model‚Äôs inability to occasionally sample correct trajectories.</strong> Here we provide a more detailed analysis of why each baseline fails in this zero-reward scenario.</p> <p>Before we begin, here are the quick takeaways:</p> <ul> <li>Instantiating Progress Rewards <d-cite key="setlur2024rewarding"></d-cite> is <strong>practically challenging</strong>.</li> <li>There is a requirement of a capable base model <strong>to begin with</strong> for VinePPO <d-cite key="kazemnejad2024VinePPO"></d-cite> and to possibly resolve unstable training of Best-of-N aware finetuning <d-cite key="chow2024inference"></d-cite>.</li> </ul> <h3 id="1-dense-rewards-are-not-really-dense-in-vineppo-and-progress-rewards">1. Dense rewards are not really dense in VinePPO and Progress Rewards</h3> <p>Methods like VinePPO <d-cite key="kazemnejad2024VinePPO"></d-cite> and Progress Rewards <d-cite key="setlur2024rewarding"></d-cite> go beyond Dr.GRPO by computing step-level advantages. <strong>However, non-zero step-level advantages are obtained only when there is a change in the value of the state before and after taking the step.</strong> This means that for VinePPO to produce a non-zero advantage for a step, some of the rollouts under the current policy must succeed. Similarly, for the Progress Rewards, some of the rollouts under the prover policy must succeed. In our setting, we observe that throughout training, neither the current policy nor the policy generates a successful rollout. Thus step-level advantages offer no learning signal.</p> <h3 id="2-instantiating-a-helpful-prover-to-get-a-meaningful-progress-rewards-is-hard">2. Instantiating a helpful prover to get a meaningful Progress Rewards is hard</h3> <p>The Progress Rewards <d-cite key="setlur2024rewarding"></d-cite> work notes that a prover that is <em>too strong or too weak</em> is ineffective: a strong prover cannot distinguish between <em>good</em> and <em>bad</em> steps, while a weak prover fails from most intermediate states, resulting in zero step-level advantages and no learning. Consequently, they identify two desirable properties for provers: (i) the prover should neither be too strong nor too weak, and (ii) it should be reasonably aligned with the policy being optimized.</p> <p>To satisfy these requirements for the Degree-10-Path-10 graph, we experiment with two provers. The first prover: $\pi_{5 \times 5}$, is a model trained using Dr.GRPO on the Degree-5-Path-5 task and achieves around 65% accuracy on Degree-10-Path-10, partially satisfying the first property. The second prover, $\pi_{5 \times 5 \text{ mixed with } 10 \times 10}$, is trained using Dr.GRPO on an equal mixture of Degree-5-Path-5 and Degree-10-Path-10 graphs and reaches around 85% accuracy on Degree-10-Path-10.</p> <p>As shown in Fig. 6, both provers have a reasonable success rate on the Degree-10-Path-10 task from the start, indicated through non-zero step-level advantages early in training. The $\pi_{5 \times 5}$ prover provides non-zero step-level advantages about 50% of the time, while the $\pi_{5 \times 5 \text{ mixed with } 10 \times 10}$ prover does so about 60% of the time. However, as seen in Fig. 6, this signal does not lead to better task performance. In both cases, the model responses often become degenerate, repeating characters or words to fill the context window. <strong>We believe this happens because the prover policy is not well aligned with the policy being optimized,</strong> possibly violating the second desirable property mentioned above, even though the prover was obtained using the same base model as the policy (you‚Äôd hope this should be a <em>good</em> <em>alignment</em>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure6a-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure6a-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure6a-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure6a.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 6a:</b> Fraction of non-zero step advantages for two provers: <span style="color: #FF9800;">Œº = Best-of-4(œÄ<sub>5√ó5</sub>)</span> and <span style="color: #2196F3;">Œº = Best-of-4(œÄ<sub>5√ó5-mixed-with-10√ó10</sub>)</span>, where the models were trained on Deg-5-Path-5 alone or mixed with Deg-10-Path-10, respectively. Both models provide non-zero step advantages for Progress Rewards due to their reasonable success rates on the harder task.</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure6b-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure6b-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure6b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure6b.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 6b:</b> Success rate on a held-out test set of Degree-10-Path-10 examples. Despite using the same two provers, both models fail on the Degree-10-Path-10 task. We believe this is because the prover policy is not well aligned with the policy being optimized.</figcaption> </figure> </div> </div> <h3 id="3-unstable-training-in-best-of-n-aware-finetuning">3. Unstable training in Best-of-N aware finetuning</h3> <p>We followed the practices suggested in <d-cite key="chow2024inference"></d-cite>, including (a) using a KL schedule and (b) clipping the sample-dependent weights multiplied by the log probability (Eq. 9, Lemma 3 in <d-cite key="chow2024inference"></d-cite>). Notably, we observed that the KL schedule in <d-cite key="chow2024inference"></d-cite> is quite aggressive, starting with a coefficient of 1 and decaying to 0.001, whereas the current standard is to keep it constant at 0.001 <d-cite key="kazemnejad2024VinePPO"></d-cite>. They also clip the failure probability (\(p_{\text{fail}}\)). During trainin, we observed large sample-dependent weights (\(g_N^+\) and \(g_N^-\) in Eq. 9, Lemma 3 in <d-cite key="chow2024inference"></d-cite>) ‚Äì we countered these large weights by directly clipping them to stabilize training. Unfortunately, interventions did not enable the model to solve the Degree-10-Path-10 dataset.</p> <p>To investigate further, we applied the method to the Degree-5-Path-5 dataset which Dr.GRPO can effectively solve (see Fig. 7). The model still did not solve the task. We believe a major reason is the presence of very high negative gradients. When the failure probability (\(p_{\text{fail}}\)) is close to 1, the sample-dependent weights become large, and multiplying them by negative log probabilities produces high magnitude negative gradients. This drives the model responses toward degeneracy where it repeats the same set of characters. Fig. 7 shows this effect: with a <span style="color: #FF9800;">lower KL penalty</span> (0.001), the model‚Äôs response lengths increase rapidly, and inspection of the outputs confirms degeneracy, while success rates remain zero. Using a <span style="color: #2196F3;">strong-to-weak KL penalty</span> (0.1 to 0.001) stabilizes training but does not help solve the hard task.</p> <p>We believe that the issue of high negative gradients can be mitigated by starting with a capable base model. Such a model has a lower failure probability (\(p_{\text{fail}}\) in Eq. 9, Lemma 3 in <d-cite key="chow2024inference"></d-cite>), which keeps the sample-dependent weights \(g_N^+\) and \(g_N^-\) (in Eq. 9, Lemma 3 in <d-cite key="chow2024inference"></d-cite>) within a reasonable range, thus ensuring stable training. As shown in Fig. 8, Best-of-N aware finetuning is able to solve the Degree-3-Path-3 task, likely due to its relatively lower initial failure rates compared to Degree-5-Path-5.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure7a-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure7a-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure7a-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure7a.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 7a:</b> Response lengths with different KL schedules on Deg-5-Path-5. Using a <span style="color: #FF9800;">lower KL coefficient</span> (0.001) in Best-of-N aware finetuning results in unstable training due to large-magnitude negative gradients, causing model responses to degenerate into repeating the same character.</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure7b-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure7b-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure7b-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure7b.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 7b:</b> Success rate with different KL schedules on Deg-5-Path-5. Using a <span style="color: #2196F3;">KL schedule</span> as recommended in <d-cite key="chow2024inference"></d-cite> (decaying from 0.1 to 0.001) remains stable but fails to learn, as success rates stay at zero.</figcaption> </figure> </div> </div> <h3 id="4-sanity-check-are-we-sure-our-implementations-for-baselines-are-correct">4. Sanity check: Are we sure our implementations for baselines are correct?</h3> <p>To ensure our implementations of the baselines were correct, we ran sanity checks on easier variants of the task (Degree-3-Path-3) where the base model has non-zero success rates.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure8-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure8-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure8.png" class="img-fluid" width="50%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 8:</b> Success rates of different RL algorithms (<span style="color: #2196F3;">Dr.GRPO</span>, <span style="color: #4CAF50;">VinePPO</span>, <span style="color: #FF9800;">Progress Rewards</span>, and <span style="color: #9C27B0;">Best-of-N aware finetuning</span>) on a held-out test set of Degree-3-Path-3 graphs. These models were trained on Degree-3-Path-3 graphs. All algorithms are able to solve the task when the model starts with a reasonable success rate. Furthermore, <span style="color: #4CAF50;">VinePPO</span> converges in fewer iterations compared to <span style="color: #2196F3;">Dr.GRPO</span>, consistent with findings reported in the literature.</figcaption> </figure> <h3 id="5-sanity-check-easier-versions-of-the-task-are-solvable">5. Sanity check: Easier versions of the task are solvable</h3> <p>We confirm that the base model can solve easier variants of the task, such as Degree-3-Path-3 and Degree-5-Path-5, as shown in Fig. 9.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-zero-rewards/figure9-480.webp 480w,/2026/assets/img/2026-04-27-zero-rewards/figure9-800.webp 800w,/2026/assets/img/2026-04-27-zero-rewards/figure9-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-zero-rewards/figure9.png" class="img-fluid" width="50%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"><b>Figure 9:</b> Success Rate of Dr.GRPO on held out test sets of different levels of difficulty. The model manages to solve simpler variants (<span style="color: #2196F3;">Degree-3-Path-3</span> and <span style="color: #FF9800;">Degree-5-Path-5</span>), but is unable to solve the harder <span style="color: #4CAF50;">Degree-10-Path-10</span> variant.</figcaption> </figure> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-zero-rewards.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/witness-problem/">The Witness Problem in Multi-Agent Cooperation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-ai-evaluations-need-error-bars/">Why AI Evaluations Need Error Bars</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>