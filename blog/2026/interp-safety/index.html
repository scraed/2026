<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Charting the Depths: Interpretability Tools to Enhance LLM Safety | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Motivated by the increasing deployment of LLMs for safety-critical applications, we provide an accessible introduction to a practical suite of interpretability tools useful for understanding LLMs’ behavior during safety-critical decisions. Previous discussions of interpretability are often heavily focused on these methods' technical aspects, rather than giving practical guidance for their immediate use; here, we provide practitioners with an overview of a range of methods for understanding LLM behavior. For each method covered, we highlight what it can and cannot tell us, and how this can help inform deployment decisions."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/interp-safety/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Charting the Depths: Interpretability Tools to Enhance LLM Safety",
            "description": "Motivated by the increasing deployment of LLMs for safety-critical applications, we provide an accessible introduction to a practical suite of interpretability tools useful for understanding LLMs’ behavior during safety-critical decisions. Previous discussions of interpretability are often heavily focused on these methods' technical aspects, rather than giving practical guidance for their immediate use; here, we provide practitioners with an overview of a range of methods for understanding LLM behavior. For each method covered, we highlight what it can and cannot tell us, and how this can help inform deployment decisions.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Charting the Depths: Interpretability Tools to Enhance LLM Safety</h1> <p>Motivated by the increasing deployment of LLMs for safety-critical applications, we provide an accessible introduction to a practical suite of interpretability tools useful for understanding LLMs’ behavior during safety-critical decisions. Previous discussions of interpretability are often heavily focused on these methods' technical aspects, rather than giving practical guidance for their immediate use; here, we provide practitioners with an overview of a range of methods for understanding LLM behavior. For each method covered, we highlight what it can and cannot tell us, and how this can help inform deployment decisions.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#the-challenge-of-model-interpretability">The Challenge of Model Interpretability</a> </div> <div> <a href="#exploring-the-surface-model-behavior-analysis">Exploring the Surface: Model Behavior Analysis</a> </div> <div> <a href="#diving-deeper-model-introspection">Diving Deeper: Model Introspection</a> </div> <ul> <li> <a href="#method-one-probing">Method One: Probing</a> </li> <li> <a href="#method-two-sparse-autoencoders">Method Two: Sparse Autoencoders</a> </li> </ul> <div> <a href="#what-s-next">What's Next?</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Let’s say we have an AI model that’s making an important decision: given a student’s college recommendation letter, output a numerical score (1-9) representing the student’s admissibility. LLMs and other AI models are on track to become part of admission processes. But how can we be confident in their behavior? In this article, we take the perspective of AI safety (rather than AI policy). Thus, instead of defining safe or ideal behaviour for this use case, our goal is to help readers understand some of the methods available to them, given such a definition. More specifically, we’ll describe a range of methods from the rapidly developing field of <strong>interpretability</strong> that can help model developers and deployers enhance safety within AI systems.</p> <p>Let’s assume that we built some kind of recommendation letter scoring model consisting of a fine-tuned LLM that inputs a recommendation letter and outputs a score as the next token. How do we make sure that this model does not do anything that we don’t want it to do, like basing students’ scores on whether or not their letters mention their love of Taylor Swift?</p> <div class="row justify-content-center mt-3"> <div class="col-10 col-sm-8 col-md-6 col-lg-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interp-safety/taylor_no-480.webp 480w,/2026/assets/img/2026-04-27-interp-safety/taylor_no-800.webp 800w,/2026/assets/img/2026-04-27-interp-safety/taylor_no-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interp-safety/taylor_no.gif" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>At a high level, there are three steps where we could intervene.</p> <p>First, we could try to prevent the model from learning that information in the first place. For example, we could filter the training data to make sure that Taylor Swift appeared in both positive and negative contexts.</p> <p>Second, we could evaluate our model to see how it was doing after being trained. Do we actually observe a difference in scoring for students who do or don’t mention Taylor Swift? Since we don’t want a student’s musical taste to have any impact on their recommendation letter score, we’d also need to look at its internals. Here, internals refer to the mechanisms by which the model goes from inputs to outputs. In particular, we’re interested in the particular features or pieces of information that affect the scores being generated. Ideally, we wouldn’t detect music fandoms having any influence on our model’s predictions!</p> <p>Third, if we did find a negative behavior (or a problematic internal association), we would need to either a) perform additional training to correct it, b) take additional steps at deployment time to override the model, or c) simply not deploy the model.</p> <p>Dataset filtering, runtime monitors, and other techniques address the first and third; however, the second remains highly challenging. For this reason, our article focuses on the second step, exploring some ways we can begin to “crack” model internals.</p> <h2 id="the-challenge-of-model-interpretability">The Challenge of Model Interpretability</h2> <p>When it comes to assessing a model’s behavior, we want to know not only <em>what</em> it will do, i.e., directly relating inputs to outputs, but also <em>why</em> it will do this, i.e., understanding the mechanisms behind how this relation occurred. For example, we may find that the model scores fans of Taylor Swift differently from students with other favorite artists. Is this because the model uses being a fan of Taylor Swift as a relevant feature for its output score, or is this merely a statistical artifact?</p> <div class="row justify-content-center mt-3"> <div class="col-12 col-sm-10 col-md-8 col-lg-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interp-safety/billie_why-480.webp 480w,/2026/assets/img/2026-04-27-interp-safety/billie_why-800.webp 800w,/2026/assets/img/2026-04-27-interp-safety/billie_why-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interp-safety/billie_why.gif" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The idea of separating <em>what</em> and <em>why</em> questions naturally allows us to categorize interpretability methods. Generally speaking, methods that work with model inputs and outputs give us our <em>what</em> answers. Methods at this level are precise, but don’t let us go beyond specific data points.</p> <p>We have to dive deeper to learn about why the model does what it does. In this post, we present examples of methods that give general insights about a model’s internal representations, at the expense of precision.</p> <p>Alright, get your snorkels! We’re going to start at the surface with a brief overview of what the input and output spaces can tell us.</p> <h2 id="exploring-the-surface-model-behavior-analysis">Exploring the Surface: Model Behavior Analysis</h2> <p>Without diving deeply into model internals, we can learn a lot about a model’s behavior by simply examining <em>what it says</em> as a function of <em>what we put in</em>. Treating models like black-box functions (which is our only option for most proprietary models) allows us to ask and answer questions without making any assumptions about what’s happening under the hood.</p> <p>Back to our college admissions example. We were worried that the model we’re using to rate recommendation letters was treating Taylor Swift fans unfairly. How do we know if this might be the case? As a <em>first</em> step, let’s take a look at the relationship between input space and output space.</p> <p>What if all we have is a model’s output? It turns out that, as discussed by <d-cite key="morris2024inversion"></d-cite>, this can actually tell us quite a bit <em>about the input!</em> Let’s take one of our recommendation letters as an example input: “They are an excellent mathematics student and a huge Taylor Swift fan.” Suppose our model has generated the output: “4.” One experiment we can do is to swap out different musical artists to see how much the likelihood of “4” changes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interp-safety/fig_1-480.webp 480w,/2026/assets/img/2026-04-27-interp-safety/fig_1-800.webp 800w,/2026/assets/img/2026-04-27-interp-safety/fig_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interp-safety/fig_1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Using this measure, we can test different inputs and explore how much the output distribution changes. Which part of our recommendation letter mattered most in producing a final score? The artist “Taylor Swift”? The subject “mathematics”? The adjective “excellent”?</p> <p>This approach can give us a sense of which words are most impactful on a particular outcome, but it doesn’t tell us whether or not the original input was within the model’s training distribution. <d-cite key="xiong2024confidence"></d-cite> presents a range of strategies to help LLMs calibrate their own uncertainty, including using human-inspired prompts, strategically sampling multiple responses, and defining an aggregation method that allows computing consistency among responses. These options give us additional insights into a model’s confidence about a particular input. Practically speaking, this allows us to handle cases flagged as OOD differently, for example by routing to a human expert for verification.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interp-safety/fig_2-480.webp 480w,/2026/assets/img/2026-04-27-interp-safety/fig_2-800.webp 800w,/2026/assets/img/2026-04-27-interp-safety/fig_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interp-safety/fig_2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>So, what <em>can’t</em> methods examining the input and output space tell us? A lot, actually. Because they require starting from inputs that either are specified by us pre-deployment or created by the users post-deployment, they don’t generalize to things that we haven’t considered or observed. Although they allow us to approximate the impact of a given word on an outcome, or a model’s certainty about an input, they don’t tell us anything about <em>why</em> the model’s predictions happen.</p> <p>If you’ve made it this far, grab your fins and an oxygen tank! It’s time to go beneath the surface.</p> <h2 id="diving-deeper-model-introspection">Diving Deeper: Model Introspection</h2> <p>Let’s suppose that through some computational magic, we were able to measure our model’s output for every possible music preference.</p> <p>If it turned out that all artists were treated equally, i.e., had the same distribution of scores, could we conclude that our model was behaving in the way we wanted it to?</p> <p>Yes… Well, yes, if we only care about <em>what</em> the model is doing and not <em>why</em>.</p> <p>Here are two reasons “why” is important. First, understanding a model’s “inner model” can allow us to make predictions about how it will behave out of distribution. For example, <d-cite key="zhou2024fourier"></d-cite> shows that LLMs use Fourier features when solving addition problems suggesting that they will be able to generalize to problems that weren’t contained in their training data, since their internal model of arithmetic aligns with ours. Within a problem space of clear correct answers, a good inner model suggests good performance in the future, even if the input distribution changes.</p> <p>Our second reason for caring about why comes from the space of problems that don’t have a right/wrong answer and are evaluated based on preferences. You can imagine many different recommendation letter scoring models that end up with the same score distribution. However, one that was basing its decisions on grades and classroom behavior would be much better than one basing them on musical preferences.</p> <div class="row justify-content-center mt-3"> <div class="col-12 col-sm-10 col-md-8 col-lg-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interp-safety/bad_bunny_better-480.webp 480w,/2026/assets/img/2026-04-27-interp-safety/bad_bunny_better-800.webp 800w,/2026/assets/img/2026-04-27-interp-safety/bad_bunny_better-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interp-safety/bad_bunny_better.gif" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Given LLMs’ text-likelihood-maximizing pre-training objective and capacity for memorization, it’s reasonable to think they would do nothing more than model statistical patterns and make (sometimes spurious) correlations <d-cite key="bender2021parrots"></d-cite>. In fact, they have been shown to meaningfully model higher-level concepts. In one notable example, <d-cite key="li2023othello"></d-cite> asked a GPT model to predict moves in the board game Othello, which it had never seen before. Using probes (which we’ll describe in detail momentarily), they discovered that the model had an internal representation of the board state that could be interpreted geometrically.</p> <p>Works like these established LLMS do, in fact, have internal concepts worth exploring. How exactly can you do that? We’re glad you asked!</p> <h3 id="method-one-probing">Method One: Probing</h3> <p>Probing methods are based on a very sensible assumption: if we can train a simple (linear) model to predict some complex attribute (e.g., a student’s favorite artist) based on the internal activations of another model (e.g., our recommendation letter scorer), then the latter model must be encoding that attribute since the linear model isn’t powerful enough to discover this attribute on its own.</p> <p>The method proposed by <d-cite key="hewitt-manning-2019-structural"></d-cite> tests whether LLMs encode syntax dependency parses by doing just that. They successfully train a model to predict syntax tree distance between all pairs of words in all sentences of a corpus, giving the model only an LLM’s hidden representations of the sentences. It turns out the model really was representing syntax!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interp-safety/fig_3-480.webp 480w,/2026/assets/img/2026-04-27-interp-safety/fig_3-800.webp 800w,/2026/assets/img/2026-04-27-interp-safety/fig_3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interp-safety/fig_3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>How exactly does this work? <d-cite key="hewitt-manning-2019-structural"></d-cite> begins by producing input pairs (x,y) where x is the hidden state of an LLM after inputting a particular sequence and y is the trait being probed. Given a dataset in this form, they fit a matrix to the data with the objective of minimizing the difference between the true label and the one assigned by the matrix. This allows us to answer the question: how much can a simple model learn about y from x alone?</p> <p>Instead of searching over all possible favorite colors and favorite artist inputs, probing allows us to determine whether our model has a (linear) internal representation of these attributes. Depending on the attributes identified, we may decide the model isn’t able to perform the prediction task in an unbiased way. If it’s easy for a classifier to guess a student’s favorite artist from the internal states of a model scoring their recommendation letter, then that information is probably way too heavily used.</p> <p>Although this is a great thing to be able to check, there are a few notable things that probing doesn’t tell us. Since we need to know what we’re looking for, probing doesn’t give us any information about concepts we hadn’t thought to test. We also don’t know where within the model that information might be encoded, so to localize it we would need to probe every layer. Probing also assumes that the information we’re looking for within the LLM is linearly encoded. Although it has worked well in practice <d-cite key="wu2025axbench"></d-cite>, , it seems reasonable that significant amounts of information are not linearly encoded. If we observe that a probe succeeds, it’s reasonable to conclude that our model encoded the information we were probing for. However, independent of the probing model’s complexity, we can’t use this approach to make conclusions about the inverse: if the probe fails, our model may <em>still know the thing we were probing for!</em></p> <div class="row justify-content-center mt-3"> <div class="col-10 col-sm-8 col-md-6 col-lg-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interp-safety/ariana_know-480.webp 480w,/2026/assets/img/2026-04-27-interp-safety/ariana_know-800.webp 800w,/2026/assets/img/2026-04-27-interp-safety/ariana_know-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interp-safety/ariana_know.gif" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="method-two-sparse-autoencoders">Method Two: Sparse Autoencoders</h3> <p>This brings us to the second method for this section: Sparse Autoencoders.</p> <p>Maybe our recommendation letter scoring model is using a concept we hadn’t thought of, or maybe (and by maybe, we mean it’s extremely likely) we simply want to understand its internal processes before deploying it in the world.</p> <p>Sparse Autoencoders (SAEs), or more general disentangling methods, offer us one lens for looking deeper inside LLMs. The sparsity of an SAE is created by a constraint: for any given input, the majority of latent neurons should be inactive. As a result, the SAE has to be <em>very</em> selective about the features it considers.</p> <p>This approach to interpretability is based on <em>sparse dictionary learning</em>. We can represent the internal activations of our LLM as a set of vectors \(X\). Assuming that this set can be created through a sparse, linear combination of unknown vectors \(G\) (which is exactly the assumption that SAEs make!), then we can learn a “dictionary” of vectors \(F\), where each network feature \(g_i\) has a corresponding dictionary feature \(f_j\) that approximates it. The goal of SAEs is to learn this dictionary set \(F\), since once we have it we can make inferences about the model’s internal feature set \(G\). <d-cite key="huben2024sparse"></d-cite> approaches this by training an autoencoder that inputs an LLM’s hidden activations given input \(x\) and outputs a reconstruction \(\hat{x}\) of that input. To achieve the desired dictionary property, <d-cite key="huben2024sparse"></d-cite> adds a sparsity penalty to the autoencoder. The model must then learn which features of the hidden activations are <em>most important</em> for the current input.</p> <p>How does this help us understand what’s going on inside an LLM? It turns out that when the authors of <d-cite key="huben2024sparse"></d-cite> examined which neurons were activated for different inputs, they were able to find highly interpretable features like names and the use of legal terms.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interp-safety/fig_4-480.webp 480w,/2026/assets/img/2026-04-27-interp-safety/fig_4-800.webp 800w,/2026/assets/img/2026-04-27-interp-safety/fig_4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interp-safety/fig_4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Does that mean if we applied this to our college admissions example, we could find all of the features that our model considered most important? Sadly, it does not. Interpreting the features the SAE identifies can be difficult (and in fact, recent work <d-cite key="hewitt2025position"></d-cite> proposes we need to understand AI models as having distinct concepts that we humans don’t have!). It’s also not clear how well these representations truly reflect what’s going on inside the LLM. More complex interactions could be obscured by the SAE. In short: SAEs do</p> <p>In short: SAEs do not provide comprehensive information about an LLM’s dynamics. The good news is that when we are able to interpret the features they locate, we can gain insight into what our model is using for its predictions. We can use these to assess how effectively our model is disentangling traits, e.g., favorite musical artist and gender. Additionally, SAEs allow us to explore concept-based interventions, e.g., by zeroing out neurons in the model associated with a concept we want it to ignore when making a prediction.</p> <h2 id="whats-next">What’s Next?</h2> <p>If we had applied all of the methods described in this post, what could we have learned?</p> <ul> <li>Making discrete changes to a specific input and measuring the effect on the output score distribution would tell us how impactful certain terms were on our observed outcome. <ul> <li> <strong>Example:</strong> we could measure how important swapping favorite artists was vs swapping GPAs.</li> </ul> </li> <li>Confidence elicitation would give us more information about how certain the model was in its predictions on specific inputs. <ul> <li> <strong>Example:</strong> we could estimate how confident the model was scoring each student and route the least confident predictions to a human for validation.</li> </ul> </li> <li>Probing would let us assess whether our model had internal representations of particular topics we cared about. <ul> <li> <strong>Example:</strong> we could train a probe to classify favorite music genres to see how clearly encoded this was within our scoring model.</li> </ul> </li> <li>Sparse autoencoders would allow us to explore the space of topics especially relevant to the model, provided we could interpret them once we found them. They also let us make interventions on parts of the model associated with these concepts. <ul> <li> <strong>Example:</strong> we could train an SAE on our recommendation scoring model and then search its embeddings for identifiable features like music preferences and favorite colors.</li> </ul> </li> </ul> <p>Generalizing beyond this example, we present the table below with (non-exhaustive) descriptions of what the methods can and can’t tell us, and how this information might practically be used.</p> <table> <thead> <tr> <th><strong>Method</strong></th> <th><strong>What can it tell us?</strong></th> <th><strong>What can’t it tell us?</strong></th> <th><strong>What can we do with this information?</strong></th> </tr> </thead> <tbody> <tr> <td>Exploring discrete changes</td> <td>For a <strong>specific</strong> set of inputs and a <strong>specific</strong> set of features, how does changing those features affect a model’s outputs?</td> <td>What happens if I have [insert input not in the set here] or [insert feature not in the set here]? <br> <br> How do the features (e.g., an artist’s name) relate to higher-level concepts (e.g., gender) within the model?</td> <td>If we identify features that should have a low (or non-existent) impact on the final prediction (e.g., gender), then we can re-train the model or choose not to deploy it.</td> </tr> <tr> <td>Confidence elicitation</td> <td>What is our <strong>estimate</strong> of how confident the model is in its prediction on a given input?</td> <td>How confident <em>should</em> the model be (i.e., how representative is this input of its training data)? <br> <br> How close is the model’s prediction (independent of confidence) to the behavior that we want it to have?</td> <td>If our model has low confidence for an input, we can route it to an alternative model (or a person!)</td> </tr> <tr> <td>Probing</td> <td>Are we able to find a <strong>specific</strong> concept in a model’s internal representations?</td> <td>Does the model have an internal representation of this concept? (A negative result doesn’t mean that the model isn’t representing the concept in some way, since we’re reliant on the quality of our classifier).<br><br>Exactly how well is the model encoding this trait? (We can get a relative sense of this, but not an absolute one)<br><br> How is this concept (compared and in addition to other ones) being used to make our model’s final prediction?</td> <td>If there is a concept that we know the model shouldn’t be using in its final prediction (e.g., if we want it to be agnostic to favorite artist), and we can effectively predict it with a probe, then we can choose not to deploy the model and attempt to train one that does not encode this concept. As a caveat here, we recommend checking out this paper <d-cite key="gonen-goldberg-2019-lipstick-pig"></d-cite>. Attempts to remove bias may end up obscuring it superficially!</td> </tr> <tr> <td>Sparse autoencoders</td> <td>What are some <strong>interpretable</strong> concepts that our model has an internal representation of? <br> <br> When (i.e., for which inputs and outputs) are the identified concepts used?</td> <td>Is the model using [insert concept the SAE didn’t find here]?<br><br>To exactly what degree are the identified concepts A and B entangled?</td> <td>If we identify interpretable concepts, we can look at which of these were used for particular predictions and update our decisions about deployment accordingly. For example, finding out that our model learned to specifically watch for red car drivers in college admission maybe a bad sign.<br><br>We note that if an SAE does not identify a particular concept, this does not a guarantee that it isn’t being used. The concept could be encoded in a more complex way, and by nature of sparsity, many less-used concepts are being dropped by the SAE (but not by the LLM).</td> </tr> </tbody> </table> <p>Yes, there’s a lot we can’t know! Depending on the application, that may be reason enough for not deploying an AI system. However, there is also a lot we can know and, given that information, a lot we can do.</p> <p>Since work on identifying circuits <d-cite key="elhage2021mathematical"></d-cite> and causal mechanisms <d-cite key="geiger2021causal"></d-cite> within LLMs is still at the frontier and is not yet actionable under many circumstances, we’re currently limited to assessing concepts, rather than full pathways and processes. There are cases where we’d want to have both. Existing methods in the input/output space give us information about <em>what</em> an LLM is doing, mechinterp approaches tell us more about <em>why</em>, but there’s a remaining gap for <em>how</em>. For many human processes (like college admissions), we care about all three!</p> <p>College admissions is only one of the many critical decision-making domains where LLMs are being used. In order to deploy as safely as possible, or avoid unsafe deployment when it is not possible, model developers should answer as many questions about their models as possible. This post has covered a few of the many promising methods available. We hope it’s inspired you to take the dive!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-interp-safety.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/beyond-attention-as-graph/">Beyond Attention as a Graph</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/attention-sinks-graph-perspective/">Attention Sinks from the Graph Perspective</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/zero-rewards/">What Can You Do When You Have Zero Rewards During RL?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-ai-evaluations-need-error-bars/">Why AI Evaluations Need Error Bars</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>