@inproceedings{zhang2024llm,
title={LLM alignment through successive policy re-weighting (SPR)},
author={Xinnan Zhang and Siliang Zeng and Jiaxiang Li and Kaixiang Lin and Mingyi Hong},
booktitle={NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability},
year={2024}
}

@inproceedings{NIPS2016_2f885d0f,
title = {Reward augmented maximum likelihood for neural structured prediction},
author = {Norouzi, Mohammad and Bengio, Samy and Chen, zhifeng and Jaitly, Navdeep and Schuster, Mike and Wu, Yonghui and Schuurmans, Dale},
booktitle = {Proc. NIPS},
year = {2016}
}

@article{peng2019advantage,
title={Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
journal={arXiv:1910.00177},
year={2019}
}

@article{du2025simplify,
title={Simplify rlhf as reward-weighted sft: A variational method},
author={Du, Yuhao and Li, Zhuo and Cheng, Pengyu and Chen, Zhihong and Xie, Yuejiao and Wan, Xiang and Gao, Anningzhe},
journal={arXiv:2502.11026},
year={2025}
}

@article{ming2025one,
  title={One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient},
  author={Ming, Rui and Wu, Haoyuan and Hu, Shoubo and He, Zhuolun and Yu, Bei},
  journal={arXiv:2509.26313},
  year={2025}
}

@article{qin2025supervised,
title={Supervised fine tuning on curated data is reinforcement learning (and can be improved)},
author={Qin, Chongli and Springenberg, Jost Tobias},
journal={arXiv:2507.12856},
year={2025}
}

@article{wu2025generalization,
  title={On the generalization of sft: A reinforcement learning perspective with reward rectification},
  author={Wu, Yongliang and Zhou, Yizhou and Ziheng, Zhou and Peng, Yingzhe and Ye, Xinyu and Hu, Xinting and Zhu, Wenbo and Qi, Lu and Yang, Ming-Hsuan and Yang, Xu},
  journal={arXiv:2508.05629},
  year={2025}
}

@article{wang2025implicit,
title={Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections},
author={Wang, Bo and Cheng, Qinyuan and Peng, Runyu and Bao, Rong and Li, Peiji and Guo, Qipeng and Li, Linyang and Zeng, Zhiyuan and Zhou, Yunhua and Qiu, Xipeng},
journal={arXiv:2507.00018},
year={2025}
}

@inproceedings{abdolmaleki2024learning,
title={Learning from negative feedback, or positive feedback or both},
author={Abdolmaleki, Abbas and Piot, Bilal and Shahriari, Bobak and Springenberg, Jost Tobias and Hertweck, Tim and Joshi, Rishabh and Oh, Junhyuk and Bloesch, Michael and Lampe, Thomas and Heess, Nicolas and others},
booktitle={Proc. ICLR},
year={2025}
}

@article{fu2025srft,
title={SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning},
author={Fu, Yuqian and Chen, Tinghong and Chai, Jiajun and Wang, Xihuai and Tu, Songjun and Yin, Guojun and Lin, Wei and Zhang, Qichao and Zhu, Yuanheng and Zhao, Dongbin},
journal={arXiv:2506.19767},
year={2025}
}

@article{yan2025learning,
title={Learning to reason under off-policy guidance},
author={Yan, Jianhao and Li, Yafu and Hu, Zican and Wang, Zhi and Cui, Ganqu and Qu, Xiaoye and Cheng, Yu and Zhang, Yue},
journal={arXiv:2504.14945},
year={2025}
}

@article{zhu2023fine,
title={Fine-tuning language models with advantage-induced policy alignment},
author={Zhu, Banghua and Sharma, Hiteshi and Frujeri, Felipe Vieira and Dong, Shi and Zhu, Chenguang and Jordan, Michael I and Jiao, Jiantao},
journal={arXiv:2306.02231},
year={2023}
}

@inproceedings{mukherjeeoffline,
title={Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization},
author={Mukherjee, Subhojyoti and Lai, Viet Dac and Addanki, Raghavendra and Rossi, Ryan A and Yoon, Seunghyun and Bui, Trung and Rao, Anup and Subramanian, Jayakumar and Kveton, Branislav},
booktitle = {Proc. NeurIPS},
year = {2025}
}

@article{shao2024deepseekmath,
title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Yang and others},
journal={arXiv:2402.03300},
year={2024}
}

@article{yu2025dapo,
title={DAPO: An Open-Source LLM Reinforcement Learning System at Scale},
author={Yu, Qiying and Zhang, Zheng and Zhu, Ruofei and Yuan, Yufeng and Zuo, Xiaochen and Yue, Yu and Dai, Weinan and Fan, Tiantian and Liu, Gaohong and Liu, Lingjun and others},
journal={arXiv:2503.14476},
year={2025}
}

@article{wang2025reinforcement,
title={Reinforcement learning for reasoning in large language models with one training example},
author={Wang, Yiping and Yang, Qing and Zeng, Zhiyuan and Ren, Liliang and Liu, Liyuan and Peng, Baolin and Cheng, Hao and He, Xuehai and Wang, Kuan and Gao, Jianfeng and others},
journal={arXiv:2504.20571},
year={2025}
}

@inproceedings{wei2021finetuned,
title={Finetuned language models are zero-shot learners},
author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
booktitle={Proc. ICLR},
year={2022}
}

@inproceedings{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  booktitle={Proc. NeurIPS},
  year={2022}
}

@inproceedings{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yuri and Edwards, Harrison and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  booktitle={Proc. ICLR},
  year={2023}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv:1707.06347},
  year={2017}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proc. SOSP},
  year={2023}
}

@misc{nvidia2023tensorrtllm,
  author = {NVIDIA},
  title = {TensorRT-LLM},
  year = {2023},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/NVIDIA/TensorRT-LLM}},
  note = {GitHub repository}
}

@article{zheng2024sglang,
  title={Sglang: Efficient execution of structured language model programs},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Sun, Chuyue Livia and Huang, Jeff and Yu, Cody Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  journal={Proc. NeurIPS},
  year={2024}
}

@article{jacobs2023deepspeed,
  title={Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models},
  author={Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Shuaiwen Leon and Rajbhandari, Samyam and He, Yuxiong},
  journal={arXiv:2309.14509},
  year={2023}
}

@inproceedings{zhao2023pytorch,
  title={Pytorch fsdp: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  booktitle={Proc. VLDB},
  year={2023}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv:1909.08053},
  year={2019}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv:2110.14168},
  year={2021}
}

@inproceedings{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle={Proc. NeurIPS},
  year={2021}
}

@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv:2505.09388},
  year={2025}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={Proc. ICLR},
  year={2022}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proc. AAAI},
  year={2020}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
}

@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={Proc. EMNLP},
  year={2017}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proc. SOSP},
  year={2023}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={Proc. ICML},
  year={2023}
}