@inproceedings{
morris2024inversion,
title={Language Model Inversion},
author={John Xavier Morris and Wenting Zhao and Justin T Chiu and Vitaly Shmatikov and Alexander M Rush},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}}

@inproceedings{
xiong2024confidence,
title={Can {LLM}s Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in {LLM}s},
author={Miao Xiong and Zhiyuan Hu and Xinyang Lu and YIFEI LI and Jie Fu and Junxian He and Bryan Hooi},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}}

@inproceedings{
zhou2024fourier,
title={Pre-trained Large Language Models Use Fourier Features to Compute Addition},
author={Tianyi Zhou and Deqing Fu and Vatsal Sharan and Robin Jia},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}

@inproceedings{bender2021parrots,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610â€“623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{
li2023othello,
title={Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task},
author={Kenneth Li and Aspen K Hopkins and David Bau and Fernanda Vi{\'e}gas and Hanspeter Pfister and Martin Wattenberg},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}}

@inproceedings{hewitt-manning-2019-structural,
    title = "{A} Structural Probe for Finding Syntax in Word Representations",
    author = "Hewitt, John  and
      Manning, Christopher D.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/N19-1419",
    pages = "4129--4138",
    abstract = "Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network{'}s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry."
}


@inproceedings{
wu2025axbench,
title={AxBench: Steering {LLM}s? Even Simple Baselines Outperform Sparse Autoencoders},
author={Zhengxuan Wu and Aryaman Arora and Atticus Geiger and Zheng Wang and Jing Huang and Dan Jurafsky and Christopher D Manning and Christopher Potts},
booktitle={Forty-second International Conference on Machine Learning},
year={2025}}

@inproceedings{
huben2024sparse,
title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
author={Robert Huben and Hoagy Cunningham and Logan Riggs Smith and Aidan Ewart and Lee Sharkey},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@inproceedings{
hewitt2025position,
title={Position: We Can{\textquoteright}t Understand {AI} Using our Existing Vocabulary},
author={John Hewitt and Robert Geirhos and Been Kim},
booktitle={Forty-second International Conference on Machine Learning Position Paper Track},
year={2025}
}

@inproceedings{gonen-goldberg-2019-lipstick-pig,
    title = "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
    author = "Gonen, Hila  and
      Goldberg, Yoav",
    editor = "Axelrod, Amittai  and
      Yang, Diyi  and
      Cunha, Rossana  and
      Shaikh, Samira  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the 2019 Workshop on Widening NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "60--63",
    abstract = "Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between ``gender-neutralized'' words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling."
}

@misc{elhage2021mathematical,
  title        = {A Mathematical Framework for Transformer Circuits},
  author       = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas
                  and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and
                  DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny
                  and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario
                  and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year         = {2021},
  month        = dec,
  howpublished = {Anthropic Research Blog},
}

@inproceedings{
geiger2021causal,
title={Causal Abstractions of Neural Networks},
author={Atticus Geiger and Hanson Lu and Thomas F Icard and Christopher Potts},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021}
}