@article{kirkpatrick_overcoming_2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1612.00796},
	doi = {10.1073/pnas.1611835114},
	abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
	number = {13},
	urldate = {2025-04-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	month = mar,
	year = {2017},
	note = {arXiv:1612.00796 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3521--3526},
	file = {Preprint PDF:/Users/tkol/Zotero/storage/4HPF9JAW/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural networks.pdf:application/pdf;Snapshot:/Users/tkol/Zotero/storage/B9Z5CT2A/1612.html:text/html},
}

@inproceedings{saha_gradient_2020,
	title = {Gradient {Projection} {Memory} for {Continual} {Learning}},
	url = {https://openreview.net/forum?id=3AOj0RCNC2},
	abstract = {The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches.},
	language = {en},
	urldate = {2025-12-06},
	author = {Saha, Gobinda and Garg, Isha and Roy, Kaushik},
	month = oct,
	year = {2020},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/96RL86XU/Saha et al. - 2020 - Gradient Projection Memory for Continual Learning.pdf:application/pdf},
}

@inproceedings{wang_training_2021,
	title = {Training {Networks} in {Null} {Space} of {Feature} {Covariance} for {Continual} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Training_Networks_in_Null_Space_of_Feature_Covariance_for_Continual_CVPR_2021_paper.html},
	language = {en},
	urldate = {2025-12-06},
	author = {Wang, Shipeng and Li, Xiaorong and Sun, Jian and Xu, Zongben},
	year = {2021},
	pages = {184--193},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/DFF5D7CW/Wang et al. - 2021 - Training Networks in Null Space of Feature Covariance for Continual Learning.pdf:application/pdf},
}

@inproceedings{zenke_continual_2017,
	title = {Continual {Learning} {Through} {Synaptic} {Intelligence}},
	url = {https://proceedings.mlr.press/v70/zenke17a.html},
	abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.},
	language = {en},
	urldate = {2025-12-06},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {3987--3995},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/CWPJ5M5F/Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf:application/pdf;Supplementary PDF:/Users/tkol/Zotero/storage/MQKV7567/Zenke et al. - 2017 - Continual Learning Through Synaptic Intelligence.pdf:application/pdf},
}

@inproceedings{aljundi_memory_2018,
	title = {Memory {Aware} {Synapses}: {Learning} what (not) to forget},
	shorttitle = {Memory {Aware} {Synapses}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Rahaf_Aljundi_Memory_Aware_Synapses_ECCV_2018_paper.html},
	urldate = {2025-12-06},
	author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
	year = {2018},
	pages = {139--154},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/227IGVL8/Aljundi et al. - 2018 - Memory Aware Synapses Learning what (not) to forget.pdf:application/pdf},
}

@inproceedings{farajtabar_orthogonal_2020,
	title = {Orthogonal {Gradient} {Descent} for {Continual} {Learning}},
	url = {https://proceedings.mlr.press/v108/farajtabar20a.html},
	abstract = {Neural networks are achieving state of the art and sometimes super-human performance on learning tasks across a variety of domains. Whenever these problems require learning in a continual or sequential manner, however, neural networks suffer from the problem of catastrophic forgetting; they forget how to solve previous tasks after being trained on a new task, despite having the essential capacity to solve both tasks if they were trained on both simultaneously. In this paper, we propose to address this issue from a parameter space perspective and study an approach to restrict the direction of the gradient updates to avoid forgetting previously-learned data. We present the Orthogonal Gradient Descent (OGD) method, which accomplishes this goal by projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task. Our approach utilizes the high capacity of a neural network more efficiently and does not require storing the previously learned data that might raise privacy concerns. Experiments on common benchmarks reveal the effectiveness of the proposed OGD method.},
	language = {en},
	urldate = {2025-12-06},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Farajtabar, Mehrdad and Azizan, Navid and Mott, Alex and Li, Ang},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3762--3773},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/W5NB5G2Z/Farajtabar et al. - 2020 - Orthogonal Gradient Descent for Continual Learning.pdf:application/pdf;Supplementary PDF:/Users/tkol/Zotero/storage/77NQD8T7/Farajtabar et al. - 2020 - Orthogonal Gradient Descent for Continual Learning.pdf:application/pdf},
}

@inproceedings{lopez_paz_gradient_2017,
	title = {Gradient {Episodic} {Memory} for {Continual} {Learning}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/f87522788a2be2d171666752f97ddebb-Abstract.html},
	abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
	urldate = {2025-12-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lopez-Paz, David and Ranzato, Marc' Aurelio},
	year = {2017},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/FWKACINM/Lopez-Paz and Ranzato - 2017 - Gradient Episodic Memory for Continual Learning.pdf:application/pdf},
}

@misc{chaudhry_efficient_2019,
	title = {Efficient {Lifelong} {Learning} with {A}-{GEM}},
	url = {http://arxiv.org/abs/1812.00420},
	doi = {10.48550/arXiv.1812.00420},
	abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz \& Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
	urldate = {2025-12-06},
	publisher = {arXiv},
	author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
	month = jan,
	year = {2019},
	note = {arXiv:1812.00420 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {Preprint PDF:/Users/tkol/Zotero/storage/QXS7IDWV/Chaudhry et al. - 2019 - Efficient Lifelong Learning with A-GEM.pdf:application/pdf;Snapshot:/Users/tkol/Zotero/storage/EL5K2JL3/1812.html:text/html},
}

@misc{riemer_learning_2019,
	title = {Learning to {Learn} without {Forgetting} by {Maximizing} {Transfer} and {Minimizing} {Interference}},
	url = {http://arxiv.org/abs/1810.11910},
	doi = {10.48550/arXiv.1810.11910},
	abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
	urldate = {2025-12-06},
	publisher = {arXiv},
	author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
	month = may,
	year = {2019},
	note = {arXiv:1810.11910 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2019},
	file = {Preprint PDF:/Users/tkol/Zotero/storage/QW7XHZLC/Riemer et al. - 2019 - Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference.pdf:application/pdf;Snapshot:/Users/tkol/Zotero/storage/XXFTFT9Z/1810.html:text/html},
}

@inproceedings{shin_continual_2017,
	title = {Continual {Learning} with {Deep} {Generative} {Replay}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html},
	abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (“generator”) and a task solving model (“solver”). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
	urldate = {2025-12-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
	year = {2017},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/KHHP8BWZ/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:application/pdf},
}

@misc{lu_rethinking_2025,
	title = {Rethinking the {Stability}-{Plasticity} {Trade}-off in {Continual} {Learning} from an {Architectural} {Perspective}},
	url = {http://arxiv.org/abs/2506.03951},
	doi = {10.48550/arXiv.2506.03951},
	abstract = {The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87\% more compact in terms of parameters. Code: https://github.com/byyx666/Dual-Arch.},
	urldate = {2025-12-06},
	publisher = {arXiv},
	author = {Lu, Aojun and Yuan, Hangjie and Feng, Tao and Sun, Yanan},
	month = jun,
	year = {2025},
	note = {arXiv:2506.03951 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted to ICML 2025},
	file = {Preprint PDF:/Users/tkol/Zotero/storage/923CDWVC/Lu et al. - 2025 - Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspectiv.pdf:application/pdf;Snapshot:/Users/tkol/Zotero/storage/W5AH4M8C/2506.html:text/html},
}

@inproceedings{kim_achieving_2023,
	title = {Achieving a {Better} {Stability}-{Plasticity} {Trade}-{Off} via {Auxiliary} {Networks} in {Continual} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Achieving_a_Better_Stability-Plasticity_Trade-Off_via_Auxiliary_Networks_in_Continual_CVPR_2023_paper.html},
	language = {en},
	urldate = {2025-12-06},
	author = {Kim, Sanghwan and Noci, Lorenzo and Orvieto, Antonio and Hofmann, Thomas},
	year = {2023},
	pages = {11930--11939},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/CFI2RXXB/Kim et al. - 2023 - Achieving a Better Stability-Plasticity Trade-Off via Auxiliary Networks in Continual Learning.pdf:application/pdf},
}

@article{chen_stabilityplasticity_2023,
	title = {On the {Stability}-{Plasticity} {Dilemma} in {Continual} {Meta}-{Learning}: {Theory} and {Algorithm}},
	volume = {36},
	shorttitle = {On the {Stability}-{Plasticity} {Dilemma} in {Continual} {Meta}-{Learning}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/57587d8d6a7ede0e5302fc22d0878c53-Abstract-Conference.html},
	language = {en},
	urldate = {2025-12-06},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Qi and Shui, Changjian and Han, Ligong and Marchand, Mario},
	month = dec,
	year = {2023},
	pages = {27414--27468},
	file = {Full Text PDF:/Users/tkol/Zotero/storage/HTGFXZA4/Chen et al. - 2023 - On the Stability-Plasticity Dilemma in Continual Meta-Learning Theory and Algorithm.pdf:application/pdf},
}

@misc{jung_new_2023,
	title = {New {Insights} for the {Stability}-{Plasticity} {Dilemma} in {Online} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2302.08741},
	doi = {10.48550/arXiv.2302.08741},
	abstract = {The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.},
	urldate = {2025-12-06},
	publisher = {arXiv},
	author = {Jung, Dahuin and Lee, Dongjin and Hong, Sunwon and Jang, Hyemi and Bae, Ho and Yoon, Sungroh},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08741 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted to ICLR2023},
	file = {Preprint PDF:/Users/tkol/Zotero/storage/MHRI4ZB3/Jung et al. - 2023 - New Insights for the Stability-Plasticity Dilemma in Online Continual Learning.pdf:application/pdf;Snapshot:/Users/tkol/Zotero/storage/UGKVBCFW/2302.html:text/html},
}

@article{hu2025open,
  title={Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model},
  author={Hu, Jingcheng and Zhang, Yinmin and Han, Qi and Jiang, Daxin and Zhang, Xiangyu and Shum, Heung-Yeung},
  journal={arXiv preprint arXiv:2503.24290},
  year={2025}
}

@article{feng2024sciknoweval,
  title={Sciknoweval: Evaluating multi-level scientific knowledge of large language models},
  author={Feng, Kehua and Ding, Keyan and Wang, Weijie and Zhuang, Xiang and Wang, Zeyuan and Qin, Ming and Zhao, Yu and Yao, Jianhua and Zhang, Qiang and Chen, Huajun},
  journal={arXiv preprint arXiv:2406.09098},
  year={2024}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{shenfeld2025rl,
  title={RL's Razor: Why Online Reinforcement Learning Forgets Less},
  author={Shenfeld, Idan and Pari, Jyothish and Agrawal, Pulkit},
  journal={arXiv preprint arXiv:2509.04259},
  year={2025}
}

@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
