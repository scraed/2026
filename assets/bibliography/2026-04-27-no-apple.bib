










@article{bronstein2021GDL,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021}
}


@inproceedings{masserano2025wavetoken,
    title={Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization},
    author={Luca Masserano and Abdul Fatir Ansari and Boran Han and Xiyuan Zhang and Christos Faloutsos and Michael W. Mahoney and Andrew Gordon Wilson and Youngsuk Park and Syama Sundar Rangapuram and Danielle C. Maddix and Bernie Wang},
    booktitle={Forty-second International Conference on Machine Learning},
    year={2025},
    url={https://openreview.net/forum?id=B6WalMoQJW}
}

@article{ansari2024chronos,
  title={Chronos: Learning the Language of Time Series},
  author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2024},
  url={https://openreview.net/forum?id=gerNCVqqtR}
}

@article{cohen2025toto,
  title={This Time is Different: An Observability Perspective on Time Series Foundation Models},
  author={Cohen, Ben and Khwaja, Emaad and Doubli, Youssef and Lemaachi, Salahidine and Lettieri, Chris and Masson, Charles and Miccinilli, Hugo and Ram{\'e}, Elise and Ren, Qiqi and Rostamizadeh, Afshin and others},
  journal={arXiv preprint arXiv:2505.14766},
  year={2025}
}


@article{qi2025timeHF,
  title={Timehf: Billion-scale time series models guided by human feedback},
  author={Qi, Yongzhi and Hu, Hao and Lei, Dazhou and Zhang, Jianshen and Shi, Zhengxin and Huang, Yulin and Chen, Zhengyu and Lin, Xiaoming and Shen, Zuo-Jun Max},
  journal={arXiv preprint arXiv:2501.15942},
  year={2025}
}

@inproceedings{das2024timesFM,
  title={A decoder-only foundation model for time-series forecasting},
  author={Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}


@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}

@InProceedings{godahewa2021monash,
  author = "Godahewa, Rakshitha and Bergmeir, Christoph and Webb, Geoffrey I. and Hyndman, Rob J. and Montero-Manso, Pablo",
  title = "Monash Time Series Forecasting Archive",
  booktitle = "Neural Information Processing Systems Track on Datasets and Benchmarks",
  year = "2021"
}

@inproceedings{xu2025specialized,
    title={Specialized Foundation Models Struggle to Beat Supervised Baselines},
    author={Zongzhe Xu and Ritvik Gupta and Wenduo Cheng and Alexander Shen and Junhong Shen and Ameet Talwalkar and Mikhail Khodak},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=JYTQ6ELUVO}
}

@article{awais2025foundation,
  title={Foundation models defining a new era in vision: a survey and outlook},
  author={Awais, Muhammad and Naseer, Muzammal and Khan, Salman and Anwer, Rao Muhammad and Cholakkal, Hisham and Shah, Mubarak and Yang, Ming-Hsuan and Khan, Fahad Shahbaz},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2025},
  publisher={IEEE}
}

@inproceedings{huh2024PRH,
  title={Position: The platonic representation hypothesis},
  author={Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@Inbook{Pfeifer2004embodyRL,
author="Pfeifer, Rolf
and Iida, Fumiya",
editor="Iida, Fumiya
and Pfeifer, Rolf
and Steels, Luc
and Kuniyoshi, Yasuo",
title="Embodied Artificial Intelligence: Trends and Challenges",
bookTitle="Embodied Artificial Intelligence: International Seminar, Dagstuhl Castle, Germany, July 7-11, 2003. Revised Papers",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--26",
abstract="The field of Artificial Intelligence, which started roughly half a century ago, has a turbulent history. In the 1980s there has been a major paradigm shift towards embodiment. While embodied artificial intelligence is still highly diverse, changing, and far from ``theoretically stable'', a certain consensus about the important issues and methods has been achieved or is rapidly emerging. In this non-technical paper we briefly characterize the field, summarize its achievements, and identify important issues for future research. One of the fundamental unresolved problems has been and still is how thinking emerges from an embodied system. Provocatively speaking, the central issue could be captured by the question ``How does walking relate to thinking?''",
isbn="978-3-540-27833-7",
doi="10.1007/978-3-540-27833-7_1",
url="https://doi.org/10.1007/978-3-540-27833-7_1"
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@article{brown2020gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{goswami2024moment,
    title={{MOMENT}: A Family of Open Time-series Foundation Models},
    author={Mononito Goswami and Konrad Szafer and Arjun Choudhry and Yifu Cai and Shuo Li and Artur Dubrawski},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
    url={https://openreview.net/forum?id=FVvf69a5rx}
}

@inproceedings{woo2024uni2ts,
author = {Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Xiong, Caiming and Savarese, Silvio and Sahoo, Doyen},
title = {Unified training of universal time series forecasting transformers},
year = {2024},
publisher = {JMLR.org},
abstract = {Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pretrained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked EncOder-based UnIveRsAl TIme Series Forecasting Transformer (MOIRAI). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, MOIRAI achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {2178},
numpages = {25},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{liu2024timer,
author = {Liu, Yong and Zhang, Haoran and Li, Chenyu and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
title = {Timer: generative pre-trained transformers are large time series models},
year = {2024},
publisher = {JMLR.org},
abstract = {Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world data-scarce scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progress has been achieved with the emergence of large language models, exhibiting unprecedented abilities such as few-shot generalization, scalability, and task generality, which are however absent in small deep models. To change the status quo of training scenario-specific small models from scratch, this paper aims at the early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet diverse application needs, we convert forecasting, imputation, and anomaly detection of time series into a unified generative task. The outcome of this study is a Time Series Transformer (Timer), which is generative pretrained by next token prediction and adapted to various downstream tasks with promising capabilities as an LTSM. Code and datasets are available at: https://github.com/thuml/Large-Time-Series-Model},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {1313},
numpages = {31},
location = {Vienna, Austria},
series = {ICML'24}
}




