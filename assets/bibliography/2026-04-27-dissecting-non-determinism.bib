@inproceedings{MoE,
title={Towards LLMs Robustness to Changes in Prompt Format Styles},
author={Lilian Ngweta and Kiran Kate and Jason Tsay and Yara Rizk},
booktitle={North American Chapter of the Association for Computational Linguistics},
year={2025},
}

@article{QuantifyingSensitivity ,
title={What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to Prompt Engineering},
author={Federico Errica and Giuseppe Siracusano and Davide Sanvito and Roberto Bifulco},
journal={ArXiv},
year={2024},
volume={abs/2406.12334},
}

@article{FormatSpread,
title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
author={Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
journal={ArXiv},
year={2023},
volume={abs/2310.11324},
}

@article{LOSTINTHEMIDDLE,
title={Lost in the Middle: How Language Models Use Long Contexts},
author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
journal={Transactions of the Association for Computational Linguistics},
year={2023},
volume={12},
pages={157-173},
}

@article{APE,
title={Large Language Models Are Human-Level Prompt Engineers},
author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},
journal={ArXiv},
year={2022},
volume={abs/2211.01910},

}

@article{HumanBrittle,
title={Are Humans as Brittle as Large Language Models?},
author={Jiahui Li and Sean Papay and Roman Klinger},
journal={ArXiv},
year={2025},
volume={abs/2509.07869},

}

@misc{negligible,
title={Hallucinations are inevitable but can be made statistically negligible. The "innate" inevitability of hallucinations cannot explain practical LLM issues},
author={Atsushi Suzuki and Yulan He and Feng Tian and Zhongyuan Wang},
year={2025},
eprint={2502.12187},
archivePrefix={arXiv},
primaryClass={[cs.CL](http://cs.cl/)},
url={https://arxiv.org/abs/2502.12187},
}


@inproceedings{racialbias,
    title = "The Risk of Racial Bias in Hate Speech Detection",
    author = "Sap, Maarten  and
      Card, Dallas  and
      Gabriel, Saadia  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1163/",
    doi = "10.18653/v1/P19-1163",
    pages = "1668--1678",
}


@inproceedings{JailbreakingHAY,
title={Jailbreaking in the Haystack},
author={Rishi R Shah and Chen Henry Wu and Shashwat Saxena and Ziqian Zhong and Alexander Robey and Aditi Raghunathan},
year={2025},
}

@article{Banerjee2024LLMsWA,
title={LLMs Will Always Hallucinate, and We Need to Live With This},
author={Sourav Banerjee and Ayushi Agarwal and Saloni Singla},
journal={ArXiv},
year={2024},
volume={abs/2409.05746},
url={https://api.semanticscholar.org/CorpusID:272524830}
}

@inproceedings{selectionbias,
title={Large Language Models Are Not Robust Multiple Choice Selectors},
author={Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=shr9PXz7T0}
}


@inproceedings{orderbias,
    title = "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
    author = "Pezeshkpour, Pouya  and
      Hruschka, Estevam",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.130/",
    doi = "10.18653/v1/2024.findings-naacl.130",
    pages = "2006--2017",
}

@inproceedings{react,
title={ReAct: Synergizing Reasoning and Acting in Language Models},
author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
booktitle={International Conference on Learning Representations (ICLR)},
year={2023},
url={https://arxiv.org/abs/2210.03629}
}

@article{agentrise,
title={The Rise and Potential of Large Language Model Based Agents: A Survey},
author={Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and others},
journal={arXiv preprint arXiv:2309.07864},
year={2023},
url={https://arxiv.org/abs/2309.07864}
}

@article{NonDeterminism,
author = {Ouyang, Shuyin and Zhang, Jie M. and Harman, Mark and Wang, Meng},
title = {An Empirical Study of the Non-Determinism of ChatGPT in Code Generation},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3697010},
doi = {10.1145/3697010},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {42},
numpages = {28},
keywords = {code generation, non-determinism, large language model}
}

@article{Degeneration,
title={Closing the Curious Case of Neural Text Degeneration},
author={Matthew Finlayson and John Hewitt and Alexander Koller and Swabha Swayamdipta and Ashish Sabharwal},
journal={ArXiv},
year={2023},
volume={abs/2310.01693},
url={https://api.semanticscholar.org/CorpusID:263608672}
} 


@misc{masfail,
title={Why Do Multi-Agent LLM Systems Fail?},
author={Mert Cemri and Melissa Z. Pan and Shuyi Yang and Lakshya A. Agrawal and Bhavya Chopra and Rishabh Tiwari and Kurt Keutzer and Aditya Parameswaran and Dan Klein and Kannan Ramchandran and Matei Zaharia and Joseph E. Gonzalez and Ion Stoica},
year={2025},
eprint={2503.13657},
archivePrefix={arXiv},
primaryClass={[cs.AI](http://cs.ai/)},
url={https://arxiv.org/abs/2503.13657},
}

@article{he2025nondeterminism,
author = {Horace He and Thinking Machines Lab},
title = {Defeating Nondeterminism in LLM Inference},
journal = {Thinking Machines Lab: Connectionism},
year = {2025},
note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
doi = {10.64434/tml.20250910}
}

@inproceedings{Holtzman,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}

@misc{hinton2015distillingknowledgeneuralnetwork,
title={Distilling the Knowledge in a Neural Network},
author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
year={2015},
eprint={1503.02531},
archivePrefix={arXiv},
primaryClass={[stat.ML](http://stat.ml/)},
url={https://arxiv.org/abs/1503.02531},
}

@misc{wang2023costeffectivehyperparameteroptimizationlarge,
title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},
author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},
year={2023},
eprint={2303.04673},
archivePrefix={arXiv},
primaryClass={[cs.CL](http://cs.cl/)},
url={https://arxiv.org/abs/2303.04673},
}

@misc{riach2019determinism,
author       = {Riach, Duncan},
title        = {Determinism in Deep Learning},
howpublished = {Presentation at GPU Technology Conference (GTC 2019), Session S9911},
year         = {2019},
month        = {March},
address      = {San Jose, CA},
url          = {https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9911-determinism-in-deep-learning.pdf}
}


@inproceedings{Temperature,
    title = "The Effect of Sampling Temperature on Problem Solving in Large Language Models",
    author = "Renze, Matthew",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.432/",
    doi = "10.18653/v1/2024.findings-emnlp.432",
    pages = "7346--7356",
}

@inproceedings{guSurveyLLMasaJudge2025,
    title = "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
    author = "Pezeshkpour, Pouya  and
      Hruschka, Estevam",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.130/",
    doi = "10.18653/v1/2024.findings-naacl.130",
    pages = "2006--2017",
}

@article{zhengCHEATINGAUTOMATICLLM2025,
  title = {CHEATING AUTOMATIC LLM BENCHMARKS: NULL MODELS ACHIEVE HIGH WIN RATES},
  author = {Zheng, Xiaosen and Pang, Tianyu and Du, Chao and Liu, Qian and Jiang, Jing and Lin, Min},
  year = 2025,
  abstract = {Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MTBench, have become popular for evaluating language models due to their costeffectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a ``null model'' that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5\% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because we assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.},
  langid = {english},
}

@misc{duboisLengthControlledAlpacaEvalSimple2025,
  title = {Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators},
  shorttitle = {Length-Controlled AlpacaEval},
  author = {Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B.},
  year = 2025,
  month = mar,
  number = {arXiv:2404.04475},
  eprint = {2404.04475},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.04475},
  urldate = {2025-12-03},
  abstract = {LLM-based auto-annotators have become a key component of the LLM development process due to their cost-effectiveness and scalability compared to human-based evaluation. However, these auto-annotators can introduce biases that are hard to remove. Even simple, known confounders such as preference for longer outputs remain in existing automated evaluation metrics. We propose a simple regression analysis approach for controlling biases in auto-evaluations. As a real case study, we focus on reducing the length bias of AlpacaEval, a fast and affordable benchmark for instruction-tuned LLMs that uses LLMs to estimate response quality. Despite being highly correlated with human preferences, AlpacaEval is known to favor models that generate longer outputs. We introduce a length-controlled AlpacaEval that aims to answer the counterfactual question: "What would the preference be if the model's and baseline's output had the same length?" To achieve this, we first fit a generalized linear model to predict the biased auto-annotator's preferences based on the mediators we want to control for (length difference) and other relevant features. We then obtain length-controlled preferences by predicting preferences while conditioning the GLM with a zero difference in lengths. Length-controlling not only improves the robustness of the metric to manipulations in model verbosity, but we also find that it increases the Spearman correlation with LMSYS Chatbot Arena from 0.94 to 0.98.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
}

@misc{wataokaSelfPreferenceBiasLLMasaJudge2025,
  title = {Self-Preference Bias in LLM-as-a-Judge},
  author = {Wataoka, Koki and Takahashi, Tsubasa and Ri, Ryokan},
  year = 2025,
  month = jun,
  number = {arXiv:2410.21819},
  eprint = {2410.21819},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21819},
  urldate = {2025-11-27},
  abstract = {Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias. To explore the causes, we hypothesize that LLMs may favor outputs that are more familiar to them, as indicated by lower perplexity. We analyze the relationship between LLM evaluations and the perplexities of outputs. Our findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
}

@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@misc{liu2023gevalnlgevaluationusing,
      title={G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment}, 
      author={Yang Liu and Dan Iter and Yichong Xu and Shuohang Wang and Ruochen Xu and Chenguang Zhu},
      year={2023},
      eprint={2303.16634},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.16634}, 
}

@misc{es2025ragasautomatedevaluationretrieval,
      title={Ragas: Automated Evaluation of Retrieval Augmented Generation}, 
      author={Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert},
      year={2025},
      eprint={2309.15217},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.15217}, 
}

@misc{kim2024prometheusinducingfinegrainedevaluation,
      title={Prometheus: Inducing Fine-grained Evaluation Capability in Language Models}, 
      author={Seungone Kim and Jamin Shin and Yejin Cho and Joel Jang and Shayne Longpre and Hwaran Lee and Sangdoo Yun and Seongjin Shin and Sungdong Kim and James Thorne and Minjoon Seo},
      year={2024},
      eprint={2310.08491},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.08491}, 
}

@misc{zhu2025judgelmfinetunedlargelanguage,
      title={JudgeLM: Fine-tuned Large Language Models are Scalable Judges}, 
      author={Lianghui Zhu and Xinggang Wang and Xinlong Wang},
      year={2025},
      eprint={2310.17631},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.17631}, 
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  month = {5},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@misc{arenahard2024,
    title = {From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline},
    url = {https://lmsys.org/blog/2024-04-19-arena-hard/},
    author = {Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica},
    month = {April},
    year = {2024}
}

@inproceedings{NEURIPS2023_91f18a12,
  title = {Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = 2023,
  volume = {36},
  pages = {46595--46623},
  publisher = {Curran Associates, Inc.}
}