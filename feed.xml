<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iclr-blogposts.github.io/2026/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2026/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-08T09:41:18+00:00</updated><id>https://iclr-blogposts.github.io/2026/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Fairness Audits as Theater: When Metrics Mask Structural Harm</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/fairness-audits/" rel="alternate" type="text/html" title="Fairness Audits as Theater: When Metrics Mask Structural Harm"/><published>2026-12-07T00:00:00+00:00</published><updated>2026-12-07T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/fairness-audits</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/fairness-audits/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Machine learning systems are everywhere—and they are broken. Across domains from criminal justice to employment screening to healthcare, algorithms have replicated, amplified, and legitimized historical inequalities. In response, the AI ethics community has converged on a solution: <strong>fairness audits</strong>.</p> <p>Audits have become the lingua franca of algorithmic accountability. Companies commission them. Regulators expect them. Researchers publish them. The idea is straightforward: measure bias, document findings, demonstrate due diligence, proceed with deployment. Yet despite this explosion in auditing infrastructure, algorithmic harm persists.</p> <p>This blog post argues that contemporary fairness audits function as <strong>legitimation theater</strong> rather than mechanisms of genuine accountability. We identify three structural problems that prevent audits from preventing harm, then propose alternatives grounded in substantive accountability and participatory oversight.</p> <p>The stakes are high. As auditing becomes institutionalized, it risks creating a false sense of resolution while marginalizing communities most affected by algorithmic systems. Understanding why audits fail is the first step toward building accountability frameworks that actually work.</p> <h2 id="the-rise-and-crisis-of-fairness-audits">The Rise and Crisis of Fairness Audits</h2> <h3 id="what-audits-promise">What Audits Promise</h3> <p>The fairness audit emerged as a response to the reproducibility and integrity crisis in machine learning. After landmark publications—Buolamwini and Gebru’s <em>Gender Shades</em> (2018), ProPublica’s COMPAS investigation (2016), Obermeyer et al. on medical AI (2019)—the research community recognized that algorithmic bias was not incidental but systematic.</p> <p>Audits promised a solution. By treating algorithms like financial systems or medical devices, audits would:</p> <ol> <li><strong>Measure bias systematically</strong> using rigorous fairness metrics</li> <li><strong>Identify disparities</strong> before deployment</li> <li><strong>Document processes</strong> for regulatory compliance</li> <li><strong>Enable accountability</strong> by creating a paper trail</li> </ol> <p>The appeal is intuitive. Audits are <em>objective</em>, <em>scalable</em>, and <em>defensible</em>. They transform the abstract problem of “fairness” into concrete numbers: disparate impact ratios, false positive rate gaps, calibration indices. A system that passes an audit appears legitimate. Checkboxes are checked. Lawyers are satisfied.</p> <h3 id="what-audits-deliver">What Audits Deliver</h3> <p>In practice, audits deliver legitimacy without justice.</p> <p>Consider the structure of a typical audit:</p> <blockquote> <ol> <li>A third-party firm is hired</li> <li>The firm tests the algorithm against a selection of fairness metrics</li> <li>The firm produces a report documenting disparities</li> <li>The report is filed away or released to satisfy legal requirements</li> <li>The system is deployed or continues operating</li> </ol> </blockquote> <p>This workflow is reassuring precisely because it appears neutral. Yet each step embeds values and choices that shape whether audits actually prevent harm.</p> <p><strong>The metric selection problem</strong>: Which fairness metrics to test? Dozens exist, many mathematically incompatible. The choice is not technical—it is political. Testing only group fairness metrics (equal outcome across groups) may miss individual fairness concerns (similar people treated similarly). Testing only predictive parity (equal accuracy across groups) may legitimize systems that amplify existing disparities in outcomes. The audit firm—accountable to the client who hired them—selects metrics convenient to pass.</p> <p><strong>The context collapse problem</strong>: Audits extract algorithms from the socio-technical contexts that give them meaning. An algorithm may achieve group fairness in aggregate but fail catastrophically for specific subpopulations (intersectional groups). An algorithm may pass all fairness tests in one jurisdiction but encode biases specific to another’s data distributions. Audits typically treat these as edge cases, not failures.</p> <p><strong>The legitimation problem</strong>: Once an audit is complete, the system becomes “certified” as fair. This certification creates a false sense of resolution. Decision-makers can point to the audit and claim due diligence was conducted. Communities affected by the system have limited recourse—the audit was done, what more can be done? The legitimacy of the audit becomes a defense against further scrutiny.</p> <h2 id="why-current-audits-fail">Why Current Audits Fail</h2> <h3 id="problem-1---the-metric-selection-problem">Problem 1 - The Metric Selection Problem</h3> <p>Fairness metrics are not discovered; they are constructed. Each metric encodes assumptions about what fairness means and how it should be measured. The key insight is that <strong>no single metric captures fairness completely</strong>.</p> <p>Consider a hiring algorithm. We might measure:</p> <ul> <li><strong>Demographic parity</strong>: Equal hiring rates across groups (if 60% of applicants are male, hire 60% male employees)</li> <li><strong>Equalized odds</strong>: Equal true positive and false positive rates across groups (equally likely to hire qualified candidates; equally unlikely to hire unqualified ones)</li> <li><strong>Calibration</strong>: If the algorithm predicts someone has a 70% chance of success, they succeed 70% of the time (across groups)</li> <li><strong>Individual fairness</strong>: Similar applicants get similar predictions</li> </ul> <p>These metrics are mathematically incompatible. An algorithm cannot simultaneously satisfy all of them. So auditors must choose.</p> <p>Who chooses? Typically, the firm conducting the audit—whose client is the company deploying the algorithm. Under this arrangement, auditors face subtle (or not-so-subtle) pressure to select metrics that reveal acceptable levels of bias. If the client is deploying an algorithm that is computationally convenient but happens to disparately impact women, auditors might prioritize demographic parity (which the algorithm likely fails) but de-emphasize calibration (which it might pass). The choice appears technical; in practice, it is strategic.</p> <p>Real-world example: Amazon’s hiring algorithm initially scored candidates on a 0-5 scale and automatically filtered out anyone below 4. After audit, the company discovered the algorithm systematically downscored women. Amazon then… stopped using the score. They didn’t change the algorithm; they simply removed the automated filtering. The audit identified the bias but didn’t prevent deployment of the underlying biased system—it just made the bias less visible.</p> <h3 id="problem-2---the-context-collapse-problem">Problem 2 - The Context Collapse Problem</h3> <p>Fairness metrics assume a kind of context-free universalism. A metric is either satisfied or not. But algorithms operate in specific socio-technical contexts that metrics cannot capture.</p> <p>Consider COMPAS, the recidivism assessment tool. The algorithm predicts who will reoffend based on historical criminal justice data. Audits of COMPAS have revealed:</p> <ul> <li><strong>Structural bias</strong>: The algorithm inherits biases from training data shaped by racist policing practices</li> <li><strong>Feedback loops</strong>: Police deploy the algorithm to target neighborhoods, resulting in more arrests in those areas, which then become training data showing higher recidivism, which triggers more enforcement</li> <li><strong>Differential consequences</strong>: A high score for a Black defendant means harsher sentencing; a high score for a white defendant means increased supervision (both harmful, but differently)</li> </ul> <p>No fairness metric—not demographic parity, not equalized odds, not calibration—captures these contextual harms. An algorithm can pass every fairness test while perpetuating systemic racism. This is because metrics measure <strong>properties of the algorithm</strong>. They do not measure <strong>properties of the world</strong>: historical inequalities, ongoing discrimination, power asymmetries between the algorithm and those it affects.</p> <p>The context collapse problem is particularly acute for algorithmic systems that operate at the intersection of multiple domains. Consider hiring algorithms that use educational credentials as predictors. The algorithm may be internally fair (women and men with the same education are equally likely to be hired). But if the algorithm inherits biases from the educational system—where admissions, funding, and opportunities are unequally distributed—then the algorithm propagates those biases forward. An audit that ignores educational context will miss this structural propagation.</p> <h3 id="problem-3---the-legitimation-problem">Problem 3 - The Legitimation Problem</h3> <p>Perhaps the most insidious failure of audits is that they create legitimacy for systems that remain unjust.</p> <p>When a company hires a reputable firm to audit an algorithm, the company gains several things:</p> <ol> <li><strong>Legal protection</strong>: If sued, the company can point to the audit as evidence of due diligence</li> <li><strong>Rhetorical power</strong>: The company can claim the system is “fair” (or “as fair as possible”)</li> <li><strong>Internal reassurance</strong>: Employees can believe they are working for an ethical company</li> <li><strong>Reduced scrutiny</strong>: Once audited, the system faces less external pressure to change</li> </ol> <p>For affected communities, audits often feel like performance. Researchers and advocates may have repeatedly warned about a system’s harms. An audit is commissioned. A report is produced. The company claims to take findings seriously. The system remains deployed. What changed? Often, very little—except that the company now has a document testifying to awareness of the problem.</p> <p>This is the legitimation trap: audits create accountability theater. They make it appear that someone is responsible for ensuring fairness. But responsibility without power is theater. Audits typically have no enforcement mechanisms. If an audit finds bias, there is no requirement to fix it. The company can acknowledge the finding, claim context makes it unavoidable, and continue deploying.</p> <p>Worse, audits can <em>prevent</em> further accountability by creating a sense that the system has been properly scrutinized. Regulators who see an audit report may conclude that no further investigation is needed. Communities may be too exhausted by the audit process to push for additional changes. The audit closes the door on demanding more.</p> <h2 id="case-studies-in-audit-failure">Case Studies in Audit Failure</h2> <h3 id="amazons-hiring-algorithm">Amazon’s Hiring Algorithm</h3> <p>Amazon’s recruiting tool was trained on 10 years of historical hiring data. The algorithm learned to predict who would be hired and who would succeed in the role. Like most machine learning systems trained on historical data, it learned to replicate existing hiring patterns—which historically favored men, particularly in technical roles.</p> <p>An audit, conducted internally, found the algorithm systematically downscored women applicants. Amazon’s response: stop using the score for final hiring decisions, but keep using it for other purposes. The algorithm remained deployed; only its application changed.</p> <p>What did the audit accomplish? It created awareness of the bias (which was known to researchers and advocates beforehand). It did not prevent the bias from influencing hiring (the algorithm still scores women lower; the company just doesn’t automatically reject based on the score). It did allow Amazon to claim it took fairness seriously.</p> <p>The deeper issue: Amazon’s hiring data reflected decades of underrepresentation of women in tech. No audit of the algorithm could fix this upstream bias. An adequate response would have required addressing why the training data was skewed—a question audits typically don’t ask. An audit that examined only the algorithm and not the historical data that shaped it was always going to miss the root cause.</p> <h3 id="compas-recidivism-assessment">COMPAS Recidivism Assessment</h3> <p>ProPublica’s investigation of COMPAS found that the algorithm had different false positive rates for Black and white defendants: Black defendants were 45% more likely to be falsely labeled as high-risk. Despite ProPublica’s analysis, Northpointe (COMPAS’s developer) claimed the algorithm was fair because it had similar predictive accuracy across racial groups (calibration).</p> <p>Subsequent audits found conflicting results depending on which fairness metrics were used. By some metrics, COMPAS was fair. By others, it was not. Each audit could cherry-pick metrics to support a predetermined conclusion.</p> <p>Meanwhile, COMPAS remained in use across U.S. courts, influencing bail decisions, sentencing, and parole determinations. The debate over which fairness metric to use—carried out in academic papers and audit reports—had no impact on deployment. The algorithm continued to shape human lives while researchers argued about how to measure its bias.</p> <p>The audit failure here was not methodological but structural. The choice of which metric to believe was decided not by auditors but by judges and policy-makers, based on which metric was most convenient. An audit that could not compel acceptance of its findings was an audit that could not prevent harm.</p> <h3 id="healthcare-risk-scores">Healthcare Risk Scores</h3> <p>Hospitals and insurance companies use algorithms to predict patient outcomes and allocate resources. One widely-used system predicted mortality risk to identify patients needing palliative care. When audited, the algorithm was found to systematically underestimate risk for Black patients.</p> <p>Why? The algorithm was trained on healthcare cost as a proxy for illness severity. Since Black patients receive systematically lower healthcare spending due to structural racism in medicine, the algorithm learned to associate Blackness with lower disease severity. When exposed to Black patients with the same objective health status as white patients, the algorithm predicted better outcomes.</p> <p>This bias had profound consequences: Black patients were less likely to be identified for palliative care and early intervention. The algorithm replicated and amplified existing racial disparities in healthcare.</p> <p>After the audit, the algorithm was updated to remove race from the training features. But this didn’t solve the problem—the bias was not in the race variable itself, but in the proxy (cost) used for the outcome. Removing race was like treating a symptom while ignoring the disease. An adequate response would have required reconstructing the outcome variable and retraining on better data, which would have required collaboration with clinicians, patients, and healthcare systems. It would have required examining why Black patients receive less healthcare spending in the first place. Instead, the quick fix was applied, the audit was completed, and the algorithm continued to encode historical bias.</p> <h2 id="beyond-audits---toward-substantive-accountability">Beyond Audits - Toward Substantive Accountability</h2> <p>If audits fail, what works? The answer is not “better metrics” or “more rigorous audits.” The answer is <strong>participation</strong>.</p> <h3 id="participatory-model-cards">Participatory Model Cards</h3> <p>Traditional audits are conducted by experts (auditors) examining systems designed by experts (engineers) based on data selected by experts (product teams). Communities affected by the systems have no role.</p> <p>An alternative is participatory model cards: documents that describe the algorithm’s purpose, training data, performance, limitations, and known biases—co-produced by engineers, domain experts, affected communities, and ethicists.</p> <p>A participatory model card for a hiring algorithm might include:</p> <ul> <li><strong>Purpose</strong>: Identify qualified candidates (written by product team)</li> <li><strong>Data sources</strong>: Historical hiring records, with analysis of how the training data encodes historical biases (written collaboratively)</li> <li><strong>Intended use</strong>: Initial screening to identify candidates for human review (written by HR)</li> <li><strong>Known failures</strong>: The algorithm systematically downscores women with engineering backgrounds due to underrepresentation in the training data (identified by auditors and affected communities working together)</li> <li><strong>Conditions for deployment</strong>: Only use the algorithm if the hiring team receives training on its limitations; only use as initial screening, never for final decisions; regularly audit for bias; create appeals process for candidates who wish to challenge the score (negotiated with affected communities)</li> </ul> <p>The key difference: a participatory process creates accountability to affected communities, not just to the company deploying the system.</p> <h3 id="threshold-advocacy-and-context-binding">Threshold Advocacy and Context Binding</h3> <p>Some fairness metrics may be useful not as universal measures of fairness, but as <strong>threshold advocates</strong>: metrics that flag when a system is clearly unacceptable.</p> <p>For instance, a metric might state: “If the false positive rate for one group is more than 20% higher than for other groups, the system should not be deployed without explicit written justification.” This is not claiming to measure fairness perfectly; it is drawing a line against egregious harm.</p> <p>Threshold advocacy pairs metrics with <strong>context binding</strong>: making explicit the contexts in which the algorithm can be deployed, and the conditions under which it must be re-audited.</p> <p>An algorithm might be approved for use in employment screening in one domain but not another, because the contexts are different. An algorithm might be approved conditionally, with requirements to audit quarterly, to maintain an appeals process, to involve affected communities in oversight.</p> <p>This approach treats audits not as one-time certifications but as the beginning of ongoing accountability.</p> <h3 id="rights-based-accountability-frameworks">Rights-Based Accountability Frameworks</h3> <p>Rather than asking “Is this algorithm fair?” audits could ask “Are the rights of people affected by this algorithm protected?”</p> <p>A rights-based framework might include:</p> <ol> <li><strong>Right to know</strong>: Affected people should know if and how an algorithm is used to make decisions about them</li> <li><strong>Right to understand</strong>: The algorithm’s logic should be explainable in terms meaningful to those affected</li> <li><strong>Right to challenge</strong>: Affected people should be able to contest algorithmic decisions and have challenges reviewed by a human with authority to override</li> <li><strong>Right to remedy</strong>: If an algorithm causes harm, affected people should have access to compensation or system change</li> <li><strong>Right to participate</strong>: Communities should have voice in decisions about whether and how algorithms are deployed</li> </ol> <p>These rights cannot be verified by auditing the algorithm alone. They require examining the socio-technical system: Does the company actually tell people when algorithms are used? Can they understand the explanations provided? Can they challenge decisions? Can they receive remedy? Are they actually participating in oversight?</p> <p>Rights-based audits would be messier than metric-based audits. They would require qualitative research, community engagement, and ongoing monitoring. But they would be more likely to prevent harm because they focus on what actually matters to affected communities.</p> <h2 id="implications-for-practice">Implications for Practice</h2> <p>For practitioners, the message is clear: <strong>audits alone are insufficient</strong>. But audits are not useless—they are just incomplete.</p> <p><strong>For companies deploying algorithms</strong>:</p> <ul> <li>Don’t use an audit as a substitute for accountability</li> <li>Commission audits, but involve affected communities in the process</li> <li>Use audit findings not as a sign the system is ready to deploy, but as the starting point for governance design</li> <li>Create accountability structures: oversight boards, appeals processes, regular re-auditing</li> <li>Be transparent about audit findings and limitations</li> </ul> <p><strong>For auditors and researchers</strong>:</p> <ul> <li>Be explicit about the values embedded in your choice of fairness metrics</li> <li>Consider multiple metrics and document trade-offs, not just those the algorithm passes</li> <li>Examine the socio-technical context, not just the algorithm</li> <li>Engage affected communities in auditing, not just as data subjects</li> <li>Decline to conduct audits when you lack power to ensure findings lead to change</li> </ul> <p><strong>For regulators and policymakers</strong>:</p> <ul> <li>Don’t accept audits as evidence of compliance</li> <li>Require procedural safeguards (appeals processes, community oversight, transparency) not just technical safeguards (metrics)</li> <li>Build in incentives for companies to genuinely address audit findings, not just acknowledge them</li> <li>Support community organizations in conducting independent audits</li> <li>Create legal liability for algorithmic harms, regardless of audit status</li> </ul> <p><strong>For affected communities</strong>:</p> <ul> <li>Audits are not accountability—they are theater</li> <li>Demand participation in audit processes, don’t accept being studied</li> <li>Push for rights-based frameworks, not metric-based ones</li> <li>Document harms and build alternative knowledge systems</li> <li>Connect with other communities affected by algorithms; collective pressure is more effective than individual appeals</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Fairness audits have become the default mechanism for algorithmic accountability. Yet they fail to prevent harm because they treat fairness as a technical problem solvable through measurement, when it is fundamentally a political problem requiring power-sharing.</p> <p>An algorithm that measures its own fairness and declares itself fair is not making a meaningful claim. A company that audits its own algorithms and publishes findings it selects is performing transparency without practicing it. A regulator that accepts audits as evidence of compliance is delegating its authority.</p> <p>The alternative is uncomfortable. It requires admitting that algorithms cannot be made “fair” if the training data encodes historical injustice. It requires involving communities in technical decisions, which is slower and messier than expertise-driven processes. It requires giving affected people power to say “no” to systems, not just to participate in their design.</p> <p>But this is what genuine accountability looks like. Not audits that certify systems as fair. But systems designed with affected communities, governed by affected communities, and changed when affected communities say they’re causing harm.</p> <p>The work of building trustworthy AI is not the work of auditors measuring systems. It is the work of power-sharing and accountability that algorithms can only support, not substitute for.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[This blog post examines why contemporary fairness audits fail to prevent algorithmic harm, despite growing adoption. We analyze structural limitations and propose substantive alternatives grounded in participatory accountability.]]></summary></entry><entry><title type="html">Beyond Attention as a Graph</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/beyond-attention-as-graph/" rel="alternate" type="text/html" title="Beyond Attention as a Graph"/><published>2026-04-29T00:00:00+00:00</published><updated>2026-04-29T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/beyond-attention-as-graph</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/beyond-attention-as-graph/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/hero-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/hero-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/hero-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/hero.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Most attention variants have been designed to retain as much sample efficiency as possible, under the constraint of achieving subquadratic scaling with respect to sequence length.</p> <p>While this has clearly been a powerful research direction, recent changes in the pretraining paradigm have directed <em>attention</em> to architectures capable of increasing sample-efficiency.</p> <p>In my previous blogpost I had briefly introduced, as a tool to explain attention sinks, a simple way of viewing attention as a graph operation.</p> <p>We will use this same viewpoint to argue that regular transformers may be <strong>fundamentally limited</strong> in their message-passing capabilities, arguing in favor of higher-order attention methods, such as 2-simplicial Attention <d-cite key="roy2025fastsimplex"></d-cite> and provide a natural way of generalizing it to $n$-simplices, while explaining them from a <strong>topological perspective</strong>.</p> <p>Finally, we will also poke at the very mechanism that makes Machine Learning “deep”: <strong>layer composition</strong>.</p> <h2 id="motivation">Motivation</h2> <p>“Deep Learning” is named after the typical definition of Neural Network models as a set of subsequent, composed <em>Layers</em>.</p> <p>Layers represent atomic, parametric transformations between vector spaces, rendered non-linear by a selection of activation functions.</p> <p>In transformers, layers are organized in transformer blocks, and the two are often used interchangeably. Transformer blocks are nothing more than subsequent attention and MLP transformations operating on the residual stream.</p> <p>Intuitively, depth is easy to justify: while the Universal Approximation Theorem guarantees that a single, infinitely wide non-linear layer can approximate any continuous function arbitrarily well, it doesn’t mean that width scaling is practical.</p> <p>As it turns out, properly approximating functions becomes exponentially hard with respect to the dimension of the spaces the functions map between, which can be seen as another angle of the curse of dimensionality <d-cite key="poggio2017deepnotshallow_journal"></d-cite>.</p> <p>For this reason, it becomes convenient to instead “break down” the approximation problem by composing several parametric layers, one after the other.</p> <p>This allows the model to increase in expressivity without exploding in (latent) dimensionality.</p> <p>As for all worthwhile architectural choices in deep learning, this exposes us to a tradeoff: composing operations sequentially is <em>by definition</em> the least <strong>parallel (and hence fast) architectural choice we can make</strong>.</p> <h3 id="depth-for-transformers">Depth for Transformers</h3> <p>While the previous considerations apply in general for all Neural Network architectures, transformers in particular have their specific drawbacks when scaling depth: Transformers’ success has been greatly propelled by their natural parallelism during Next Token Prediction tasks, and, apart from inevitably increasing latency in both inference and training, depth exposes the network to further instability in gradients, as, depending on normalization, the model risks vanishing or exploding gradients.</p> <p>In sequence modelling, though, one key element justifies depth: attention is an operation that message-passes between pairs of tokens in a graph. This means that individual transformer blocks can only possibly encode interactions between pairs of tokens. <strong>Depth allows information to be passed beyond a single-hop</strong>: if we reframe the $AV$ multiplication as in the attention sinks blogpost (seeing as “diffusion” of $V$ on the graph), we can reconnect this intuition to regular graph theory by noticing how powers of the adjacency matrix of a graph, $A^k$, represent $k$-hop walks from each node, and therefore depth approximates this due to attention’s fully connected, yet sparse, input-dependent adjacency matrix.</p> <p>As a result, depth is a fundamental ingredient in transformers that allows them to effectively message-pass between <em>tuples</em> of tokens, and hence build complex and useful representations of tokens in sequences.</p> <p>But what if there existed a way to message-pass between tuples of tokens without resorting to depth?</p> <h2 id="what-lies-beyond-graphs">What Lies Beyond Graphs</h2> <p>As we know, the message-passing operation happening during attention can be conceptualized as a graph operation. This simple observation, while trivial, has a relevant practical consequence: an entire field of science has, since roughly 2017, been extensively studying Neural Networks as message-passing on graphs, and has developed a variety of theories and techniques to best represent information on topological objects. Of course, the field in question is Geometric Deep Learning, and its central contributions, Graph Neural Networks and Topological Deep Learning.</p> <p>Notably, one key element of that vast literature has been an expressivity bound on GNN architectures: if we define “expressivity” as the capability of distinguishing graphs that are different, then a GNN is only as expressive as the Weisfeiler-Lehman test <d-cite key="huang2022wl"></d-cite> (also referred to as the WL-test). I won’t go in the details of what the test is, and will gladly refer the interested reader to Federico Barbero’s excellent video explaining it.</p> <p>If you don’t have the time, here’s the gist of it: the WL-test is designed to understand when two graphs are isomorphic (the same graph), but it doesn’t always work. It can be shown that a GNN is at most as expressive at graph isomorphism as the WL-test itself <d-cite key="xu2018powerful"></d-cite>.</p> <p>If you’re anything like me, this sounds like bad news: what do you mean we have a theoretically bounded expressivity? Isn’t universal approximation the reason we like Neural Networks so much?</p> <p>Fortunately, not everything is lost. As it turns out, it’s possible to “break” the WL-test bound by inserting higher-order topological information.</p> <p>But what does it mean?</p> <p>As you know, a graph is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = {1, 2, \cdots, n}$ is a set of <em>nodes</em>, and $\mathcal{E}: \mathcal{V} \times \mathcal{V} \rightarrow {0,1}$ is a set of <em>links</em>, also called <em>edges</em> if $(i,j) \in \mathcal{E}$ also implies $(j,i) \in \mathcal{E}$.</p> <p>In other words, elements in $\mathcal{E}$ represent directed, pairwise relations between nodes in the graph.</p> <p>This can be naturally extended by considering a generalization of $\mathcal{E}$, say $\mathcal{E}^{(k)}$, with $k \in \mathbb{N}$, where</p> \[\mathcal{E}^{(k)} : \mathcal{V}^{k+1} \rightarrow \{0,1\}.\] <p>Intuitively, this represents <em>$k$-sized tuples</em> of nodes. For example, for $k=2$, this is equivalent to all <strong>directed triangles</strong> between nodes, while the case $k=1$ recovers the original graph with pairwise links. Note, how, intuitively, for $\mathcal{E}^{(k)}$, we would be effectively considering $k$-dimensional geometric objects: nodes would be 0-dimensional points, edges 1-dimensional lines, triangles 2-dimensional surfaces, and so on (of course this is just an intuition, for this to be true we would need to embed our nodes in a space and require relations to be undirected).</p> <p>Inserting higher-order information in message passing in GNNs can be shown to increase expressivity beyond the regular WL-test. More generally, it can be shown <d-cite key="bodnar2021weisfeiler"></d-cite> that the networks with <strong>order $k$ topological information are bounded by the $k$-WL test</strong>.</p> <p>While this is by no means a formal introduction to higher-order topological objects like Simplicial Complexes, it should be sufficient to paint an intuition about where we’re going: if we manage to message-pass also considering higher order topological objects, instead of just pairs of tokens, we may be able to capture more complex patterns in parallel, instead of having to rely on depth.</p> <h2 id="2-simplicial-attention">2-Simplicial Attention</h2> <p>The Higher-order Attention idea has been floating around for a while: its first implementation in a transformer architecture is dated to the 2019 work by Clift et al. <d-cite key="clift2020simplicialtransformer"></d-cite>, and further along has been reinvented/reinterpreted/tangentially rediscovered in a series of works, such as Representational Strengths and Limitations of Transformers <d-cite key="sanford2023representational"></d-cite>, Tensor attention <d-cite key="liang2024tensorattentiontraining"></d-cite>, The Cellular Transformer <d-cite key="ballester2024cellulartransformer"></d-cite>, AlphaFold 2 <d-cite key="jumper2021alphafold"></d-cite>, and TransNAR <d-cite key="bounsi2024transformersmeetnar"></d-cite>. Even I, since last year, have been obsessed with the idea, proposing it in public a couple of times.</p> <p>Apart from theoretical work, what this idea really needed was a step towards experimental validation under a modern paradigm. Fortunately, Aurko, Rohan and their colleagues delivered well beyond that: a novel implementation <d-cite key="roy2025fastsimplex"></d-cite> of a Higher-order Attention method was the first architectural change to seem to induce a change in the exponent in the scaling law of Large Language Models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/scaling_laws.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 1: Scaling law results from the Fast and Simplex: 2-Simplicial Attention in Triton paper. </div> <h3 id="rediscovering-simplicial-attention-from-the-topological-perspective">Rediscovering Simplicial Attention from the Topological Perspective</h3> <p>So, how do we extend our graph-based perspective on attention, so that it naturally becomes a (potentially higher-order) topological perspective?</p> <p>Refreshing the graph case, let’s take, for example</p> \[X \in \mathbb{R}^{n \times d}\] <p>And let’s treat the rows of $X$ as a <strong>point-cloud</strong>:</p> \[X = \begin{bmatrix} x_1^{\top}\\ x_2^{\top}\\ \vdots\\ x_n^{\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{n \times d_q}\] \[K = X W_k \in \mathbb{R}^{n \times d_q}\] \[V = X W_v \in \mathbb{R}^{n \times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle = q_i k_j^{\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^{\top} \in \mathbb{R}^{n \times n}.\] <p>The intuition is: the more points align in Query-Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_graph.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 2: An attention matrix encodes a graph. </div> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp(\alpha_{ij}/\sqrt{d_k})}{\sum_{j'=1}^n \exp(\alpha_{ij'}/\sqrt{d_k})}, \qquad A = \text{softmax}\left(\frac{\alpha}{\sqrt{d_k}}\right)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\text{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong> which is diffused from its neighbors to each node.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/message_passing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/message_passing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 3: Left: central node (i) weighs via attention neighboring nodes; Right: central node aggregates via attention weights the value function defined on neighboring nodes. </div> <p>But we already knew all of this from the previous blogpost. The key point to notice, here, is the operation we perform to extract a graph: we project $X$ into two distinct spaces via $W_Q$ and $W_K$, precisely because we need to perform a <em>bi</em>-linear form (the dot product) to extract a two-way relationship.</p> <p>What if we wanted to capture three-way relationships? Naturally, one could think of adding a second $K’$ matrix, resulting from a $W_{K’}$ projection, such that we would have a 3D tensor</p> \[T_{ijk} = \sum_{l} Q_{il} K_{jl} K'_{kl}\] <p>Which can also be seen as taking a multilinear product, if viewed per query:</p> \[T_{ijk} = \langle q_i, k_j, k'_s \rangle\] <p>Notice how, before, each attention score $A_{ij}$ represented the link weight going from node $i$ to node $j$. Now, each entry $T_{ijk}$ can instead be seen as the collective weight assigned to the triangle determined by the (directed) walk from node $i$, passing through node $j$, and ending up in node $k$.</p> <p>Such a triangle, in algebraic topology, may also be called a <em>2-simplex</em> (a node is a 0-simplex, an edge is a 1-simplex), explaining the naming of the attention mechanism.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_tensor.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 4: 2-simplicial attention's tensor T, in each of its entries, represents a (directed) 2-simplex (triangle). </div> <p>Now that we’ve found a formulation to represent 2-simplices (or simplexes, one day I’ll have to decide which version of the plural I prefer), how do we transfer our regular sparsification mechanism (softmax) to it? And, moreover, what even is a neighborhood in this case?</p> <p>The intuitive extension of attention (also used in 2-simplicial attention) treats this by keeping the query token as central: instead of being a matrix, our attention score is now a 3D tensor. This simply means that, instead of rows, we now normalize over entire slices associated with query $i$.</p> <p>Meaning, our softmax operation becomes:</p> \[\alpha^{(2)}_{ijk} = \text{softmax}(T)^{(2)}_{ijk} = \frac{e^{T_{ijk}}}{\sum_{jk} e^{T_{ijk}}}\] <p>Intuitively, this is defining the node’s neighborhood as the <strong>triangles it’s included in</strong>. Hence, here, we’re squashing to zero triangles with low three-way similarity, and amplifying the signal from the more similar ones.</p> <p>This makes sense because our final goal will be to use this information to update the nodes’ embeddings. With that said, there exist more ways to define adjacency for higher order structures: an interesting idea could be to normalize over triangles sharing faces, instead.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/simplex_message_passing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 5: Left: message passing now happens between 2-simplices (oriented triangles). Each 2-simplex is weighed by an entry in tensor T. Right: each 2-simplex has an aggregated value vector that is used to update the node's representation. </div> <p>The last piece of the puzzle is the $V$ matrix of regular attention. As we discussed previously, it can be thought of as a vector-valued function defined on nodes, where individual vectors are rows $V_i$.</p> <p>So what about 2-simplicial attention? Naturally, $V$ would still have to be defined token-wise, but now we have to engineer it so that it can represent, for node $i$, the value associated with the neighbors in a triangle, just like in regular attention $V$ was being aggregated from neighbors in the graph. Furthermore, in order to express value of tokens with full degrees of freedom, we introduce a second value projection, $V’$, that we use analogously to $K’$.</p> <p>What we need is for all triangles $(i,j,k)$ to aggregate $V_j$ and $V_{k}^{\prime}$ with some function $f:\mathbb{R}^{h}\times \mathbb{R}^{h} \rightarrow \mathbb{R}^{h}$. such that we have, for each triangle, a resulting vector \(V^{(2)}_{ijk} = f(V_{k},V_{k}^{\prime})\). In the paper, f is just the product of the entries of $V$, which can be conveniently written as an element-wise product between $V$ and $V^{\prime}$: \(V^{(2)}_{ijk} = V_{ik}V_{jk}^{\prime}\) Apart from convenience, this choice can also be seen as combining value vectors using an “AND” operation, in the sense that large values will compound, and a single small value is sufficient to drop the magnitude of the vector. This is opposed, for example, to having the function be \(V^{(2)}_{ijk} = V_{ik}+ V_{jk}^{\prime}\) which would, instead, be analogous to an “OR” operation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/value_aggregation.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 6: v and v' from each triangle are aggregated and used to update the central node's embedding. </div> <p>At last, we end up with $V^{(2)}$ being another 3D tensor. This allows us to perform the final operation of attention as a tensor contraction taking us back to our regular $\mathbb{R}^{n \times d}$ shape:</p> \[\text{attention}(x)_{il} = \sum_{jk} \frac{\alpha^{(2)}_{ijk} V^{(2)}_{jkl}}{\sqrt{d}}\] <p>Note how this operation can still be thought of as some kind of “diffusion”: we are aggregating value vectors from each triangle including node $i$, scaling them and summing them to update the vector in node $i$.</p> <p>Now, the extension to the n-simplicial case is trivial:</p> <p>For n-simplices, we just repeat the 2-simplicial recipe with $n$ Key projections. For an $(n+1)$-tuple $(i, j_1, \ldots, j_n)$ define the score tensor by a multilinear form</p> \[T_{i\,j_1 \cdots j_n} = \sum_{\ell} Q_{i\ell} \prod_{m=1}^n K^{(m)}_{j_m \ell} = \langle q_i, k^{(1)}_{j_1}, \ldots, k^{(n)}_{j_n} \rangle,\] <p>and normalize per-query over all $n$-tuples to get</p> \[\alpha^{(n)}_{i\,j_1 \cdots j_n} = \frac{\exp T_{i\,j_1 \cdots j_n}}{\sum_{(j_1, \ldots, j_n)} \exp T_{i\,j_1 \cdots j_n}}.\] <p>Values remain token-wise but are combined along each $n$-simplex via a symmetric $n$-ary reducer $f$; the simplest is the element-wise product “AND”</p> \[V^{(n)}_{i\,j_1 \cdots j_n} = \prod_{m=1}^{n} V^{[m]}_{j_m i},\] <p>though sum/mean (an “OR”) or MLP reducers are possible. The update is then a contraction over all $n$-tuples incident to $i$:</p> \[\text{attn}(X)_{i\ell} = \frac{1}{\sqrt{d}} \sum_{j_1, \ldots, j_n} \alpha^{(n)}_{i\,j_1 \cdots j_n} \left[V^{(n)}_{j_1 \cdots j_n}\right]_\ell\] <p>Topologically, we’re diffusing over the star of $i$ in the $n$-skeleton (cofaces incident to $i$), so higher-order interactions are captured in one hop.</p> <p>Naturally, an $n$-simplicial attention mechanism’s memory scales catastrophically quickly with sequence length, precisely with $O(L^{n+1})$. This means that we have to come up with ways of sparsifying this mechanism in order to make it practical.</p> <p>In the 2-simplicial attention paper, this is solved by performing Sliding Window Attention (SWA) with potentially different windows per dimension in the attention tensor.</p> <p>But is this the only way to tackle this? When I first started pondering these ideas, my first thought was instead to route tokens dynamically to a fixed size window. A very similar idea came recently with DeepSeek 3.2 <d-cite key="deepseek2025v32exp"></d-cite>, in the shape of DeepSeek Sparse Attention (DSA). The intuition is simple: why have a sliding window when you can hand-pick the tokens you want to use, with your preferred sparsity?</p> <p>DSA (DeepSeek Sparse Attention) replaces dense attention with a two-stage sparse mechanism: a <strong>lightweight indexer</strong> followed by <strong>top-k token selection</strong>.</p> <p>The indexer computes cheap similarity scores between each query and all past tokens. For each query token $i$ and indexer head $h$, it first computes</p> \[s_{ijh} = \text{ReLU}(\langle q^I_{ih}, k^I_{j} \rangle) \cdot w^I_{ih},\] <p>where $q^I_{ih}$ is the indexer’s query vector for token $i$ and head $h$, $k^I_j$ is the (shared) indexer key for token $j$, and $w^I_{ih}$ is a learned per-head weight. Summing over heads gives the final score</p> \[S_{ij} = \sum_{h=1}^{H_I} s_{ijh}.\] <p>For each query $i$, the top-k keys according to $S_{ij}$ are selected:</p> \[\mathcal{K}_i = \text{TopK}_j(S_{ij}, k),\] <p>and full attention is then computed <strong>only</strong> on this restricted set.</p> <p>This reduces the core attention complexity from $O(L^2)$ to $O(Lk)$, while preserving the most relevant interactions, making it particularly effective for long contexts.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/attention_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 7: Intuitively representing full attention, SWA and DSA in the regular case. </div> <p>In our case, we use a modified version of DSA to substitute SWA: first, we notice that substituting ReLU with softmax performs better on our small experiments on a token-wise level. Furthermore, to avoid individual computation of $qk_1^T$ and $qk_2^T$ distinct pairs, we instead leverage existing $QK^T$ from the previous regular attention layers, and directly index based on those scores, obtaining the same exact top-k scorers for both $k_1$ and $k_2$.</p> <p>This yields a tiny speedup to our very small model / small token-horizon run, while keeping the same scaling as SWA, where we have $O(Lk^2)$ (with $k$ chosen to be equivalent to the window size of SWA) instead of the full sequence $O(L^3)$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/loss_curves.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 8: Losses for a 127M variant of nanogpt using a 3:1 regular to 2-simplicial attention ratio, with a block size of 512 and a top-k/SWA window of 128 tokens. In gray, is the SWA-sparsified version, in green the DSA-inspired technique we introduced. In orange, regular self-attention. Total token horizon is of around 60M tokens. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/speedup-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/speedup-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/speedup-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/speedup.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig. 9: speedup across steps of DSA and SWA vs baseline </div> <p>While in Fig. 8 we can see that baseline appears to have roughly the same acceleration as windowed simplicial attention, we notice how the 2-simplicial attention paper itself only notices gains against the transformer at a much larger parameter size, as seen in Fig. 10.</p> <p>Overall, though, our acceleration is of an average of <strong>0.76%</strong> (so barely noticeable) with respect to the Sliding Window Attention version.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-480.webp 480w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-800.webp 800w,/2026/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-29-beyond-attention-as-graph/performance_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Fig. 9: Reported performance comparison between transformers and 2-simplicial attention in the original paper. </div> <h3 id="what-does-n-simplicial-attention-mean-for-depth">What Does n-Simplicial Attention Mean for Depth</h3> <p>As we’ve discussed, one of the key elements of depth is <strong>multi-token representation-learning</strong>. Another way to view it, is that individual tokens are in a <strong>constant relay race</strong>: each token wants to get to a target representation, but needs crucial information from other tokens’ representations to do so. If the proper representation is very hard to find, the model eventually runs out of depth to message-pass. 2-simplicial attention goes in the direction of fixing this, because it <strong>combinatorially opens up surface area</strong> for the model to do message-passing, for each block. Of course, the present one is just its first, prototypal iteration, which will inevitably change in the future.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>We’ve explored a recent advance in attention architecture, and explained it using our previously established topologically-oriented angle. We’ve also outlined a trivial extension to n-simplices of the mechanism, as well as demonstrated tiny gains in expressivity by utilizing a DSA-like sparsification of 2-simplicial attention keys, substituting SWA. Given my obsession with the topic, you’re very likely to read something from me on the topic soon. In the meantime, let me know what you think!</p>]]></content><author><name>anonymous</name></author><summary type="html"><![CDATA[We extend a graph-based perspective on attention to higher-order topological structures, exploring 2-simplicial attention and its implications for transformer depth and expressivity.]]></summary></entry><entry><title type="html">Attention Sinks from the Graph Perspective</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/attention-sinks-graph-perspective/" rel="alternate" type="text/html" title="Attention Sinks from the Graph Perspective"/><published>2026-04-28T00:00:00+00:00</published><updated>2026-04-28T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/attention-sinks-graph-perspective</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/attention-sinks-graph-perspective/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/hero-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/hero.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Attention sinks have recently come back to the forefront of architecture discussion, especially due to their appearance in <a href="https://github.com/openai/gpt-oss">gpt-oss</a> (although in a different form than the effect we’re discussing today).</p> <p>As a mechanism, attention sinks are easy to describe: when trained, decoder-only transformer models tend to allocate a disproportionate amount of attention to the first few tokens, and especially to the first.</p> <p>This effect is well studied in its practical terms, and is often attributed to the model “offloading” probability mass to the early tokens to avoid their spurious allocation elsewhere. Recent works, like Softpick <d-cite key="softpick2025"></d-cite>, provide architectural choices that prevent sinks from forming. While this explanation may sound convincing at a first glance, my intuition is still bothered by it: what do you mean the model “offloads”? Of course it doesn’t explore that possibility intentionally, there must be some mechanism by which the attention sinks are either advantageous or a result of an intrinsic bias to the model. In this blogpost, we will argue that there is a significant bias in decoder-only transformers that may be to blame, at least partially, for this phenomenon. Moreover, this will also allow us to introduce a series of blogposts focused on analyzing transformers from the lens of message passing on graphs.</p> <h2 id="attention-as-message-passing">Attention as Message-Passing</h2> <p>Recent work by Chaitanya K. Joshi <d-cite key="joshi2025"></d-cite> has finally freed us from having to formalize independently a well known property of Transformers (and especially of attention layers): them being a special case of Graph Neural Networks (just like pretty much anything else, to be fair).</p> <p>As a setting to our discussion, though, we will go over another angle with which attention can be seen as message-passing on a graph.</p> <p>Most people are usually introduced to (multi-headed) self-attention directly via the original Transformer paper <d-cite key="vaswani2017attention"></d-cite>. Despite this being generally a good practice in my opinion, it generally directs attention to being interpreted as the simplest way of making tokens interact in a transformer, or as just a soft version of a dictionary lookup. While neither being wrong, such interpretations often drown out some interesting geometric details that lie in attention itself.</p> <p>Let’s start with regular, multiheaded attention.</p> <p>Say you have $n$ tokens, with an embedding dimension $d$.</p> <p>Let our input tokens be shaped as a matrix $X \in \mathbb{R}^{n \times d}$, we first process $X$ with three different linear projections, namely $W_q$, $W_k$ and $W_v$, and end up with the respective $Q \in \mathbb{R}^{n \times d_q}$, $K \in \mathbb{R}^{n \times d_k}$ and $V \in \mathbb{R}^{n \times d_v}$ matrices.</p> <p>We then perform the well-known attention operation</p> \[\text{attention}(X) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Let’s take a look at $\alpha = QK^T$.</p> <p>If we rewrite it component-wise we get</p> \[\alpha_{ij} = \sum_{l=1}^{d_k} Q_{il}(K^T)_{lj} = \sum_{l=1}^{d_k} Q_{il}K_{jl}\] <p>and if we note that $Q$ and $K$’s rows, respectively $q_i$ and $k_i$, we see that</p> \[\alpha_{ij} = q_i k_j^T = \langle q_i, k_j \rangle\] <p>The attention matrix $\alpha$’s entries are thus simply speaking the euclidean dot product between token embeddings, projected via the query and key matrices.</p> <p>This still falls within the classical presentation of attention, so nothing to see here as of yet.</p> <p>What if we could reinterpret these operations from a more geometric/topological perspective?</p> <p>Let’s take, for example</p> \[X \in \mathbb{R}^{n \times d}\] <p>And let’s treat the rows of $X$ as a <strong>point-cloud</strong>:</p> \[X = \begin{bmatrix} x_1^{\top}\\ x_2^{\top}\\ \vdots\\ x_n^{\top} \end{bmatrix}, \qquad x_i \in \mathbb{R}^d.\] <p>Constructing the $Q$, $K$, $V$ matrices for attention, we effectively project that cloud in three ways</p> \[Q = X W_q \in \mathbb{R}^{n \times d_q}\] \[K = X W_k \in \mathbb{R}^{n \times d_q}\] \[V = X W_v \in \mathbb{R}^{n \times d_v}.\] <p>We use these distinct projections to capture a <strong>graph-like structure</strong>, building an adjacency matrix between tokens, which can be seen as <strong>nodes</strong></p> \[\alpha_{ij} = \langle q_i, k_j \rangle = q_i k_j^{\top}, \qquad q_i = [Q]_{i,:},\; k_j = [K]_{j,:}.\] <p>Stacking all scores:</p> \[\alpha = Q K^{\top} \in \mathbb{R}^{n \times n}.\] <p>The intuition is: the more points align in Query-Key space, the stronger their connection will be, and hence the stronger the link between the nodes.</p> <p>Finally, we use softmax to normalize outgoing weights from each node</p> \[A_{ij} = \frac{\exp(\alpha_{ij}/\sqrt{d_k})}{\sum_{j'=1}^n \exp(\alpha_{ij'}/\sqrt{d_k})}, \qquad A = \text{softmax}\left(\frac{\alpha}{\sqrt{d_k}}\right)\] <p>Each row of $A$ is a probability distribution and corresponds to the <strong>node’s neighbors</strong>; small logits shrink toward 0, meaning most edge weights are very close to zero, apart from a few. This effectively heavily sparsifies the neighborhood, assigning most of the link weights to just a few connections, while the rest go to zero.</p> <p>Lastly, the final operation</p> \[\text{attention}(x) = AV\] <p>can now be interpreted from an interesting perspective: $V$ can be seen as a <strong>vector-valued function defined on nodes of the graph</strong>.</p> <p>If we write it row-wise (hence focusing on each token, or node, at a time), we see that the updated function’s value associated with the node becomes</p> \[\text{attention}(x)_i = \sum_l A_{il} V_l\] <p>But what does multiplying a function defined on a graph by the adjacency mean? Let’s say we have a directed graph $\mathcal{G} = (V, E)$ with adjacency $A$, with a function $f: v \rightarrow \mathbb{R}$ and $v \in V$.</p> <p>Then, the multiplication $y = Af$ can be written, component-wise, as</p> \[y_i = \sum_{j} A_{ij} f_{j}\] <p>Remember that, for an adjacency matrix, elements of column $i$ represent incoming links from other nodes in the graph. This means that $y_i$, or the result of the adjacency-multiplied function $f$, is the weighted average of $f$ over incoming nodes to node $i$, where the weights are decided by the adjacency matrix’ entries. Intuitively, you can think of this process as a sort of <em>diffusion</em>: features are aggregates of their neighbours. This means that, if we start with a rather unequally spatially distributed function (say a very localized highly positive region, and the rest being zero), then nodes on the boundary of the highly positive region would “diffuse” the highly positive values towards neighbouring nodes. Of course the topology of the graph heavily influences the speed of this diffusion. Unsurprisingly, this ties back very well with the actual physical phenomenon of heat diffusion, as we will see in a future blogpost.</p> <h2 id="causal-transformers-and-attention-sinks">Causal Transformers and Attention Sinks</h2> <p>Note that the discussion so far has been agnostic of masking strategies applied to the attention score. While several uses of Transformer models employ attention bidirectionally, LLMs, our Large Model protagonists, are usually causally masking attention to leverage parallelism for their Next Token Prediction task.</p> <p>In our attention mechanism, this is done by substituting our $\alpha$ adjacency matrix with a masked, causal one, in the shape of $\alpha_m = \alpha \odot M$, with $M_{ij} = 1$ if $j \leq i$ and zero otherwise. Note that this gives our attention graph an even more interesting structure: our graph is now, by design, a <strong>Directed Acyclic Graph</strong> (<em>DAG</em>), meaning the graph contains no loops, and its adjacency matrix is nilpotent (meaning there exists $k$ such that $(A^k)_{ij} = 0$, $\forall i,j$).</p> <p>One interesting corollary of this observation is that adjacency-based diffusion over DAGs is bound to accumulate information in sinks, specifically, in the first tokens of a causal model. This can be made explicit by looking at the shape of powers of $A$:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/A_power_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig.1-4: We simulate an attention matrix being composed with itself several times, across three different configurations: bidirectional, masked pre-softmax, softmax. As we can see, the combination of masking (making the matrix nilpotent) and softmax (forcing row-wise mass to sum to one) rapidly recovers the familiar attention pattern we see for attention sinks. </div> <p>These plots (Fig.1-4) show exactly what we expect on a DAG: as we take powers of the (masked) attention matrix $A$ the mass moves “leftward” toward early tokens. In the strictly lower-triangular case (no self-loops) this is a nilpotent operator, so sufficiently high powers collapse entirely into the earliest positions.</p> <p>To connect this with learning dynamics, linearize one residual attention block (one head, for intuition; treat the MLP as a node-wise map) as</p> \[X^{\ell+1} \approx X^{\ell} + A^{\ell} X^{\ell} B^{\ell}, \qquad B^{\ell} = W_v^{\ell} W_o^{\ell}.\] <p>Stacking $L$ such blocks yields an end-to-end map that is a polynomial in the $A^{\ell}$’s:</p> \[X^{L} \approx \left(\prod_{\ell=1}^{L}(I + A^{\ell} B^{\ell})\right) X^{0} = X^{0} + \sum_{\ell} A^{\ell} B^{\ell} X^{0} + \sum_{\ell_2 &gt; \ell_1} A^{\ell_2} B^{\ell_2} A^{\ell_1} B^{\ell_1} X^{0} + \cdots\] <p>When the $A^{\ell}$ are geometrically similar across depth, dominant terms behave like <strong>powers of a causal $A$</strong>. That is the same “multi-hop diffusion” we saw in the previous figures, progressively concentrating influence onto the first columns (early tokens).</p> <p>But if that’s the case during a forward pass, what makes a model exhibit this bias across training, as it’s been noticed in the literature?</p> <p>As it turns out, backprop itself mirrors this geometry. Gradients w.r.t. hidden states propagate with Jacobian transposes along the value path:</p> \[g^{\ell} \approx (I + {B^{\ell+1}}^{\top} {A^{\ell+1}}^{\top}) \cdots (I + {B^{L}}^{\top} {A^{L}}^{\top}) g^{L}.\] <p>Hence token-wise gradients accumulate along <strong>column sums of products of $A$</strong> (or, equivalently, row sums of products of $A^{\top}$). In a causal DAG those column sums are largest for earlier positions, so both activations <strong>and</strong> gradients preferentially route through (and update) paths that point to early tokens.</p> <p>Practically, residual connections make the map a <strong>polynomial</strong> (not a single $A^k$), multi-head mixing and $B^{\ell}$ projections reshape directions, and layer-norm rescales signals. But the structural bias remains: deeper layers inherit updates that look like compositions of attention-diffusion steps, which, under causal masking, tend to be more and more “first-column concentrated”.</p> <p>Another corollary of our observation is that it would suggest that later layers are more subject to the attention sink phenomenon, while the very first layer should be much less impacted. This turns out to be true and well known when studying attention sinks, as is the case, for example, for LLaMA-2 <d-cite key="xiao2023streamingllm"></d-cite>, or in Sun et al. <d-cite key="sun2024massive"></d-cite> and Cancedda et al. <d-cite key="cancedda-2024-spectral"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-480.webp 480w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-800.webp 800w,/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-28-attention-sinks-graph-perspective/layer_analysis.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Attention patterns across layers showing the accumulation effect in later layers. </div> <p>Note that, while this <strong>may not be the single effect responsible for attention sinks</strong>, this means we should expect any causal decoder-only transformer to exhibit a bias towards allocating attention to its first few tokens (and increasingly so to the first).</p> <p>This fundamentally clashes with many interpretations of sinks: several works characterize them as a useful feature that is learned by the model. If what we propose is true, it’s exactly the opposite: when sinks <strong>don’t</strong> show up, it means <strong>the message-passing mechanism of your transformer is fundamentally flawed</strong>, and hence it performs worse.</p> <p>The attention sinks become a signal of <strong>healthy communication</strong> of tokens in attention, being a bias that is <strong>intrinsic to the causal, decoder-only transformer</strong>.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>So, to recap, what does this mean? We individuated a possible mechanism that may bias Causal Transformers to accumulate attention on its first few tokens. Note that we showed the mechanism in a highly simplified setting, and are proposing the idea that, despite those simplifications, the underlying effect is still strong enough to accumulate across training steps of a large transformer, and eventually explain the existence of attention sinks as we know them. In the next blogposts, we will use the same graph-centric framing of attention to analyze the problem of long context in transformer models, connecting it to heat diffusion and the oversmoothing and oversquashing phenomena known in the GNN literature. Stay tuned!</p>]]></content><author><name>anonymous</name></author><summary type="html"><![CDATA[We explore attention sinks in decoder-only transformers through the lens of message passing on graphs, revealing an intrinsic structural bias toward early tokens that may explain this phenomenon.]]></summary></entry><entry><title type="html">Hypes and Hopes for Causal Inference for Brain Dynamics</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/causal-ts/" rel="alternate" type="text/html" title="Hypes and Hopes for Causal Inference for Brain Dynamics"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/causal-ts</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/causal-ts/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The problem of identifying latent sources from sensor-level time series appears in many scientific domains, including neuroscience. A central question is whether it is possible to recover independent underlying sources from sensor mixtures, especially when the mixing process is nonlinear. We examine this question in the context of EEG recordings and evaluate whether Time-Contrastive Learning (TCL), a nonlinear ICA method, can recover meaningful source-level representations.</p> <blockquote> <p>In short: we find that while TCL can exploit nonstationarity, it does not outperform PCA in recovering components aligned with ground-truth cortical sources.</p> </blockquote> <hr/> <h2 id="background">Background</h2> <p>A classical formulation considers a vector of sources ( s = (s_1, …, s_n) ) and a mixing process:</p> \[x = A s ,\] <p>where ( x ) is observed sensor activity. ICA methods aim to recover sources by assuming statistical independence and non-Gaussianity.</p> <p>Standard ICA succeeds for linear mixtures but fails in general nonlinear settings. This motivated nonlinear ICA approaches such as the method of Hyvärinen &amp; Morioka (2016), which introduces <strong>nonstationarity</strong> as additional information for identifiability.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-causal-ts/mixing-480.webp 480w,/2026/assets/img/2026-04-27-causal-ts/mixing-800.webp 800w,/2026/assets/img/2026-04-27-causal-ts/mixing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-causal-ts/mixing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="what-is-identifiability">What Is Identifiability?</h2> <p>Identifiability formalizes whether the original sources can be uniquely recovered given observations. For nonlinear mixing ( x = f(s) ), the mapping is typically non-identifiable: many combinations of ( f ) and ( s ) produce the same observed distribution.</p> <p>Nonlinear ICA becomes identifiable only when extra structure—such as temporal nonstationarity—is introduced.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-causal-ts/identifiability-480.webp 480w,/2026/assets/img/2026-04-27-causal-ts/identifiability-800.webp 800w,/2026/assets/img/2026-04-27-causal-ts/identifiability-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-causal-ts/identifiability.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="time-contrastive-learning">Time-Contrastive Learning</h2> <p>TCL exploits the idea that if latent sources change their distributions differently across <em>time segments</em>, then a classifier trained to predict segment identity must extract features informative about those sources.</p> <p>The steps are:</p> <ol> <li><strong>Segment</strong> the multivariate time series ( x_t ) into windows indexed by ( \tau ).</li> <li><strong>Label</strong> points using their segment index.</li> <li><strong>Train</strong> a neural network encoder ( h(x_t; \theta) ) and a linear classifier:</li> </ol> \[w_\tau^\top h(x_t) + b_\tau.\] <p>The classifier approximates log-density differences</p> \[w_\tau^\top h(x) + b_\tau \approx \log p_\tau(x) - \log p_1(x),\] <p>and the learned representation satisfies:</p> \[h(x) \approx A q(s) + d ,\] <p>under assumptions on nonstationarity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-causal-ts/tcl-480.webp 480w,/2026/assets/img/2026-04-27-causal-ts/tcl-800.webp 800w,/2026/assets/img/2026-04-27-causal-ts/tcl-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-causal-ts/tcl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="experiments">Experiments</h2> <p>We evaluate TCL on both simulated nonlinear mixtures and EEG recordings. For EEG, we also compute source reconstructions to serve as proxy ground truth.</p> <h3 id="workflow-overview">Workflow Overview</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-causal-ts/workflow-480.webp 480w,/2026/assets/img/2026-04-27-causal-ts/workflow-800.webp 800w,/2026/assets/img/2026-04-27-causal-ts/workflow-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-causal-ts/workflow.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We obtain:</p> <ul> <li><strong>TCL embeddings</strong> from sensor data</li> <li><strong>PCA embeddings</strong> as a baseline</li> <li><strong>Source reconstruction embeddings</strong> as the evaluation target</li> </ul> <h3 id="evaluation">Evaluation</h3> <p>For simulated data, sources are nonstationary Laplacian processes with nonlinear mixing.</p> <p>For EEG, source-level signals correspond to 116 anatomical regions. We compare learned representations ( h_{\mathrm{tcl}} ) and PCA representations ( h_{\mathrm{pca}} ) against these reconstructed sources.</p> <h3 id="sparse-prediction-analysis">Sparse Prediction Analysis</h3> <p>We perform canonical correlation-style linear prediction:</p> <ul> <li>Train a linear model to map ( h_{\mathrm{tcl}} \rightarrow h_r )</li> <li>Train a baseline model ( h_{\mathrm{pca}} \rightarrow h_r )</li> </ul> <p>Ideal recovery would produce a near-permutation matrix. Instead we observe:</p> <ul> <li>PCA: sharp diagonal structure</li> <li>TCL: weak, sparse, non-diagonal mappings</li> </ul> <hr/> <h2 id="results">Results</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-causal-ts/results-480.webp 480w,/2026/assets/img/2026-04-27-causal-ts/results-800.webp 800w,/2026/assets/img/2026-04-27-causal-ts/results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-causal-ts/results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Regression coefficients comparing learned components with source-level components:</p> <table> <thead> <tr> <th>method</th> <th>unseen_size</th> <th>MDD</th> <th>ODER</th> </tr> </thead> <tbody> <tr> <td>PCA</td> <td>4</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>PCA</td> <td>10</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>PCA</td> <td>25</td> <td>0.0004</td> <td>0.9998</td> </tr> <tr> <td>TCL</td> <td>4</td> <td>0.0003</td> <td>0.9998</td> </tr> <tr> <td>TCL</td> <td>10</td> <td>0.0004</td> <td>0.9997</td> </tr> <tr> <td>TCL</td> <td>25</td> <td>0.0004</td> <td>0.9998</td> </tr> </tbody> </table> <p>TCL did not surpass PCA in alignment with source components. PCA maintained strong diagonal structure, while TCL representations produced weaker and more diffuse correlations.</p> <hr/> <h2 id="discussion">Discussion</h2> <h3 id="why-causal-inference-style-identifiability-fails-on-realistic-eeg">Why causal-inference-style identifiability fails on realistic EEG?</h3> <p>Real EEG includes strong 1/f background activity from many neural populations. The nonstationarity signal needed by TCL is overshadowed. Thus TCL learns features reflecting global variability rather than source-specific fluctuations.</p> <h3 id="are-causal-effects-recoverable-at-the-scalp">Are causal effects recoverable at the scalp?</h3> <p>Some directed-dependency methods (e.g., Granger-style approaches) can reveal predictive relationships, but these differ from structural causal models. Identifiability for nonlinear SCMs would require:</p> <ul> <li>known interventions,</li> <li>control over confounded pathways,</li> <li>or guaranteed invertibility of the generative process,</li> </ul> <p>none of which hold for EEG sensors.</p> <hr/> <h2 id="appendix">Appendix</h2> <p>Dataset details, implementation notes, and source reconstruction parameters follow those in the accompanying project documentation.</p> <p>Additional figures (e.g., replication checks) verify that our TCL implementation behaves as expected.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We study whether modern identifiability-based nonlinear ICA methods, in particular Time-Contrastive Learning (TCL), can recover meaningful sources from realistic scalp-level brain recordings such as EEG. Using simulated data, EEG sensor data, and source-reconstructed cortical activity, we evaluate whether TCL provides representations aligned with the underlying sources.]]></summary></entry><entry><title type="html">Can Coding Agents be General Agents?</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/coding-agents/" rel="alternate" type="text/html" title="Can Coding Agents be General Agents?"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/coding-agents</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/coding-agents/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Immediately after the emergence of Transformer-based Language Models (LMs), researchers and developers began exploring LM code generation.<d-cite key="ahmad2021plbart"></d-cite> With heavy investment into training models specifically on coding data, LMs have seen dramatic improvements in coding capabilities.<d-cite key="chen2021codex"></d-cite> The top score on SWE-Bench Verified, a benchmark testing models on real-world software engineering tasks, jumped from 49% to 78% through 2025.<d-cite key="jimenez2023swebench"></d-cite><d-cite key="openai2024swebenchverified"></d-cite> The newer Terminal Bench, released in May of 2025, has also tracked frontier LLMs improving from 43% to 61% on complex tasks in the terminal, including analyzing data, calling APIs, and addressing security vulnerabilities.<d-cite key="terminalbench2024"></d-cite></p> <p>Propelled by compounding improvement, frontier labs have been developing coding agents, which augment foundation models with a shell sandbox and code editor to help with coding tasks. These agents, including Claude Code, Codex CLI, and Gemini CLI, have seen explosive developer adoption: since launching in May 2025, Claude Code now receives over 4.4 million downloads every week.<d-cite key="anthropic2025claudecode"></d-cite></p> <p>Surprisingly, people are using these coding agents for purposes far beyond the realm of software development. Users report applying coding agents to tax preparation, creating content, personal knowledge management, and more.<d-cite key="steipete2025claudecomputer"></d-cite> At their core, “coding” agents are versatile: even Anthropic has acknowledged this shift, rebranding its Claude Code SDK to the general “Agent SDK.”<d-cite key="anthropic2024agentsdk"></d-cite> This makes sense: knowledge work takes place entirely in software - business apps, browsers, spreadsheets, databases. Coding agents are naturally fit to meet knowledge work where it lives.</p> <h3 id="how-does-a-coding-agent-work">How Does a Coding Agent Work?</h3> <p>Broadly, an “AI Agent” is a system that autonomously pursues a goal by perceiving its environment, reasoning iteratively, and taking actions, with minimal human intervention. Typically, agents take action through manually predefined tools.<d-cite key="schick2023toolformer"></d-cite> Often, these tools include complex business logic that reduces the reasoning burden on the LM itself. For example, a simple banking agent may be provided: “read_balance,” “withdraw_money,” “deposit_money,” and “transfer_funds.” In practice, each of these tools would include guardrails to prevent prohibited transactions or incorrect calculations.</p> <p>Coding agents are a specific type of agent designed for software engineering tasks. These agents operate within software development environments (repositories, IDEs, sandboxes, terminals). <em>Coding agents are unique because they write, execute, and debug their own scripts at runtime,</em> <em>instead of being limited to a pre-defined set of tools.</em> They are especially versatile: they can quickly orient themselves in new software environments by querying for information, installing packages to unlock new capabilities, and resolving errors from logs. This self-governed feedback loop unlocks utility beyond software development: coding agents can theoretically plug-and-play into <em>any</em> software environment—offering an interesting pathway to generalizability.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/1_Coding_Agent_Architecture-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/1_Coding_Agent_Architecture-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/1_Coding_Agent_Architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/1_Coding_Agent_Architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 1: The coding agent is an emergent agent architecture, which allows a model to create its own code tools at runtime and iterate with the help of code outputs and error logs. </div> <p>Experimentally, past research has shown that training models on code improves general reasoning. For instance, Aryabumi et al. find that adding code data to pre-training yields up to 8.2% relative improvement on natural language reasoning benchmarks.<d-cite key="aryabumi2024codepretraining"></d-cite> Separately, Wang et al. demonstrate that their CodeAct agent, using executable Python scripts, outperformed JSON and text-based tool-calling alternatives with a 20% higher success rate on non-coding tasks.<d-cite key="wang2024codeact"></d-cite> Furthermore, the continued scaling of test-time compute means coding agents are increasingly adept at attacking multi-hour, project-scale tasks.<d-cite key="openai2025gpt51"></d-cite> Practically and experimentally, there is visible potential in applying coding agents to general tasks.</p> <p>Given this convergence, we ask: <strong>Can coding agents succeed as general business agents? And if not yet, where do they break down?</strong></p> <p>This post makes three contributions:</p> <ol> <li><strong>A framework for coding-agent generalization.</strong> <br/> We propose that success as a general business agent requires bidirectional translation between business/domain and code/software layers, and articulate four concrete capabilities this entails.</li> <li><strong>An evaluation gap analysis.</strong> <br/> We survey prominent benchmarks and show that code-level evals (SWE-bench, Terminal-Bench) lack business context while domain-reasoning evals (τ-bench<d-cite key="yao2024taubench"></d-cite>, BFCL<d-cite key="gorilla2024bfcl"></d-cite>) lack complex code execution - leaving full-stack translation underexplored.</li> <li><strong>Observations from an ERP case study.</strong> <br/> We deploy frontier coding agents on a production-grade Enterprise Resource Planning (ERP) software instance with multi-constraint Sales-to-Fulfilment and HR operational tasks, document distinct failure modes at the business-code boundary, and propose asymmetric feedback from the environment to explain persistent agent overconfidence.</li> </ol> <h2 id="defining-success">Defining Success</h2> <p><strong>In order to be a reliable general agent, a coding agent must be excellent at translating between the business/domain layer and the code/software layer.</strong> In practice, that means it can:</p> <ol> <li> <p>Understand business‑level requests and policies: turning instructions like “approve this expense if it fits our travel policy and budget” into precise goals and constraints.</p> </li> <li> <p>Inspect the current system state via generated and executed code: writing and running queries or scripts to see what requests, data, and approvals already exist across the relevant systems.</p> </li> <li> <p>Plan a business‑level solution: deciding, at the domain level, what should actually happen (approve, modify, escalate, or reject) given the policies and current state.</p> </li> <li> <p>Encode that plan back into the software: implementing the decision as code/API calls that update the system state to match the intended business outcome.</p> </li> </ol> <h2 id="what-gets-measured-gets-improved">What Gets Measured Gets Improved</h2> <p>It’s worth examining how we evaluate frontier models and agents today. <strong>Current evaluations largely fall into two camps: those that test code-level competence and those that test business/domain-level reasoning</strong>. The full-stack, bi-directional <em>translation</em> between them is undercovered.</p> <h3 id="code-and-system-level-benchmarks">Code and System-Level Benchmarks</h3> <p><strong>SWE-Bench</strong> tests whether models can resolve real GitHub issues by generating code patches. Given a repository snapshot and an issue description like <em>“DatetimeIndex.to_period() fails with timezone-aware timestamps”</em>, the agent must locate the bug across thousands of files, generate a fix, and pass the repository’s unit tests. The input is technical; the output is technical; the evaluation is whether tests pass. SWE-Bench has become a standard because it captures real software engineering difficulty, but the “business context” facet is thin.</p> <p><strong>Terminal-Bench</strong> takes a similar approach for evaluating terminal mastery. Tasks range from <em>“Build Linux kernel 6.9 from source with a custom printk message”</em> to <em>“Configure a git server that pushes to a webserver on port 8080.”</em> Agents interact with a sandboxed Linux terminal and are evaluated with task-specific checks: whether files exist, services respond, and commands succeed. Again, the request is system-level, and the evaluation is system-level.</p> <h3 id="tool-use-and-domain-reasoning-benchmarks">Tool-Use and Domain Reasoning Benchmarks</h3> <p>Tool-use benchmarks like <strong>The Berkeley Function Calling Leaderboard (BFCL)</strong> evaluate whether a model can turn prompts and pre-defined function specifications into <em>correct function calls</em> across many domains. For example, if tasked with calculating a five-year compounding return and given a tool set containing a “calculate_compound_interest” function, the agent must locate that tool, identify the principal, interest rate, and time frame from context, and correctly pass them as arguments into the function.</p> <p>BFCL measures how often models get tool names and arguments right, probing the flow from business requests to tool selection and arguments, but without complex code execution or policy adherence behind it.</p> <p><strong>τ-bench</strong> pushes further by adding policy compliance. An airline agent may handle a user request: <em>“I want to change my reservation to a different destination.”</em> The agent must gather information through dialogue, consult policy documents about change fees and cabin-class restrictions, and call domain APIs correctly to make the change. This is close to what a real deployment would require, but the underlying, synthetically-generated database and API are intentionally much simpler than real-world business software.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/2_Critical_Evaluation_Gap-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/2_Critical_Evaluation_Gap-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/2_Critical_Evaluation_Gap-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/2_Critical_Evaluation_Gap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 2: The critical evaluation gap: Code evaluations lack business context; business evaluations lack code interaction. </div> <p>Existing evaluations usually stress one or two capabilities at a time: choosing tools, business reasoning, writing code, or manipulating a complex software system. Few require an agent to do everything end-to-end.</p> <h1 id="case-study-coding-agent-in-enterprise-resource-planning">Case Study: Coding Agent in Enterprise Resource Planning</h1> <p>To cover this gap, we ask: <em>Can a coding agent run business processes end-to-end, under realistic constraints, by writing and executing its own code against a live ERP instance?</em></p> <h2 id="setup">Setup</h2> <p>An ERP is the single source of truth for businesses’ operations, supporting more than 3.8 million companies worldwide and, by some estimates, underpinning 77% of the world’s transaction revenue. We identified the ERP as our ideal testing ground for “general” business tasks because it contains nearly all core business functions - finance, HR, supply chain, and customer relationship management - as native, interdependent modules. In this environment, we can test how the agent completes simple, single-module tasks like onboarding an employee, but also complex, cross-module tasks like fulfilling a customer order end to end. Furthermore, the ERP has certain validation rules built into its software, while a majority of business logic still remains as the agent’s burden.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/3_Example_ERP_Process-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/3_Example_ERP_Process-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/3_Example_ERP_Process-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/3_Example_ERP_Process.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 3: An example of a process in an ERP, involving an order-to-delivery workflow. </div> <h3 id="environment">Environment</h3> <p>We built a fictional company inside Odoo 19.0 Community Edition, the open-core ERP with over 12 million users across SMBs and large enterprises. Odoo is particularly well suited for this research: unlike most proprietary business apps, it is free to self‑host, easy to spin up in many parallel sandboxed instances, and provides visibility into the underlying PostgreSQL database. This gives us a realistic “world model” of business operations (sales, inventory, manufacturing, HR, purchasing) while still allowing fine-grained control. To mimic a production environment, we populate the Odoo instance with complete company data - products, vendors, price lists, bills of material, lead times, etc.</p> <p>We containerize the environment, agent execution, and verification modules separately to prevent information from the verifier from contaminating agent reasoning and to facilitate reproducibility.</p> <h3 id="task-structure">Task Structure</h3> <p>We test the agent’s ability to complete real-world business workflows that are commonly found in an ERP. Each task gives the agent a natural-language instruction with an objective, constraints, and a policy rulebook. For example:</p> <p>Instruction: <em>TechStart Solutions needs 40 ergonomic chairs within 7 days (budget: <code class="language-plaintext highlighter-rouge">$12,000</code>). DesignHub Agency needs 30 chairs within 10 days (budget: <code class="language-plaintext highlighter-rouge">$9,000</code>). Handle both orders, allocating stock and creating manufacturing orders as needed.</em></p> <p>Policy: <em>Maintain at least 25% gross margin. Minimize procurement costs. You are not allowed to increase the list price of the finished goods.</em></p> <p>In each scenario, the agent has to make at least 2 interdependent business decisions while being subject to at least 2 interacting operational constraints. To pass the example scenario above, the agent must query the ERP, determine the optimal fulfillment strategy, and document its solution (as confirmed sales, manufacturing, and purchase orders) - all through writing and executing code at runtime.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/4_Simple_Scenario_Visualization-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/4_Simple_Scenario_Visualization-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/4_Simple_Scenario_Visualization-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/4_Simple_Scenario_Visualization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 4: A visualization of the decisions present in a simplified scenario. The solution to this task requires multiple choices and execution steps within the ERP. </div> <h3 id="agent-harness">Agent Harness</h3> <p>We developed a simple coding agent harness for our trials. The harness includes only a bash tool, which is how the agent writes and executes code scripts to interact with the Odoo environment. We provide the agent with database credentials, an isolated workspace for temporary files, and rudimentary examples of five Odoo data models. We do not provide comprehensive documentation on the Odoo instance and API. This compels the agent to iteratively map out the ERP environment from scratch.</p> <p>We test the agent with GPT-5 and Claude Sonnet 4.5, using maximum allowed reasoning effort/thinking budget settings.</p> <h3 id="evaluation">Evaluation</h3> <p>After each trial, our task verifier compares the PostgreSQL database with ground-truth rubrics that define the correct database end-state. Our rubric considers the following:</p> <ol> <li> <p>Constraint Resolution: Did the agent satisfy all constraints posed by the task instruction and data loaded into the ERP instance?</p> </li> <li> <p>Resource Optimization: Did the agent solve the task optimally? E.g. figured out the most cost-effective fulfillment plan.</p> </li> <li> <p>Traceability: Are the resulting data objects linked correctly in the ERP? E.g. procurement orders are linked to the sales orders they fulfill.</p> </li> <li> <p>Policy Adherence: Did the agent follow all the rules outlined in the policy rulebook provided to it in the prompt?</p> </li> </ol> <h2 id="results">Results</h2> <h3 id="success-out-of-the-box">Success Out-Of-The-Box</h3> <p>In the first trial of 10 easy scenarios, our coding agent using Claude Sonnet 4.5 reliably scores above 80% on the verifiers. These simple tasks include creating sales orders for one or two customers, selecting the cheapest vendor, and generating invoices. This consistent accuracy is impressive: even given limited documentation, the agent intuitively issued correct API calls and navigated Odoo’s data model.</p> <p>The coding agent was so successful on these tasks that, in order to challenge the agent, we needed to scale up the complexity of the tasks to require the agents to make 5+ decisions and weigh 5+ constraints, at which point many domain-level resource allocation tasks became challenging even for humans to work out without computational assistance.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/5_Results_Chart-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/5_Results_Chart-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/5_Results_Chart-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/5_Results_Chart.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 5: We test Claude Sonnet 4.5 and GPT-5 on 20 scenarios across a gradient of difficulty from Easy to Hardest. We observe that increasing the complexity of the scenarios by increasing the number of constraints results in a breakdown of performance. We separate our evaluation of each scenario into 4 dimensions: Constraint Resolution, Resource Optimization, Traceability, and Policy Adherence. Noticeably, GPT-5 tended to come up with comparable or even better-quality business plans as Claude 4.5 Sonnet, but GPT-5 struggled more with correct API calls, which caused lower scores across the board. </div> <h3 id="failure-modes">Failure Modes</h3> <p>As complexity increased, characteristic failures began to emerge.</p> <p>Certain issues came from the business reasoning side alone. Initially, in “medium” and some “hard” scenarios, the agent produced solutions that were feasible but suboptimal. For instance, all or most constraints were satisfied, but the agent failed to calculate the most cost-effective outcome. Eventually, for the remaining “hard” and “hardest” scenarios, the agent stops satisfying constraints altogether. The agent’s traceability also degrades with additional complexity as it neglects maintaining documentation of its more complex decisions.</p> <p>Beyond simple reasoning failures, however, we started to see a breakdown in the agent’s ability to maintain coherence between the business level and the code level.</p> <h4 id="lazy-code-heuristics">Lazy Code Heuristics</h4> <p>One class of such problems can be described as “lazy code heuristics” that don’t accurately implement business logic.</p> <p>The most salient example involved a task policy to <em>only order goods and components from American vendors</em>. Rather than filtering vendors by address, the agent used a glaringly wrong lazy heuristic based on the vendor’s name.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre><span class="c1"># Policy in the request:
#   "...order only from American vendors..."
# Relevant Odoo data model:
#   res.partner
#     ├── name
#     ├── contact_address          
#     ├── Country Info
#     │     ├── country_code    
#     │     └── country_id                         
#     └── Company Info (...)
</span>
<span class="c1"># Code the agent wrote to filter vendor list
</span><span class="n">is_american</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">American</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">vendor</span><span class="p">.</span><span class="n">name</span>
    <span class="ow">or</span> <span class="sh">"</span><span class="s">Northern</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">vendor</span><span class="p">.</span><span class="n">name</span>
    <span class="ow">or</span> <span class="sh">"</span><span class="s">Catalyst</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">vendor</span><span class="p">.</span><span class="n">name</span>
    <span class="c1"># etc.
</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The agent correctly understands the natural‑language policy (“order only from American vendors”) but implements it with a crude string‑matching shortcut. The business intent is correct, but the code that is supposed to enforce it is wrong.</p> <h4 id="hallucinations-in-the-business-layer">Hallucinations in the Business Layer</h4> <p>We also observed the case where the agent hallucinates at the business layer, causing it to write functional code that ultimately leads it astray. In one case, the agent is tasked with scrapping faulty LED Boards that were found to have condensation damage. The agent asserts that if the boards have water damage, they <em>must</em> be stored in a fridge, even though the setup actually has them in the warehouse with all others. When it queries the imaginary “fridge” location, it finds no results, so it assumes those boards have been discarded.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/6_LED_Hallucination-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/6_LED_Hallucination-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/6_LED_Hallucination-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/6_LED_Hallucination.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 6: The agent hallucinates that damaged LED boards must be stored in the fridge, without any supporting evidence in the system, leading to functional-but-misleading code. </div> <p>The direction of failure flips from the previous case. The code is locally coherent given the agent’s belief that there is a “Fridge” storage location. The query execution makes sense, but the underlying reasoning is hallucinated.</p> <h4 id="ignored-policy-constraints">Ignored Policy Constraints</h4> <p>In some runs, agents failed to internalize explicit rulebook constraints at all.</p> <p>For an employee vacation request HR scenario, a policy stating that “<em>days off should be consecutive</em>” was simply ignored during reasoning, leading to prohibited fragmented schedules.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/7_HR_Policy_Violation-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/7_HR_Policy_Violation-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/7_HR_Policy_Violation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/7_HR_Policy_Violation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 7: The agent disregards the policy requirement to only schedule consecutive days off, and incorrectly claims successful completion. </div> <p>In more complex procurement tasks, margin requirements and lead-time rules were dropped despite being queried correctly from the ERP. Here, the break happens one step earlier: the agent simply does not carry certain rules forward through its reasoning at all.</p> <h4 id="overconfidence">Overconfidence</h4> <p>Whether the solution was optimal, suboptimal, or incorrect, one thing remained constant: the agent almost always reported success and remained unaware of its shortcomings.</p> <p>This reflects an asymmetry in the feedback profile of the environment. At the code level, the agent receives concrete signals: bad imports, malformed payloads, and incorrect field names, which all produce exceptions. Conversely, we observe that at the business level, feedback is sparse. The inbuilt ERP guardrails reject obviously invalid and ill-formatted interactions but won’t flag suboptimal choices or policy violations. Only when we run the task verifier do we see that the outcome was wrong.</p> <p>This pattern can be viewed as a form of specification gaming: the agent optimizes for the measurable proxy (code execution) rather than the true objective (business correctness). Recent work demonstrates that gaming behaviors generalize, models trained to exploit easily-discovered reward signals will zero-shot transfer those behaviors to novel environments.<d-cite key="denison2024sycophancy"></d-cite> Coding agents may be particularly susceptible: their training provides dense, unambiguous feedback at the code layer (tests pass, scripts execute, errors resolve), effectively teaching that execution success equals task success. This learned prior does not transfer when code is merely the <em>medium</em> for a business objective rather than the objective itself. These silent failures point to the untrustworthiness of the agent’s own pass/fail conclusions.</p> <h2 id="synthesis">Synthesis</h2> <p>The most striking result is that the coding agent can immediately complete straightforward tasks out-of-the-box, at near-human efficacy. With no ERP-specific tooling, the agent reliably executed real business workflows - creating orders, selecting vendors, generating invoices - that would typically require dedicated integrations.</p> <p>Let us revisit our criteria for a “successfully” generalizable coding agent: (1) the agent was able to understand our instructions, (2) inspect the undocumented environment state, (3) reason through the business-level solution, and (4) write a series of scripts to perfectly complete the task and match our verifiers. If all real-world tasks were this simple, we could label the coding agent as “general” already.</p> <p>But when scenarios were loaded with complex decisions and constraints, failures emerged in ways disconnected from raw coding ability. The four failure modes are all different ways of breaking the business-code translation:</p> <ul> <li>Lazy heuristics: correct understanding, incorrect implementation</li> <li>Hallucinations: correct code, incorrect world model</li> <li>Ignored constraints: rules dropped before reasoning even begins</li> <li>Overconfidence: code-level success mistaken for task-level success</li> </ul> <p><strong>In order to get coding agents to generalize, we should measure and optimize code and business-level correctness together.</strong></p> <h2 id="conclusion">Conclusion</h2> <p>Coding agents represent a promising path toward general-purpose AI. They are already widely adopted, rapidly improving, and operate in the same medium through which most white-collar work already flows: software.</p> <p>After examining performance in our simulations, we find that coding agents have advantageous traits that help them succeed in settings where the goal is not to write code, but to achieve correct real-world outcomes <em>through</em> code in complex systems. If these agents can learn to reason reliably across unmapped business domains while retaining their native fluency in code, the distance to broad automation shrinks considerably.</p> <p>Today, coding agents still do not generalize to complex business workflows. Domain-specific tools and guardrails will remain important in the near term - but they address failure modes one instance at a time. This risks running afoul of the bitter lesson<d-cite key="sutton2019bitterlesson"></d-cite>, and history suggests that general methods eventually win.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[As coding agents have seen rapid capability and adoption gains, users are applying them to general tasks beyond software engineering. In this post, we investigate whether coding agents can successfully generalize to end-to-end business process automation. We identify gaps in current evaluations, and conduct a case study to evaluate a coding agent on practical business tasks in an open-core Enterprise Resource Planning system. We find that the agent reliably completes simple tasks but exhibits characteristic failures on complex tasks, suggesting that bridging domain logic and code execution is a key bottleneck to generalizability.]]></summary></entry><entry><title type="html">Diffusion Guidance - Opportunities for Physical Sciences</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-guidance/" rel="alternate" type="text/html" title="Diffusion Guidance - Opportunities for Physical Sciences"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/diffusion-guidance</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-guidance/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Diffusion models have emerged as a state-of-the-art approach for sampling from complex probability distributions. Prominent examples are image-generation models like Stable Diffusion, where the model generates a high-quality image within seconds based on a given text prompt like <em>“A corgi with sunglasses on the beach”</em>. What makes these models stand out is their ability to faithfully adhere to a given prompt.</p> <p>These capabilities arise from techniques collectively known as guidance, which direct the output of diffusion models toward specified conditions. To guide the models, we use a score function divided into a <strong style="color: #25a18e;">prior</strong> and <strong style="color: #00a5cf;">likelihood term</strong>:</p> \[\begin{equation*} \textcolor{#A125A1}{\nabla_{\mathbf{x}} \log \, p(\mathbf{x} \mid \mathbf{y})} = \textcolor{#25a18e}{\nabla_{\mathbf{x}} \log p(\mathbf{x})} + \textcolor{#00a5cf}{\nabla_{\mathbf{x}} \log p(\mathbf{y} \mid \mathbf{x})} \end{equation*}\] <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/score_decomposition.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/score_decomposition.svg" class="img-fluid" width="100%" height="auto" style=" max-width: 25%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This blog post aims to share insights into methods that go beyond traditional guidance techniques. We begin by explaining the fundamentals of the classifier guidance approach in the first section and then explore recent developments in guiding diffusion models. Our goal is not to favor any specific method but to present alternatives, especially useful when data is limited or training resources are constrained.</p> <h2 id="background">Background</h2> <p>In this section, we will briefly summarize the key features of diffusion models. Readers familiar with the score-based formulation might want to skip ahead.</p> <h3 id="diffusion-models">Diffusion Models</h3> <p>A <em>forward</em> diffusion process, which gradually destroys data over time, can be defined via a stochastic differential equation (SDE)<d-cite key="song2021generative"></d-cite>:</p> \[\begin{equation} d\mathbf{z}_t = \mathbf{f}(\mathbf{z}_t, t) \; \mathrm{d}t + \mathrm{g}(t) \; d\mathbf{w} \textrm{,} \end{equation}\] <p>where $\mathbf{f}$ and $\mathrm{g}$ are determined by the noise schedule, and $d\mathbf{w}$ is a Brownian motion. For an affine drift $\mathbf{f}$, the forward process can be rewritten in closed form:</p> \[\begin{equation} \label{eq:diff_co} \mathbf{z}_t = \alpha_t \mathbf{x} + \sigma_t \boldsymbol{\epsilon} , \end{equation}\] <p>with $\mathbf{x} \sim p_{\text{data}}(\mathbf{x})$ sampled from the data distribution and $\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ drawn from a standard gaussian distribution. The forward diffusion processes starts with a clean data sample $\mathbf{z}_0=\mathbf{x}$ (where $\alpha_0=1$ and $\sigma_0=0$), and gradually adds noise. In the case of a variance-exploding noise schedule, $\alpha_t$ remains $1$ and $\sigma_t$ grows with $t$.</p> <p>The <em>reverse</em> process<d-cite key="song2021generative,anderson1982reverse"></d-cite> is also an SDE running backward in time:</p> \[\begin{equation} \label{eq:reverse_process} \mathrm{d}\mathbf{z}_t = [\mathbf{f}(\mathbf{z}_t, t) - \mathrm{g}(t)^2 \; \textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)}]\; \mathrm{d}t + \mathrm{g}(t) \; d\mathbf{w} , \end{equation}\] <p>where \(\textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)}\) is called the score function (aka Stein score), and is approximated by a neuronal network \(s_{\theta}(\mathbf{z}_t, t) \approx \nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)\) using denoising score matching<d-cite key="hyvarinen2005estimation"></d-cite>.</p> <p>The score function defines a <strong>time-dependent vector field</strong> that guides points toward the data distribution.</p> <h3 id="conditional-diffusion-models">Conditional Diffusion Models</h3> <p>Previously, we demonstrated how to create a process for sampling from an unconditional distribution. Extending this to the conditional case, we aim to sample from a posterior $p(\mathbf{x} \mid \mathbf{y}) \text{,}$ which we can decompose using Bayes’ rule:</p> \[\begin{equation} p(\mathbf{x} \mid \mathbf{y}) = \frac{p(\mathbf{y} \mid \mathbf{x}) p(\mathbf{x})}{p(\mathbf{y})} . \end{equation}\] <p>Applying the logarithm and differentiating with respect to $\mathbf{x}$, allows us to define a conditional form of the score function (relying on the fact that the denominator does not depend on $\mathbf{x}$):</p> \[\begin{equation} \nabla_{\mathbf{x}} \log \, p(\mathbf{x} \mid \mathbf{y}) = \nabla_{\mathbf{x}} \log p(\mathbf{x}) + \nabla_{\mathbf{x}} \log p(\mathbf{y} \mid \mathbf{x}) . \end{equation}\] <p>Where the conditional <em>reverse process</em> from Eq. \eqref{eq:reverse_process} is given by:</p> \[\begin{equation} \mathrm{d}\mathbf{z}_t = [\mathbf{f}(\mathbf{z}_t, t) - \mathrm{g}(t)^2 \; (\textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)} + \textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)})] \; \mathrm{d}t + \mathrm{g}(t) \; d\mathbf{w} . \end{equation}\] <p>This formulation allows us to reuse our unconditional model (\(\textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)}\)) and simply add a guidance term \(\textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)}\). The guidance term acts like a force pushing our samples to be consistent with the condition \(\mathbf{y}\). The remaining practical challenge is deriving</p> \[\textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)} .\] <p>There are two distinct strategies for doing so:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. <strong>Classifier Guidance (Learning the Likelihood)</strong>: <br/>Learn a time-dependent classifier that approximates the likelihood score through supervised training on labeled data.</p> <p style="margin: 0;">2. <strong>Analytical Likelihoods (Defining the Likelihood)</strong>: <br/>Leverage analytically tractable likelihood functions, which are particularly valuable for inverse problems, where paired training data $(\mathbf{x}, \mathbf{y})$ is scarce or unavailable.</p> </div> <p><strong>In both approaches, the diffusion model itself remains frozen</strong>, serving only as a fixed prior distribution $p(\mathbf{x})$. We are left to define or learn how to assess likelihoods, particularly their gradients, which makes this framework highly data-efficient for adapting to new tasks.</p> <h2 id="classifier-guidance-cg">Classifier Guidance (CG)</h2> <p>Classifier Guidance<d-cite key="dhariwal2021diffusion"></d-cite> trains a time-dependent neuronal network to approximate the likelihood:</p> \[\begin{equation} p_{\phi}(\mathbf{y} \mid \mathbf{z}_t, t) \approx p(\mathbf{y} \mid \mathbf{z}_t, t) . \end{equation}\] <p>We therefore train a classifier whose inputs are noisy samples that resemble the intermediate steps of the reverse diffusion process. In particular, given a dataset of paired samples $(\mathbf{x}, \mathbf{y})$, we can train a time-conditional classifier $p_{\phi}(\mathbf{y} \mid \mathbf{z}_t, t)$ by minimizing</p> \[\begin{equation*} \mathbb{E}_{t \sim \mathcal{U}(0, T), (\mathbf{x}, \mathbf{y}) \sim p_{\text{data}}, \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})}[-\log p_{\phi}(\mathbf{y} \mid \mathbf{z}_t, t)] , \end{equation*}\] <p>with the noisy sample $\mathbf{z}_t = \alpha_t \mathbf{x} + \sigma_t \boldsymbol{\epsilon}$. Handling inputs at varying noise levels allows the classifier to operate across the entire diffusion trajectory.</p> <p>The trained classifier can be used as an approximation of the <strong><span style="color: var(--color-likelihood);">log-likelihood gradient</span></strong>:</p> \[\begin{equation} \textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)} \approx \textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p_{\phi}(\mathbf{y} \mid \mathbf{z}_t, t)} , \end{equation}\] <p>where the gradient with respect to the input is easy to compute using automatic differentiation frameworks such as PyTorch or JAX.</p> <p>Finally we combine the prior score with the log likelihood gradient. In practice, classifier guidance works with a scaled version of the guidance term controlled by the parameter $\gamma$,</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{z}_t} \log \, p_{\gamma}(\mathbf{z}_t \mid \mathbf{y})} = \textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)} + \gamma \textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)} . \end{equation}\] <p>This scaling lets us adjust the guidance strength: if $\gamma$ is too high, we obtain unrealistic samples that move away from the data manifold, whereas if $\gamma$ is too low, the guidance has little effect.</p> <p>Before continuing, we want to mention that classifier-free guidance is currently more commonly used, as it often outperforms classifier guidance and does not require training a separate classifier. However, since this blog post focuses on techniques that modify sampling behavior without retraining the original diffusion model, classifier guidance serves as a better illustrative example. Additionally, the name classifier-free guidance is somewhat misleading, because a classifier is still involved, it is simply embedded implicitly within the diffusion model itself. For completeness, we have included classifier-free guidance in the expandable section below.</p> <details> <summary> Classifier-Free Guidance (CFG) </summary> <div style="color: black;"> Classifier-free guidance can be derived in a similar manner, the main difference is that the diffusion model itself acts as classifier. Applying Bayes' rule to $$ \nabla_{\mathbf{z}_t}\,p(\mathbf{y} \mid \mathbf{z}_t) $$ results in $$ \nabla_{\mathbf{z}_t}\,p(\mathbf{y} \mid \mathbf{z}_t)=\nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t \mid \mathbf{y}) - \nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t). $$ To avoid training of an unconditional and a conditional model, we train one single model with an additional condition: $$ \nabla_{\mathbf{z}_t}\,p(\mathbf{y} \mid \mathbf{z}_t)=\nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t \mid \mathbf{y}) - \nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t \mid \emptyset), $$ where we condition the model on the null token $\emptyset$, to represent the unconditional model. We can now replace the likelihood term in classifier guidance: $$ \nabla_{\mathbf{z}_t} \log \, p_{\gamma}(\mathbf{z}_t \mid \mathbf{y}) = (1 - \gamma) \nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t, \mid \emptyset) + \gamma \nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t \mid \mathbf{y}) $$ </div> </details> <h2 id="analytical-likelihoods">Analytical Likelihoods</h2> <p>In the last section, we discussed how to train an additional model serving as a likelihood. For a class of problems where the relationship between the data and the observations follows a known probabilistic model, the likelihood function can be derived analytically. A canonical example are inverse problems, where we have observations $\mathbf{y}$ and we want to recreate $\mathbf{x}$. We consider observations of the form:</p> \[\begin{equation} \mathbf{y} = \mathcal{A}(\mathbf{x}) + \mathbf{n} \quad \textrm{where} \quad \mathbf{y}, \mathbf{n} \in \mathbb{R}^m, \mathbf{x} \in \mathbb{R}^n. \end{equation}\] <p>Here \(\mathbf{y}\) is our observation, \(\mathbf{x}\) is the underlying clean data, \(\mathcal{A}:\mathbb{R}^n \mapsto \mathbb{R}^m\) is the <strong>forward operator</strong> (which may be linear or nonlinear), and $\mathbf{n}$ is observation noise.</p> <p>For Gaussian noise $\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$, the likelihood is defined by:</p> \[\begin{equation} \label{eq:y-given-x-gaussian-noise} p(\mathbf{y} \mid \mathbf{x}) = \mathcal{N}(\mathbf{y}; \mathcal{A}(\mathbf{x}), \sigma^2 \mathbf{I}) \propto \exp \left(-\frac{1}{2 \sigma^2} ||\mathbf{y} - \mathcal{A}(\mathbf{x})||^2 \right) . \end{equation}\] <p>Typical inverse problems are:</p> <ul> <li> <p><strong>Inpainting</strong>, where the operator $\mathcal{A}$ acts as a mask that zeros out certain pixels, requiring the model to fill in the missing areas.</p> </li> <li> <p><strong>Super-resolution</strong>, where the operator $\mathcal{A}$ performs a downsampling operation and the model’s task is to generate the image at high resolution.</p> </li> <li> <p><strong>Deblurring</strong>, where the operator $\mathcal{A}$ acts as a gaussian filter operation blurring out pixels.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/cover-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-guidance/cover-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-guidance/cover-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/cover.png" class="img-fluid" width="100%" height="auto" title="Posterior Sampling with Diffusion Models" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1: Source: https://dps2022.github.io/diffusion-posterior-sampling-page</figcaption> </figure> <h3 id="diffusion-posterior-sampling-dps">Diffusion Posterior Sampling (DPS)</h3> <p>Direct computation of the time-dependent likelihood gradient $\textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)}$ poses significant computational challenges. Let’s calculate the likelihood by marginalizing over $\mathbf{x}$:</p> \[\begin{equation} p(\mathbf{y} \mid \mathbf{z}_t) = \int p(\mathbf{y} \mid \mathbf{x}) \, p(\mathbf{x} \mid \mathbf{z}_t) d \mathbf{x} = \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x} \mid \mathbf{z}_t)}[p(\mathbf{y} \mid \mathbf{x})] . \end{equation}\] <p>The intractability arises from two sources: (1) the marginalization over the high-dimensional space of clean samples $\mathbf{x}$, and (2) the dependence of $p(\mathbf{y} \mid \mathbf{z}_t)$ on the reverse diffusion process, which requires integrating over all trajectories from time $t$ to $0$. DPS<d-cite key="chung2022diffusion"></d-cite> addresses this through an approximation that avoids explicit marginalization:</p> \[\begin{equation} p(\mathbf{y} \mid \mathbf{z}_t) \approx p(\mathbf{y} \mid \hat{\mathbf{x}}(\mathbf{z}_t)), \end{equation}\] <p>where \(\hat{\mathbf{x}}(\mathbf{z}_t) = \mathbb{E}[\mathbf{x} \mid \mathbf{z}_t]\). By Tweedie’s formula<d-cite key="efron2011tweedie,tweedie1957"></d-cite>, the posterior mean is given by</p> \[\begin{equation} \mathbb{E}[\mathbf{x} \mid \mathbf{z}_t] = \frac{1}{\alpha_t} (\mathbf{z}_t + \sigma_t^2 \nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)) . \end{equation}\] <p>Substituting \(\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)\) with the learned score function \(s_\theta(\mathbf{z}_t;t)\) gives the estimator:</p> \[\begin{equation} \mathbf{x}_\theta(\mathbf{z}_t;t) = \frac{1}{\alpha_t} (\mathbf{z}_t + \sigma_t^2 s_\theta(\mathbf{z}_t;t)) . \end{equation}\] <p>Instead of learning the score function, we can train a network to predict the clean data directly, i.e., act as a denoiser<d-cite key="karras2022elucidating"></d-cite>. There are three main parameterizations: score prediction, data prediction, and noise prediction. These parameterizations are mathematically equivalent and allow seamless integration of all current diffusion model formulations into this framework. <d-cite key="kingma2023understanding"></d-cite> provides an extensive analysis.</p> <p>We can now combine the Gaussian likelihood from Eq. \eqref{eq:y-given-x-gaussian-noise} with the network’s data prediction, \(\mathbf{x}_\theta(\mathbf{z}_t;t)\), to analytically obtain the log-likelihood gradient:</p> \[\begin{equation} \nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t) \approx -\frac{1}{\sigma^2} \nabla_{\mathbf{z}_t} ||\mathbf{y} - \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t))||^2_2 . \end{equation}\] <p>Again, we combine the prior and the likelihood term to obtain the score function:</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{z}_t} \log p(\mathbf{x} \mid \mathbf{z}_t)} \approx \textcolor{#25a18e}{s_\theta(\mathbf{z}_t;t)} - \textcolor{#00a5cf}{\zeta \nabla_{\mathbf{z}_t} ||\mathbf{y} - \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t))||^2_2} . \end{equation}\] <h3 id="example">Example</h3> <p>Let’s consider a simple example using the Swiss Roll data to demonstrate the effectiveness of diffusion priors in solving inverse problems. We define a linear forward operator $\mathcal{A}$ that performs a projection of the 2D data onto the \(x_1\) axis:</p> \[\begin{equation} \mathbf{y} = \mathcal{A}(\mathbf{x}) = \mathbf{A}\mathbf{x} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \end{equation}\] <p>We sample observations from the ground-truth manifold but restrict them to the left side of the Swiss Roll. Next, for each observation \(\mathbf{y}\), we sample from the posterior distribution \(p(\mathbf{x} \mid \mathbf{y})\), which places the samples back on the spiral manifold. The results are displayed in the figure below, where we have:</p> <ol> <li> <p><strong>Observations (Left)</strong>: This plot shows the observations $p(\mathbf{y} \mid \mathbf{x})$. Since the operator projects everything onto the x-axis, the unique spiral structure of the Swiss Roll is entirely gone. This creates an ill-posed inverse problem: for any observed point on this line, there are multiple possible “correct” locations on the original spiral that could have produced the outcome.</p> </li> <li> <p><strong>Standard Sampling (Right)</strong>: This represents the unconditional generation from our trained diffusion model. While these points perfectly inhabit the Swiss Roll manifold, they are random samples from the prior $p(\mathbf{x})$. They show us what the model “knows” about the data distribution, but they have no connection to the specific measurements we observed.</p> </li> <li> <p><strong>DPS Sampling (Center)</strong>: Here we show the reconstruction using Diffusion Posterior Sampling with $\zeta=0.25$. By using the measurement gradient to guide the generation process, DPS pushes \(\mathbf{x}\) to match the observation \(\mathbf{y}\). It solves the ambiguity by finding points that are both consistent with the measurement $\mathbf{y}$ and highly probable under the learned prior $p(\mathbf{x})$, recovering the spiral shape despite information loss through the operator.</p> </li> </ol> <iframe src="/2026/assets/html/2026-04-27-diffusion-guidance/measurements_dps_standard.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> <p><br/></p> <p>We note that DPS is only one possible way to estimate the likelihood term \(\textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)}\). For interested readers, we recommend the excellent survey by Daras et al. <d-cite key="daras2024survey"></d-cite>, which compares a range of different approaches.</p> <h2 id="applications-for-physical-sciences">Applications for Physical Sciences</h2> <p>We now turn to scientific applications and examine how flexible the discussed approaches are in practice, particularly in settings where physical constraints must be respected. The following is just an high level overview.</p> <h3 id="diffusionpde">DiffusionPDE</h3> <p>DiffusionPDE<d-cite key="huang2024diffusionpde"></d-cite> use DPS to sample Partial Differential Equations (PDEs) solutions from sparse observation, by exploiting the structure of the underlying PDE, a guidance term can be derived to align the sampling to the underlying structure of the PDE.</p> <p>We take the <strong>Darcy flow</strong> PDE equation as an example:</p> \[\begin{aligned} -\nabla \cdot \big(a(\mathbf{c})\nabla u(\mathbf{c})\big) &amp;= q(\mathbf{c}), \quad \mathbf{c} \in \Omega, \\ u(\mathbf{c}) &amp;= 0, \quad \mathbf{c} \in \partial\Omega. \end{aligned}\] <p>Defining the residual as an operator</p> \[\begin{aligned} f(\mathbf{c}) = \nabla \cdot \big(a(\mathbf{c})\nabla u(\mathbf{c})\big) + q(\mathbf{c}), \end{aligned}\] <p>so that valid solutions satisfy $f(\mathbf{c}) = 0 $. This residual is used to construct an additional physics-based guidance loss:</p> \[\begin{aligned} \mathcal{L}_{\textrm{pde}} = ||\mathbf{0} - f(\hat{\mathbf{x}}(\mathbf{z}_t)) ||^2_2 . \end{aligned}\] <p>DiffusionPDE then augments the conditional score function as</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t \mid \mathbf{y}, f)} \approx \textcolor{#25a18e}{s_\theta(\mathbf{z}_t;t)} \textcolor{#00a5cf}{+ \zeta \nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t) - \zeta_{\text{pde}} \nabla_{\mathbf{z}_t} \mathcal{L}_{\text{pde}}}. \end{equation}\] <p>The sampling process is very similar to the one we saw before, combining the part we saw in DPS sampling and the additional term for the PDE solution.</p> <h3 id="inequality-constraints-for-rare-event-sampling">Inequality constraints for rare event sampling</h3> <p>Rare events play a central role in many scientific settings, yet they are often difficult to sample. This is evident in weather prediction, where extreme events such as floods are of particular concern.</p> <p>The work by Finzi et al. <d-cite key="finzi2023user"></d-cite> showed that operators can also be defined via inequality constraints. For a one-dimensional inequality constraint \(\mathcal{A}(\mathbf{x}) &gt; y\), we want to sample from \(p(\mathcal{A}(\mathbf{x}) &gt; y \mid \mathbf{z}_t)\).</p> <p>This inequality constraint is defined by a Gaussian CDF function \(\Phi\):</p> \[\begin{equation} p(\mathcal{A}(\mathbf{x}) &gt; y \mid \mathbf{z}_t) \approx \Phi \left( \frac{\mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t)) &gt; y}{\sqrt{\nabla \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t))^T \hat{\Sigma}(\mathbf{z}_t) \nabla \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t)) }} \right) , \end{equation}\] <p>with the covariance of \(\mathbf{x}\) given \(\mathbf{z}_t\)</p> \[\hat{\Sigma}(\mathbf{z}_t)=\frac{\sigma_t^2}{\alpha_t^2} (\mathbf{I} + \sigma_t^2 \nabla_{\mathbf{z}_t}^2 \log p(\mathbf{z}_t) )\] <p>and</p> \[\nabla \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t))= \left. \nabla \mathcal{A}(\mathbf{x}) \right|_{\mathbf{x} = \mathbf{x}_\theta(\mathbf{z}_t;t)} .\] <p>Using the Gaussian CDF function assigns high probability to events with \(\mathcal{A}(\mathbf{x}) &gt; y\).</p> <p>We can then sample from these event by using</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{z}_t} \log \, p(\mathbf{z}_t \mid \mathcal{A}(\mathbf{x}) &gt; y)} = \textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)} + \textcolor{#00a5cf}{\zeta \nabla_{\mathbf{z}_t} \log p(\mathcal{A}(\mathbf{x}) &gt; y \mid \mathbf{z}_t)} . \end{equation}\] <p>The work shows how to effectively sample extreme events in a Fitzhugh-Nagumo system, modeling neuron spiking events occurring only in 1/30 of the trajectories.</p> <h2 id="closing-takeaways">Closing takeaways</h2> <p>To sum it up, guidance enables us to adapt the sampling process of diffusion models by modifying the direction of the underlying vector field. While Classifier Guidance and Classifier-Free Guidance are well known tools, guidance based on analytical likelihoods is still less widely known, especially in scientific applications.</p> <ul> <li><strong>Analytical Likelihoods:</strong> Enable pre-trained diffusion models to be reused as flexible priors across many downstream tasks, without retraining.</li> <li><strong>Applications in Physical Sciences:</strong> By defining forward operators for PDE residuals, or inequality constraints, guidance can steer the sampler toward solutions that are not only data-consistent but also physically more plausible.</li> </ul> <p>The approaches we discussed, represent just a small part of the full landscape and we see that there is currently growing interest in new guidance strategies. A particularly exciting path is the idea of learning strong universal priors in the physics domain and using them across various downstream tasks.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Guidance has been a central driver of the success of diffusion models, enabling precise control over the sampling process toward desired target conditions. The most widely used techniques include Classifier Guidance and Classifier-Free Guidance. Recently, however, there has been growing interest in alternative guidance strategies. In this blog post, we review recent progress in training-free diffusion guidance methods and highlight their applications in scientific domains.]]></summary></entry><entry><title type="html">Dissecting Non-Determinism in Large Language Models</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/dissecting-non-determinism/" rel="alternate" type="text/html" title="Dissecting Non-Determinism in Large Language Models"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/dissecting-non-determinism</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/dissecting-non-determinism/"><![CDATA[<p>In scientific research, reproducibility is the currency. If an experiment cannot be replicated, its validity crumbles. However, in the era of Large Language Models (LLMs), we face a fundamental contradiction: we are attempting to build deterministic science upon inherently stochastic tools. So, how do we ensure that our results are not merely the product of chance?</p> <p>To address this, we must first dissect the nature of the problem. In this post, we will begin by exploring the non-deterministic nature of LLMs. We will see that this variability is not a “bug,” but rather a “feature” essential for simulating creativity and human fluency <d-cite key="Holtzman"></d-cite>. We will observe that while it is possible to enforce reproducibility, doing so incurs significant computational costs.</p> <p>However, determinism does not shield us from Prompt Brittleness, a phenomenon where a minuscule change in the input can drastically alter the output probability distribution. This makes prompt engineering notoriously fragile, as semantically insignificant perturbations in the prompt lead to significant variations in performance <d-cite key="FormatSpread"></d-cite>. Consequently, finding the optimal prompt becomes a tedious and onerous process.</p> <p>Finally, we will address the LLM-as-a-Judge paradigm. If the judge itself varies due to its non-deterministic nature, can we trust its verdict? We will analyze how this variance affects evaluation metrics and the dangerous implications of using these models as arbitrary evaluators.</p> <p>We conclude that understanding these dynamics in LLM experimentation can help us avoid misleading conclusions and ensure the integrity of our evaluation sets. In light of our findings, we suggest the imperative need to adopt consistency-oriented practices, recognizing non-determinism as a critical variable that must be managed in any rigorous experiment.</p> <hr/> <h2 id="non-determinism-as-a-crisis-of-reproducibility">Non-Determinism as a Crisis of Reproducibility</h2> <p>The bedrock of any scientific claim or rigorous engineering system is reproducibility <d-cite key="he2025nondeterminism"></d-cite>. In the context of LLMs, this foundation is critically compromised. While “instability” is often marketed as “creativity” for open-ended tasks <d-cite key="Temperature, NonDeterminism"></d-cite>, its persistence in validation contexts, even when explicitly disabled, represents a systemic flaw in modern AI infrastructure.</p> <p>There is a fundamental tension between the perceptual quality of text and the computational stability required for rigorous software engineering. While intuition suggests that a model should always select the most probable sequence of words to maximize quality, empirical evidence demonstrates that deterministic decoding strategies lead to severe text degradation <d-cite key="Holtzman"></d-cite>. This phenomenon forces the adoption of stochastic methods, sacrificing deterministic reproducibility in favor of semantic coherence and human-like diversity.</p> <h3 id="the-probabilistic-engine-why-maximization-fails">The Probabilistic Engine: Why Maximization Fails</h3> <p>Although LLM text generation is formally based on left-to-right probability decomposition, the deterministic search for the most probable sequence proves counterproductive. This approach leads to “neural text degeneration,” a phenomenon where models produce generic, incoherent content or get stuck in infinite loops instead of natural discourse <d-cite key="Holtzman"></d-cite>. As shown in Figure 1, this occurs because the probability assigned to a phrase increases with each repetition, creating a positive feedback loop that is difficult for strategies like Beam Search to break.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure1-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure1-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1: The probability of a repeated phrase, extracted from: <d-cite key="Holtzman"></d-cite>.</div> <p>The reason for this failure is not a search error, but an intrinsic property of human language: natural text rarely remains in a “high probability zone” for multiple consecutive steps, instead fluctuating towards lower probability but higher information tokens, as illustrated in Figure 2. This occurs because humans optimize their speech to be informative, actively avoiding stating the obvious, whereas a model that consistently selects the most probable token favors the “lowest common denominator” <d-cite key="Holtzman"></d-cite>, resulting in sentences that are grammatically correct but semantically empty.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure2-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure2-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure2.png" class="img-fluid w-50 mx-auto d-block" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 2: The probability assigned to tokens generated by Beam Search and humans given the same context, extracted from: <d-cite key="Holtzman"></d-cite>.</div> <p>Therefore, to generate text indistinguishable from humans, it is imperative to reintroduce variance through stochastic sampling strategies, accepting that $Output(Input_A) \neq Output(Input_A)$ in subsequent runs.</p> <h3 id="the-role-of-sampling-strategies">The Role of Sampling Strategies</h3> <p>The selection of the next token, despite a clear probability distribution computed by the LLMs, is determined by the chosen decoding algorithm. These strategies control the balance between coherence and creativity <d-cite key="Holtzman, Temperature"></d-cite>. Traditionally, Maximization-Based Decoding Methods operate under a deterministic logic, aiming to find the sequence with the highest accumulated probability. Specific strategies include Greedy Search, which always selects the single token with the highest probability, and Beam Search, which searches for the best continuation by maintaining a fixed number of hypotheses (beam width $b$) at each step. However, empirical evidence concludes that this objective is often inappropriate for open-ended text generation, as these methods frequently produce repetitive or degenerate text and tend to get stuck in infinite loops<d-cite key="Holtzman"></d-cite>.</p> <p>To introduce human-like diversity, Stochastic Decoding Methods are employed. Pure Sampling selects the next token directly from the full probability distribution, but this can be refined using Sampling with Temperature $(T)$, which reshapes the distribution by scaling the logits before the Softmax function<d-cite key="Temperature"></d-cite>. Low temperatures $(T \to 0)$ exaggerate probability differences to make the model conservative and deterministic, while high temperatures $(T &gt; 1)$ “flatten” the curve to allow less probable tokens, increasing creativity but also the risk of hallucinations <d-cite key="Holtzman, hinton2015distillingknowledgeneuralnetwork, Temperature, wang2023costeffectivehyperparameteroptimizationlarge"></d-cite>. To improve coherence without sacrificing diversity, the most effective methods employ Truncation to avoid the “unreliable tail” of the low-confidence distribution<d-cite key="Holtzman"></d-cite>. Top-K Sampling achieves this by restricting the sampling pool to the $K$ most probable tokens. However, the state-of-the-art solution is Nucleus Sampling (Top-P), which selects a dynamic set of tokens whose cumulative probability mass exceeds a pre-chosen threshold $P$ <d-cite key="wang2023costeffectivehyperparameteroptimizationlarge"></d-cite>. Unlike Top-K, Nucleus Sampling adjusts the size of the sampling set dynamically based on the shape of the probability distribution at each time step, offering a superior balance between quality and variety <d-cite key="Holtzman"></d-cite>.</p> <p>According to Table 1, Nucleus Sampling $(p=0.95)$ is the best overall decoding strategy. It achieves a perplexity score of $13.13$, which is remarkably close to that of human text $(12.38)$. In stark contrast, maximization-based methods like Greedy and Beam Search exhibit unnaturally low perplexity (around 1.50) and suffer from significantly higher repetition rates.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/table1-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/table1-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/table1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/table1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Table 1: Main results for comparing all decoding methods with selected parameters of each method. The numbers closest to human scores are in bold, extracted from: <d-cite key="Holtzman"></d-cite>.</div> <h3 id="the-myth-of-temperature-zero">The Myth of Temperature Zero</h3> <p>A prevailing misconception in the engineering of LLMs is that non-determinism is merely a configurable setting. The logic assumes that if the sampling temperature is set to 0.0 (utilizing greedy sampling), the model will purely select the most probable token, thereby rendering the output fully deterministic. However, as noted by Ouyang et al. (2025) <d-cite key="NonDeterminism"></d-cite>, this finding:</p> <blockquote> <p>“is contrary to many people’s belief… that setting the temperature to 0 can make ChatGPT deterministic… because… the model applies greedy sampling which should indicate full determinism”.</p> </blockquote> <p>Research into code generation using ChatGPT challenges the assumption that setting the temperature to 0 guarantees deterministic results. While decreasing the temperature reduces randomness compared to the default setting ($T=1$), empirical evidence demonstrates that it does not eliminate it entirely. Even with the temperature set to 0, a significant ratio of problems persists where the model produces inconsistent results across identical requests <d-cite key="NonDeterminism"></d-cite>. Similarly, and contrary to the belief that lower temperatures always yield higher accuracy for logical tasks, extensive testing on Multiple-Choice Question-Answering (MCQA) tasks indicates a stable plateau of performance as show in Figure 3 <d-cite key="Temperature"></d-cite>. Accuracy remains statistically stable at temperatures ranging from 0.0 to 1.0, and only begins to degrade significantly once the threshold of 1.0 is crossed, plummeting towards zero as the text becomes incoherent near 1.6.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure3-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure3-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 3: Accuracy by temperature from 0.0 to 1.6 for GPT-3.5 using the CoT prompt on the 100-question exam, extracted from <d-cite key="Temperature"></d-cite>.</div> <p>Within this stability range (0.0 - 1.0), an interesting trade-off between precision and creativity emerges. Although accuracy remains flat, text similarity metrics decrease as temperature rises in Figure 4 (a), confirming that higher temperatures produce more diverse outputs without necessarily compromising the correctness of the answer. This stability trend generalizes across most major models, including GPT-4, Claude 3 Opus, and Gemini Pro as show in the Figure 4 (b) <d-cite key="Temperature"></d-cite>. Specifically in code generation, although GPT-4 appears slightly more deterministic than GPT-3.5 at the default temperature $(T=1)$, both models suffer from similar levels of non-determinism at $T=0$, reinforcing that model architecture improvements alone do not solve the variability issue <d-cite key="NonDeterminism"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure4a-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure4a-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure4a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure4a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure4b-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure4b-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure4b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure4b.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <p style="text-align: center;">(a)</p> </div> <div class="col-sm mt-3 mt-md-0"> <p style="text-align: center;">(b)</p> </div> </div> <div class="caption"> Figure 4: Analysis of the 100-question exam using the CoT prompt. (a) TF-IDF text similarity by temperature (0.0–1.0) and model. (b) Accuracy by temperature and model, extracted from <d-cite key="Temperature"></d-cite>. </div> <p>Finally, the impact of temperature is strongly mediated by the prompting strategy used. In general problem solving, accuracy stability remains constant regardless of whether Chain-of-Thought (CoT), domain expertise, or self-recitation is used, although CoT generally outperforms others in absolute accuracy <d-cite key="Temperature"></d-cite>. However, in code generation, the type of prompt significantly affects determinism: “concise” prompts produce more deterministic results at $T=0$, while Chain-of-Thought prompts introduce higher randomness and variance, even at zero temperature settings <d-cite key="NonDeterminism"></d-cite>.</p> <h3 id="the-root-cause-infrastructure-level-chaos">The Root Cause: Infrastructure-Level Chaos</h3> <p>Even when the temperature is set to zero $(T=0)$, LLM outputs often remain non-deterministic. Research into deep learning infrastructure reveals that this instability stems from the collision between physical hardware limitations and software optimization strategies. According to Riach et al. (2019) <d-cite key="riach2019determinism"></d-cite>, the fundamental source of this non-determinism lies in the asynchronous nature of floating-point operations on GPUs. Because floating-point arithmetic is not associative due to finite precision and rounding errors, mathematically, $(a+b)+c \neq a+(b+c)$ precision is lost when adding numbers with different exponents <d-cite key="he2025nondeterminism"></d-cite>. This issue is exacerbated during parallel execution, where thousands of threads accumulate values using operations like <code class="language-plaintext highlighter-rouge">atomicAdd</code>. Since the order in which these threads finish is random (a race condition), the sequence of summation changes from run to run, causing rounding errors to accumulate differently and leading to bitwise differences in the final result <d-cite key="riach2019determinism, he2025nondeterminism"></d-cite>.</p> <p>However, while floating-point instability provides the mathematical potential for error, Horace He et al. (2025) <d-cite key="he2025nondeterminism"></d-cite> argue that this hypothesis alone is insufficient to explain the non-determinism observed in production inference servers. Their research identifies the true culprit as a lack of “batch invariance” within the “noisy neighbor” environment of commercial APIs. To maximize throughput, engines like vLLM utilize dynamic batching, grouping a user’s request with random requests from other users. Critical GPU kernels, specifically Matrix Multiplication, RMSNorm, and Attention are often not batch-invariant, meaning the numerical output for a specific input actually shifts depending on the size of the concurrent batch. Consequently, a request’s output depends on the server load and the specific activity of other users sharing the GPU at that exact millisecond, effectively making server load a hidden non-deterministic variable that alters the forward pass <d-cite key="he2025nondeterminism"></d-cite>.</p> <h3 id="the-cost-of-truth">The Cost of Truth</h3> <p>We can address these inconsistencies, but doing so imposes a significant “Determinism Tax.” Achieving bitwise reproducibility requires ensuring that every operation occurs in a fixed order, yet implementing these fixes places tangible penalties on system performance and complexity. According to He et al. (2025) <d-cite key="he2025nondeterminism"></d-cite>, eliminating non-determinism is not merely a matter of setting flags; it often requires rewriting kernels to be “batch-invariant,” a process that precludes the use of highly optimized dynamic execution paths. This results in a steep performance penalty: in benchmarks conducted a standard vLLM setup completed a task in 26 seconds, whereas the unoptimized deterministic version took 55 seconds a slowdown of more than 100% <d-cite key="he2025nondeterminism"></d-cite>. Even with further optimizations, the process remained significantly slower at 42 seconds. Similarly, Riach et al. (2019) <d-cite key="riach2019determinism"></d-cite> notes that even on a single GPU, enforcing deterministic constraints, such as disabling cuDNN auto-tuning, resulted in a 6% decrease in relative performance for perception models.</p> <p>To pay this tax, developers must enforce strict controls across the entire software stack. This begins with fixing random seeds across all libraries, including Python, NumPy, and PyTorch or TensorFlow, to ensure that weights and stochastic operations initialize identically. Beyond initialization, we must force the hardware to avoid non-deterministic optimizations. Measures such as setting <code class="language-plaintext highlighter-rouge">TF_CUDNN_DETERMINISTIC=true</code> or enabling <code class="language-plaintext highlighter-rouge">torch.use_deterministic_algorithms(True)</code> are necessary to disable faster, non-deterministic atomic operations and specific convolution algorithms, trading speed for consistency.</p> <p>This operational overhead forces a difficult question: Is reproducibility a requirement or a luxury? For creative assistants, the cost is likely unjustified, as variance often functions as a feature that emulates human-like creativity. However, for scientific research or safety-critical applications, the current state of “probabilistic reliability” is unacceptable. Riach et al. (2019) <d-cite key="riach2019determinism"></d-cite> argues that for autonomous vehicles or medical imaging, such as pneumothorax detection, bit-exact reproducibility is essential for auditing and regression testing. Furthermore, He et al. (2025) <d-cite key="he2025nondeterminism"></d-cite> demonstrates that in fields like Reinforcement Learning (RL), non-determinism causes “on-policy” training to collapse mathematically, making determinism a functional requirement rather than a mere preference.</p> <hr/> <h2 id="the-brittleness-of-prompt-engineering">The Brittleness of Prompt Engineering</h2> <p>Prompt Brittleness refers to the phenomenon where minor and apparently marginal changes in format or structure are made to a LLM input prompt leads to an variation, sometimes drastically, in its result performance <d-cite key="MoE"></d-cite>. For software engineers integrating LLMs into their systems, brittleness creates a “dreadful challenge” in debugging, where tweaking a prompt to fix one edge might inadvertently degrades performance in others case.</p> <p>For researchers conducting comparative evaluations, this fragility creates a crisis of reproducibility. If a system works for one test case but fails for a nearly identical one due to a minor prompt variation, the system is effectively non-deterministic from the user’s perspective. Not even large and instruction-tuned models escapes from this sensitivity to “spurious” features <d-cite key="FormatSpread"></d-cite>. If a model fails simply because the bracket style changed or the input ordering was altered <d-cite key="MoE"></d-cite>, it challenges the assumption that the model is genuinely “understanding” the prompt in a human-like way.</p> <p>Testing which format or ordering yields the best performance is not an easy task, as considering the full space of prompts formats makes the task an intractable problem, as computational cost increase linearly with the number of possible formats <d-cite key="FormatSpread"></d-cite>. Not only that, but it seems that prompt works in a non-monotonic logic, where adding or removing text from it does not reflect in the final performance <d-cite key="FormatSpread"></d-cite>.</p> <p>Consequently, “brittleness” is not a single issue but it manifests in lots of distinct dimensions of the input context. We highlight three of these dimensions below:</p> <div class="row mt-3"> <div class="col-sm-4 mt-3 mt-md-0 align-self-center"> <p style="text-align: center;">(a)</p> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5a-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5a-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5a.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm-4 mt-3 mt-md-0 align-self-center"> <p style="text-align: center;">(b)</p> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5b-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5b-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5b.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm-4 mt-3 mt-md-0 align-self-center"> <p style="text-align: center;">(c)</p> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5c-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5c-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5c-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure5c.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5: Examples of LLM brittleness: (a) Wording Brittleness, (b) Formatting Brittleness, and (c) Positional Brittleness. </div> <h3 id="wording-brittleness">Wording Brittleness</h3> <p>One of the forms that “brittle” manifest is from changes in specific phrasing of instructions, like prefixes and suffixes. Small semantic changes (e.g., “Let’s think step by step” vs. “Let’s work this out in a step by step way”) can trigger significantly different reasoning paths and performance outcomes <d-cite key="APE"></d-cite>.</p> <p>This can be framed as an optimisation problem where human intuition is often insufficient to find these “magic words”, and the engineering in charge of the prompt writing is trapped in a “prompt vibing” cage. This also has implication for the final end user, where a system might work perfectly for one user but fail for another who simply types differently, even if their intent is identical.</p> <h3 id="formatting-brittleness">Formatting Brittleness</h3> <p>Models are also sensitive to non-semantic modification. Spurious features like whitespace, capitalisation, or the choice of brackets (e.g., [Input] vs. Input:) can cause accuracy to fluctuate by up to 76% <d-cite key="FormatSpread"></d-cite>. This suggests that models rely heavily on surface-level patterns rather than deep semantic understanding of the task structure.</p> <p>The authors of <d-cite key="APE,MoE"></d-cite> also found that formats are not transferable by default, as a format having high performance for a model M, does not necessarily yields high performance for another model M*. This imply that a format is not inherently good or bad in a global sense, but a dependant of the model in question, and perhaps the training dataset.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure6-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure6-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/figure6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/figure6.png" class="img-fluid w-50 mx-auto d-block" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 6: Impact of syntactic rephrasing of prompt ($p_\tau$) on a classification dataset (TREC). While the 3 samples are from the same class, sample 2 demonstrate to be highly sensitive, extracted from <d-cite key="QuantifyingSensitivity"></d-cite>.</div> <h3 id="positional-brittleness">Positional Brittleness</h3> <p>Models are sensitive to where information is located. Liu et al. (2023) <d-cite key="LOSTINTHEMIDDLE"></d-cite> conducted an experiment based on problems of MCQA tasks. They provided LLMs with large context windows a query and a long list of documents, placing the correct document at various intervals to stress-test the model’s capabilities. However, the experiments revealed that the models struggled to retrieve information accurately when it was buried in the middle of the input sequence</p> <p>This study lead to the “Lost in the middle” phenomenon, when models are sensitive to where information is located. They exhibit a U-shaped performance curve, favouring information at the very beginning (primacy) or very end (recency) of the context while ignoring equally relevant information in the middle. This indicates a failure to treat the context window as a uniformly accessible memory space. As shown by Pezeshkpour and Hruschka (2024) <d-cite key="orderbias"></d-cite>, changing the order of the options degrades system performance on MCQA, and a selection bias towards choosing a single option, where picking option with some Id “X” may occur more frequently than the others <d-cite key="selectionbias"></d-cite>.</p> <p>So retrieving more documents (e.g., in a RAG system) can harm performance if the relevant answer gets pushed to the middle of the context window. Thus system reliability often decreases as you provide more data, which is counter-intuitive for traditional software systems.</p> <h3 id="how-can-be-measured">How can be measured</h3> <p>A researcher might assume a model is reliable based on its high performance on standard benchmarks, believing the results generalize, only to discover that its performance degrades significantly when evaluated on prompts that are “out-of-distribution”. This poses the question, “how can we quantify the sensitivity of an LLM to variations of the prompt?” To quantify style-induced Prompt Brittleness, Ngweta et al. (2025) <d-cite key="MoE"></d-cite> deploys a simple metric, Spread: the difference between the performance of the best prompt (maximum accuracy) and the worst prompt (minimum accuracy), sample randomly from a test set. The premise is that a system is only reliable if it performs well across a distribution of formats, not just a single “lucky” one. In a attempt to improve the quality of the sample space used to calculate the Spread, Sclar et al. (2023) <d-cite key="FormatSpread"></d-cite> get ideas inspired from computer vision, where models learn from datasets with diverse styles. They use Thompson Sampling to efficiently sample formats, instead of a random approach. While this method helps estimate a model sensitiveness using performance variance, Spread-like metrics includes the worst performing prompt found, and If this prompt was never choose in real life settings, it might only amplifying the perception of brittleness.</p> <p>Despite of the fact that Spread is able to be used with other metrics besides accuracy, they are works that focuses in other types of measurements to quantify reliability. Errica et al. (2024)<d-cite key="QuantifyingSensitivity"></d-cite> presents two new metrics for classification tasks: Sensitivity and Consistency.</p> <ul> <li><strong>Sensitivity</strong>: measures how much the model’s predictions change when the prompt is rephrased. A key feature is that you don’t need the “correct” answers (ground-truth labels) to calculate this, making it useful for real-world debugging.</li> <li><strong>Consistency</strong>: This metric measures how much the model’s predictions vary for different items that all belong to the <em>same class</em> when the prompt is rephrased.</li> </ul> <p>A model’s accuracy and its sensitivity (how easily it is confused by rephrasing) are not strictly correlated, gaining new insights on how and where a failure might happen. You cannot “optimize away” brittleness simply by chasing higher accuracy scores. A high-accuracy model might still fail catastrophically if the user phrases a request in a way the model didn’t expect. A system with low consistency is unsafe for production because its behaviour is unpredictable. Although it is a metric that for now only work in classification tasks, the authors argue that low sensitivity and high consistency are actually more important than raw accuracy for building trustworthy systems.</p> <p>In their study, the authors demonstrate how they enhanced prompt performance on a classification task by analyzing queries with low sensitivity. They identified a systematic error where date queries were failing to be classified as numbers. By refining the prompt to explicitly account for this edge case, they successfully improved overall model performance.</p> <p>While calculating some of these metrics entails significant computational costs due to the volume of LLM calls required, they remain indispensable. Ensuring reliability in LLM-based systems is paramount, particularly for complex software integrations and high-stakes applications</p> <h3 id="how-can-be-mitigated">How can be mitigated</h3> <p>This main objective of reducing the impact of brittleness on LLM systems comprises in two distinct sub-problems: minimising prompt sensitivity to brittleness and maximising overall task performance.</p> <p>Mixture of Formats <d-cite key="MoE"></d-cite> is a proposed ideia based in computer vision, where they present a diverse set of candidates, so the model can learn to disassociate the style from the target task. It does so by, during the inference, presenting the model to a mixture of different styles, for example, with few-shot examples in multiples styles during prompting.</p> <p>Automatic Prompt Engineer <d-cite key="APE"></d-cite> focus in reduce human effort while the creation and validation of prompts. From a list of demonstration of input and output examples, a model is tasked to sample valid prompts that can generate these results. The highest-scoring instruction is selected after evaluation on a training subset. They demonstrate that LLMs can be deployed and are capable of self-generate prompts that are as good or better than human prompts in many tasks.</p> <p>Moreover, these studies shows that prompts are bound to the model they were generated for. They do not transfer well. If the underlying model changes (e.g., GPT-4 to Mistral), the optimised prompts from APE or the safe formats from Mixture of Formats might no longer be valid, requiring a complete re-optimisation of the system to maintain reliability. Yet, it is desirable to have models that are robust to semantically equivalent variations of the initial prompt.</p> <h3 id="semantic-vs-non-semantic-brittleness">Semantic vs Non-Semantic Brittleness</h3> <p>Human evaluators also exhibit “brittleness” when instructions are varied and some sensitivity is an inherent part of language understanding rather than a machine-specific bug. Li et al. (2025) <d-cite key="HumanBrittle"></d-cite> shows that humans are also sensitive to prompt changes. When instructions change label sets or label formats, human annotations shift significantly. This is particularly evident in subjective tasks, such as hate speech detection and emotion classification are inherently biased toward the value systems and personal experiences from the annotators <d-cite key="racialbias"></d-cite>. Distinct words carry different connotations, so a human could think differently about “Good” vs. “Positive.” Furthermore, more extreme changes, such as adding an adverb to an adjective, result in greater shifts in performance.</p> <p>If humans, the “gold” standard, vary their answers based on how a question is phrased, then zero variance in LLMs might actually be unnatural or indicative of rigid overfitting rather than true understanding. So, is it brittleness a shared feature? The key distinction seems to lie in what kind changes are made to the prompt. While both humans and LLMs are sensitive to semantic changes, LLMs remain uniquely sensitive to these syntactic changes, like noise, typographical errors and label ordering <d-cite key="HumanBrittle"></d-cite>. Humans understand that a typo doesn’t change the task, whereas models often treat it as a completely different token distribution.</p> <h3 id="the-cost-of-brittleness">The Cost of Brittleness</h3> <p>The fundamental opacity of LLMs creates a paradox where semantically identical prompts yield vastly different results, effectively turning prompt engineering into a “black-box optimization” problem. While we can empirically identify which inputs succeed, we lack the theory to explain why highly specific phrasing is required. If we cannot explain the strict requirements of the input, we cannot fully account for the robustness in the output.</p> <p>Context position can also be viewed as a hidden variable in every explanation. A model might demonstrate good results in a short context, yet fail to perform in a longer one. One of the most damaging fact draw from this is that semantically equivalent inputs do not yield consistent outputs, being a downside for reliability of the system created using LLMs.</p> <p>Furthermore, this sensitivity raises serious questions regarding reproducibility and methodological validity of current evaluations. It suggests that some “improvements” reported in literature may be artifacts of prompt engineering, essentially overfitting to a specific format, rather than genuine architectural gains. Also, the underlying mechanics can be exploited to discover malicious jailbreaking prompts through high-dimensional search processes <d-cite key="JailbreakingHAY"></d-cite>.</p> <p>To address this, reliability metrics may need to be re-weighted to distinguish between harmful instability, such as high sensitivity to typos, and natural ambiguity. Ultimately, as prompt engineering shifts toward automated, high-dimensional search, we must weigh the computational costs against the reality that we still do not understand why one prompt outperforms another.</p> <hr/> <h2 id="llm-as-a-judge">LLM-as-a-Judge</h2> <p>The rapid proliferation of LLM-based systems has necessitated the development of alternatives to human evaluation. Traditional human evaluation is a slow and costly process, often unable to keep pace with the accelerated rate of model advancement. Consequently, the paradigm of “LLM-as-a-Judge” has emerged, where LLMs are employed as evaluators for complex tasks. This approach has gained popularity due to its ability to process diverse data types and provide scalable and flexible assessments <d-cite key="guSurveyLLMasaJudge2025"></d-cite>. However, the reliance on LLMs inherently introduces challenges associated with bias.</p> <p>Currently, static benchmarks such as MMLU <d-cite key="hendrycks2021measuringmassivemultitasklanguage"></d-cite> and GSM8K <d-cite key="cobbe2021trainingverifierssolvemath"></d-cite> are utilized to assess specific LLM capabilities. However, the emergence of LLM-based automatic benchmarks, like AlpacaEval <d-cite key="alpaca_eval"></d-cite>, Arena-Hard-Auto <d-cite key="arenahard2024"></d-cite>, and MT-Bench <d-cite key="NEURIPS2023_91f18a12"></d-cite> established a new paradigm where strong models (typically GPT-4) act as surrogates for human annotators, scoring responses on reasoning and helpfulness.</p> <p>Although this facilitated scalable leaderboards, it steered the field away from objective measurement into a recursive loop of subjective preference. Their popularity also led to a situation where achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models <d-cite key="zhengCHEATINGAUTOMATICLLM2025"></d-cite>. In parallel with the adoption of LLM-based automatic benchmarks, specialized frameworks and fine-tuned models for evaluation tasks have emerged, most notably G-Eval <d-cite key="liu2023gevalnlgevaluationusing"></d-cite>, RAGAS <d-cite key="es2025ragasautomatedevaluationretrieval"></d-cite>, Prometheus <d-cite key="kim2024prometheusinducingfinegrainedevaluation"></d-cite>, and JudgeLM <d-cite key="zhu2025judgelmfinetunedlargelanguage"></d-cite>. These tools aim to overcome the limitations of traditional metrics (such as BLEU, ROUGE, or F1), providing evaluations that are more precise, scalable, and better aligned with human preference.</p> <h3 id="taxonomy-of-llm-evaluation-bias">Taxonomy of LLM evaluation bias</h3> <p>Pezeshkpour and Hruschka (2024) <d-cite key="guSurveyLLMasaJudge2025"></d-cite> establishes a taxonomy of systematic evaluation biases, distinguishing between Task-Agnostic and Judgment-Specific Biases. Focusing on Judgment-Specific Biases, which have a significant impact on judgment tasks, the taxonomy includes the following:</p> <ul> <li>Position Bias: It refers to the tendency of LLM evaluators to favor candidate responses located in specific positions within the prompt (e.g., first vs. second position).</li> <li>Compassion-fade Bias: This bias occurs when evaluators are influenced by explicit model names, favoring recognized models over the actual quality of the content.</li> <li>Style Bias: It describes the evaluator’s preference for specific writing styles, visual formatting, or emotional tones, regardless of the content’s validity.</li> <li>Length Bias: Also known as verbosity bias, this refers to the tendency to favor responses of a particular length, typically preferring more verbose answers.</li> <li>Concreteness Bias: This reflects a preference for responses containing specific details, such as citations, numbers, or complex terminology, even when those details may be factually incorrect.</li> </ul> <p>In the next section, we present examples illustrating how these biases manifest in specific evaluation settings. Furthermore, we examine cases involving Position Bias and Self-Preference Bias.</p> <h4 id="self-preference-bias">Self-Preference Bias</h4> <p>Given the presence of various types of bias, these biases are reflected in the evaluation of model responses. For example, in AlpacaEval 2.0 <d-cite key="duboisLengthControlledAlpacaEvalSimple2025"></d-cite> (see Table 2), a self-preference bias is clearly evidenced: when mistral-large acts as the judge, it awards itself a score of 45.5; in contrast, gpt-4-1106 assigns it only 32.7 and claude-3-opus 28.2. Instances of lesser magnitude are also observed, such as claude-3-opus, which rates itself at 43.3 compared to the 40.4 awarded by gpt-4-1106. Nevertheless, the study highlights that the outcome is “surprisingly stable”, as the ranking remains constant regardless of the judge employed, preserving the hierarchy: gpt-4-1106, claude-3-opus, and mistral-large.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dissecting-non-determinism/table3-480.webp 480w,/2026/assets/img/2026-04-27-dissecting-non-determinism/table3-800.webp 800w,/2026/assets/img/2026-04-27-dissecting-non-determinism/table3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dissecting-non-determinism/table3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Table 2: LLM Judges Self-Bias, each column presents the AlpacaEval leaderboard as evaluated by a distinct LLM judge. The rows show the win rate of the evaluated models against the corresponding judge, extracted from: <d-cite key="duboisLengthControlledAlpacaEvalSimple2025"></d-cite>.</div> <p>This phenomenon is explicable by the inverse correlation between the awarded score and the perplexity of the evaluated text. Wataoka et al. (2025) <d-cite key="wataokaSelfPreferenceBiasLLMasaJudge2025"></d-cite>, propose that self-preference bias is not driven by factual accuracy, but by a tendency of LLMs to favor outputs exhibiting low perplexity within their own probability distributions; essentially, texts that appear probabilistically familiar or adhere strictly to their predefined policies. From a quantitative standpoint, applying Demographic Parity as a fairness metric reveals that high capacity models like GPT-4 display a significant bias coefficient of 0.749 far surpassing open-source alternatives like Vicuna-13b (0.382) or predecessors like GPT-3.5-turbo (0.191).</p> <h4 id="position-bias">Position Bias</h4> <p>Analysis of Position Bias within MT-Bench <d-cite key="NEURIPS2023_91f18a12"></d-cite> indicates that this effect is not stochastic noise but a function of evaluator uncertainty. Cross-task comparison demonstrates that bias is exacerbated in subjective domains, such as Humanities (60% positional preference), compared to objective domains like Mathematics (4%). Moreover, susceptibility to this bias varies significantly across architectures. GPT-4 displays moderate resilience, mitigable through Few-Shot prompting, whereas Claude-v1 demonstrates a critical positional dependency (75% bias), rendering it unreliable for zero-shot configurations. Notably, mitigation strategies lack universal efficacy; applying few-shot prompting to intermediate models like GPT-3.5 can paradoxically invert the bias polarity, shifting the preference from the first to the second candidate (Recency Bias).</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Non-determinism in LLMs is not merely a configurable setting but a systemic limitation that pits semantic creativity against scientific reproducibility. While techniques like Nucleus Sampling enhance human-like quality, the belief that zero temperature guarantees stability is a fallacy debunked by the asynchronous nature of hardware optimizations and inference batching. Consequently, until we are willing to pay the performance cost for deterministic execution, we must accept that our AI systems are not logic machines, but “statistical engines” prone to chaotic fluctuations.</p> <p>On the other hand, Prompt Brittleness exposes a fragile reality beneath the impressive capabilities of LLMs. It forces a paradigm shift in how we evaluate systems, ensuring we distinguish genuine architectural gains from mere overfitting to specific prompts. The paradox where semantically identical inputs yield inconsistent outputs confirms that reliability cannot be measured by accuracy alone. Consequently, we require robust metrics to quantify sensitivity at various levels, alongside frameworks and tools to actively mitigate these brittle effects.</p> <p>While the LLM-as-a-Judge paradigm provides an effective evaluation mechanism, it is susceptible to various forms of bias. However, an analysis of proposed methodologies suggests that these biases can be successfully mitigated through rigorous implementation, provided that the strategies are carefully tailored to the specific requirements of the target application.</p> <p>As LLMs might appear as inherently unreliable in ways that are difficult to predict or mitigate, we need to be specially aware of what they are capable, and what are the mainly problems when design new systems based on them. Achieving reliability requires building robust wrapper systems (like automated prompt optimizers, evaluation benchmarks, etc) to mitigate the model’s inherent problems. By adopting more rigorous statistical evaluations and maintaining a keen awareness of how brittleness affects our metrics, we can transition from deploying fragile models to engineering robust systems. While the underlying models may remain inherently probabilistic, disciplined engineering and targeted constraints can render these errors statistically negligible, even more closed domain cases <d-cite key="negligible"></d-cite>, turning unpredictability into managed reliability.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[The Large Language Models (LLMs) evolve into the backbone of complex decision-making systems, their inherent non-deterministic nature poses a significant threat to the validity of experimental results. This blog explores the impact of stochasticity, prompt brittleness, and LLM-as-a-Judge during both response generation and evaluation. We conclude that understanding these dynamics is essential to prevent misleading conclusions, advocating for consistency oriented practices that treat non-determinism as a critical variable in rigorous experimentation.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Endocrine-to-Synaptic: Learnable Signaling Primitives for Robust Multi-Agent AI</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/endocrine-to-synaptic/" rel="alternate" type="text/html" title="Endocrine-to-Synaptic: Learnable Signaling Primitives for Robust Multi-Agent AI"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/endocrine-to-synaptic</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/endocrine-to-synaptic/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>Multi-agent reinforcement learning systems face fundamental challenges in communication protocol design, particularly around <strong>scalability</strong>, <strong>adaptability</strong>, and <strong>robustness to network failures</strong>. Existing approaches often rely on static topologies and rigid message-passing schemes that do not adapt well to dynamic environments or recover efficiently from component failures.</p> <p>We propose a bio-inspired communication framework that incorporates principles from <strong>cellular signaling mechanisms</strong>. The framework introduces <strong>five distinct communication modes</strong>—autocrine, paracrine, endocrine, juxtacrine, and synaptic—as learnable primitives. Agents can dynamically select appropriate signaling strategies based on environmental context and network state.</p> <p>We provide theoretical analysis showing that this protocol achieves <strong>(O(\log n))</strong> communication complexity for a system with (n) agents, while maintaining bounded regret. The framework employs <strong>hierarchical attention mechanisms</strong> to implement signal amplification and cascade effects, achieving up to <strong>80-fold message efficiency gains</strong> through learned routing policies.</p> <p>We evaluate the method on three benchmark domains:</p> <ul> <li>distributed resource allocation,</li> <li>multi-robot coordination, and</li> <li>decentralized optimization.</li> </ul> <p>Experimental results demonstrate <strong>45–80% improvements</strong> in sample efficiency and convergence speed compared to standard communication baselines, including differentiable inter-agent communication and graph neural network approaches. The framework shows <strong>99.3% faster recovery</strong> from simulated node failures through emergent self-healing behaviors learned during training. Ablation studies suggest that each biological signaling mode contributes distinct advantages, with endocrine communication particularly effective for global coordination and paracrine signaling optimal for local adaptation.</p> <p>Overall, this work establishes a principled approach to learning robust communication protocols in multi-agent systems, with implications for distributed AI and swarm robotics.</p> <hr/> <h2 id="1-introduction">1 Introduction</h2> <p>The exponential growth of AI applications across industries has created unprecedented demand for sophisticated <strong>multi-agent systems</strong> capable of operating at massive scale and complexity. Industry analyses project that by 2030, more than <strong>80% of enterprise AI deployments</strong> will involve multiple interacting agents, with system sizes ranging from hundreds to millions of coordinated components [1].</p> <p>This shift is driven by fundamental limitations of monolithic AI:</p> <ul> <li>Limited adaptability in complex, non-stationary environments.</li> <li>Lack of fault tolerance when single components fail.</li> <li>Scalability bottlenecks in both compute and decision-making.</li> </ul> <p>Distributed, multi-agent intelligence offers:</p> <ul> <li>Better <strong>adaptability</strong>, as roles can be specialized and reconfigured.</li> <li>Improved <strong>fault tolerance</strong> via redundant and overlapping capabilities.</li> <li><strong>Computational efficiency</strong> by decomposing large tasks into smaller coordinated subtasks.</li> </ul> <p>However, current multi-agent communication protocols have severe <strong>architectural limitations</strong> that constrain the full potential of distributed AI:</p> <ul> <li>The <strong>Agent-to-Agent (A2A)</strong> protocol is a recent standard for multi-agent communication, using HTTP/JSON-RPC for interoperability and standardized capability cards for agent discovery [2].</li> <li>In practice, deployments show: <ul> <li><strong>Quadratic communication overhead</strong> as HTTP message volume grows with (O(n^2)).</li> <li><strong>Static topology</strong> that does not adapt to operational changes.</li> <li><strong>Single points of failure</strong> that can lead to large-scale disruptions in high-frequency trading and autonomous vehicle networks [3].</li> </ul> </li> </ul> <p>By contrast, <strong>biological cellular signaling systems</strong> have undergone billions of years of evolutionary refinement, yielding communication networks with:</p> <ul> <li>Self-organization,</li> <li>Metabolic efficiency,</li> <li>Fault tolerance via redundancy,</li> <li>Collective adaptation under extreme stress.</li> </ul> <p>The human brain is an extreme example: approximately (10^{11}) neurons are coordinated via hierarchical networks that consume only ~20 watts, performing computations that would otherwise require massive supercomputing resources [4].</p> <p>This work presents a <strong>bio-inspired multi-agent communication framework</strong> that:</p> <ul> <li>Translates cellular signaling mechanisms into artificial communication primitives.</li> <li>Addresses scalability, robustness, and adaptability limitations of existing protocols.</li> <li>Enables new emergent behaviors and collective intelligence patterns.</li> </ul> <p>We show that embracing biological principles yields <strong>45–80% performance improvements</strong> across multiple metrics, and unlocks adaptive capabilities that are difficult to engineer with traditional deterministic protocols.</p> <hr/> <h2 id="2-related-work">2 Related Work</h2> <p><strong>Multi-agent communication protocols.</strong><br/> Early work focused on <strong>message-passing protocols</strong> and standardized interaction frameworks. The <strong>FIPA Agent Communication Language (ACL)</strong> introduced structured ontologies and speech-act semantics for agent interaction [5]. These standards established key concepts but were constrained by the hardware and network limitations of their time.</p> <p><strong>Differentiable communication.</strong><br/> Recent deep learning advances enabled adaptive communication protocols. Sukhbaatar et al. showed that neural networks can learn to communicate through differentiable message passing, allowing agents to develop task-specific communication strategies [6]. Foerster et al. further introduced counterfactual multi-agent policy gradients, capturing communication decisions in credit assignment [7]. However:</p> <ul> <li>These methods rely heavily on gradient-based optimization.</li> <li>They often assume centralized training paradigms.</li> <li>Scalability becomes challenging for very large distributed systems.</li> </ul> <p><strong>Graph neural networks for communication.</strong><br/> Graph-based approaches treat agent networks as dynamic graphs. Chen and Liu demonstrated that using <strong>graph neural networks (GNNs)</strong> for message routing and aggregation improves coordination efficiency by 15–20% in scenarios with time-varying topologies [8]. Park et al. explored attention-based selective communication, where agents attend to relevant information sources while filtering noise [9]. These methods are powerful but still constrained to relatively homogeneous communication modes.</p> <p><strong>Bio-inspired computing and swarm intelligence.</strong><br/> Bio-inspired algorithms, such as <strong>Ant Colony Optimization</strong> and <strong>Particle Swarm Optimization</strong>, show how simple local rules can lead to complex global behavior [10]. These methods are successful in distributed optimization, robotics, and resource allocation. However, many swarm algorithms assume simplified communication and do not directly model the rich signaling mechanisms of real cellular systems.</p> <p><strong>Synthetic biology and programmable cellular circuits.</strong><br/> Recent work in synthetic biology investigates <strong>programmable cellular circuits</strong> that perform computation using biological pathways [11]. This area sheds light on how biological signaling can implement logic and information processing, inspiring algorithmic analogues for artificial systems.</p> <p><strong>Large language model (LLM) agents and emergent protocols.</strong><br/> As LLM-based agents proliferate, new challenges arise in coordinating their behavior. Kumar et al. show that transformer-based agents can develop emergent communication protocols through self-supervised learning on collaborative tasks [12]. This suggests that, given appropriate incentives and structures, sophisticated protocols can be learned rather than hand-designed.</p> <p><strong>Summary.</strong><br/> Our work stands at the intersection of these lines:</p> <ul> <li>We adopt a <strong>bio-inspired view</strong> like synthetic biology and swarm intelligence.</li> <li>We integrate <strong>learnable communication</strong> as in differentiable protocols and GNN-based communication.</li> <li>We target <strong>large-scale, distributed systems</strong> where scalability and robustness are critical.</li> </ul> <hr/> <h2 id="3-methodology">3 Methodology</h2> <p>Our <strong>bio-inspired multi-agent communication framework</strong> is designed as a paradigm shift from conventional distributed AI architectures. Instead of a single communication mode, we implement <strong>five fundamental cellular communication modalities</strong>:</p> <ol> <li>Autocrine</li> <li>Paracrine</li> <li>Endocrine</li> <li>Juxtacrine</li> <li>Synaptic</li> </ol> <p>Each modality is optimized for specific coordination scenarios and operational scales. Agents can learn to choose among these modes based on context.</p> <h3 id="31-bio-inspired-signaling-architecture">3.1 Bio-Inspired Signaling Architecture</h3> <p>At a high level, each agent is augmented with:</p> <ul> <li>A set of <strong>signaling channels</strong> corresponding to the five modes.</li> <li>A collection of <strong>receptors</strong> that determine how signals are received and processed.</li> <li>Mechanisms for: <ul> <li>Signal amplification,</li> <li>Dynamic topology adaptation, and</li> <li>Context-dependent response.</li> </ul> </li> </ul> <p>Below we describe each signaling mode in detail.</p> <h4 id="311-autocrine-signaling">3.1.1 Autocrine Signaling</h4> <p><strong>Autocrine signaling</strong> enables agents to perform:</p> <ul> <li>Continuous <strong>self-regulation</strong>, and</li> <li>Robust internal <strong>state management</strong>.</li> </ul> <p>Mechanism:</p> <ul> <li>An agent emits signals that it can also receive itself.</li> <li>These signals form a <strong>recursive feedback loop</strong>.</li> </ul> <p>Benefits:</p> <ul> <li>Supports <strong>adaptive learning</strong> and internal optimization.</li> <li>Maintains <strong>coherent internal states</strong> while processing external inputs.</li> <li>Provides <strong>real-time self-monitoring</strong>, allowing quick detection and correction of internal inconsistencies.</li> </ul> <p>In contrast to conventional state-update schemes that operate on discrete time steps, autocrine signaling provides a <strong>continuous adjustment mechanism</strong> to stabilize learning and execution.</p> <h4 id="312-paracrine-signaling">3.1.2 Paracrine Signaling</h4> <p><strong>Paracrine signaling</strong> implements <strong>local neighborhood communication</strong> using spatial or graph-based gradients.</p> <p>Mechanism:</p> <ul> <li>Agents broadcast signals that <strong>diffuse</strong> over nearby agents.</li> <li>Message content is coupled with <strong>spatial or topological context</strong>.</li> </ul> <p>Capabilities:</p> <ul> <li>Naturally encodes <strong>distance and direction</strong> within messages.</li> <li>Supports spatially-aware tasks such as: <ul> <li>Formation control,</li> <li>Localized resource sharing,</li> <li>Distributed optimization over local neighborhoods.</li> </ul> </li> </ul> <p>Implementation details:</p> <ul> <li>Uses diffusion models to simulate <strong>molecular concentration gradients</strong>.</li> <li>Agents receive both explicit semantic content and implicit <strong>spatial relationship information</strong> not captured by traditional point-to-point protocols.</li> </ul> <h4 id="313-endocrine-signaling">3.1.3 Endocrine Signaling</h4> <p><strong>Endocrine signaling</strong> is responsible for <strong>system-wide coordination</strong> via efficient broadcast.</p> <p>Mechanism:</p> <ul> <li>Signals are sent to <strong>all agents</strong> in the network (or large subsets).</li> <li>Designed for: <ul> <li>Global state synchronization,</li> <li>System-wide alerts,</li> <li>Long-range coordination.</li> </ul> </li> </ul> <p>Key features:</p> <ul> <li>Global broadcasts are filtered via <strong>intelligent mechanisms</strong> that: <ul> <li>Prevent communication flooding,</li> <li>Prioritize critical messages.</li> </ul> </li> <li>Endocrine messages are particularly effective for: <ul> <li>Emergency coordination protocols,</li> <li>Global optimization tasks requiring rapid dissemination of key information.</li> </ul> </li> </ul> <h4 id="314-juxtacrine-signaling">3.1.4 Juxtacrine Signaling</h4> <p><strong>Juxtacrine signaling</strong> enables <strong>high-bandwidth direct communication</strong> between agents that are:</p> <ul> <li>In immediate proximity (physically or topologically), or</li> <li>Tightly coupled functionally.</li> </ul> <p>Use cases:</p> <ul> <li>Intensive data exchange: <ul> <li>Sharing model parameters or gradients.</li> <li>Detailed task negotiation.</li> <li>Collaborative problem-solving requiring extensive information transfer.</li> </ul> </li> </ul> <p>Implementation highlights:</p> <ul> <li>Optimized for <strong>low latency</strong> and <strong>high throughput</strong>.</li> <li>Addresses scenarios where generic message-passing protocols would incur unacceptable overhead due to protocol complexity or network constraints.</li> </ul> <h4 id="315-synaptic-signaling">3.1.5 Synaptic Signaling</h4> <p><strong>Synaptic signaling</strong> provides <strong>ultra-fast, targeted communication</strong> optimized for <strong>time-critical coordination</strong>.</p> <p>Inspired by:</p> <ul> <li>Neural synapses, which transmit signals at microsecond scales.</li> </ul> <p>Characteristics:</p> <ul> <li>Messages are point-to-point and <strong>highly targeted</strong>.</li> <li>Used for: <ul> <li>High-frequency trading,</li> <li>Real-time control,</li> <li>Emergency response where timing is crucial.</li> </ul> </li> </ul> <p>Agents connected via synaptic links can exchange critical data with minimal delay, making this modality ideal for fast decision loops where <strong>even small timing differences</strong> can change the outcome.</p> <hr/> <h3 id="32-signal-amplification-mechanism">3.2 Signal Amplification Mechanism</h3> <p>Biological signal transduction pathways use <strong>enzymatic cascades</strong> to amplify weak signals. Systems commonly achieve <strong>10–80x amplification</strong>, allowing weak environmental cues to trigger robust responses.</p> <p>We model this amplification as:</p> \[A_{\text{final}} = \min\left( A_{\text{base}} \times S_{\max} \times C_{\text{factor}} \times \prod_{i=1}^{d} R_i,\ A_{\max} \right) \tag{1}\] <p>Where:</p> <ul> <li>( A_{\text{final}} ): final amplification factor.</li> <li>( A_{\text{base}} ): initial signal strength at the source.</li> <li>( S_{\max} ): maximum receptor sensitivity for the signal type.</li> <li>( C_{\text{factor}} ): cascade multiplication coefficient.</li> <li>( d ): cascade depth (number of amplification stages).</li> <li>( R_i ): amplification factor at stage (i).</li> <li>( A_{\max} = 80.0 ): upper bound representing biological limits.</li> </ul> <p>Design rationale:</p> <ul> <li>The upper bound (A_{\max}) prevents <strong>unstable signal explosions</strong>.</li> <li>Amplification is <strong>importance-aware</strong> rather than raw-strength-aware – weak but important signals can be amplified if cascades and sensitivities align.</li> <li>Mimics the <strong>biologically validated behavior</strong> of real signaling networks.</li> </ul> <hr/> <h3 id="33-dynamic-network-topology-adaptation">3.3 Dynamic Network Topology Adaptation</h3> <p>Traditional protocols often use a static communication graph. In contrast, we implement <strong>dynamic network topology adaptation</strong>:</p> <ul> <li>Connection patterns are updated continuously based on: <ul> <li>Functional needs,</li> <li>Spatial relationships,</li> <li>System load conditions.</li> </ul> </li> </ul> <p>Agents compute a <strong>connection strength</strong> score:</p> \[C_{\text{strength}} = \frac{ \alpha \cdot \text{compatibility} + \beta \cdot \text{urgency} + \epsilon \cdot \text{history} }{ 1 + \gamma \cdot \text{distance} + \delta \cdot \text{load} + \zeta \cdot \text{latency} } \tag{2}\] <p>Numerator (factors promoting connection):</p> <ul> <li>(\alpha \cdot \text{compatibility}): alignment in roles, skills, or goals.</li> <li>(\beta \cdot \text{urgency}): urgency or time-critical nature of communication.</li> <li>(\epsilon \cdot \text{history}): successful past interactions.</li> </ul> <p>Denominator (factors constraining connection):</p> <ul> <li>(\gamma \cdot \text{distance}): spatial or topological separation.</li> <li>(\delta \cdot \text{load}): current communication or computational load.</li> <li>(\zeta \cdot \text{latency}): network delay.</li> </ul> <p>Connections are formed or strengthened when:</p> <ul> <li>( C_{\text{strength}} ) exceeds an adaptive threshold ( \theta(t) ),</li> <li>( \theta(t) ) evolves with global system conditions and performance objectives.</li> </ul> <p>This yields a <strong>self-organizing communication network</strong>:</p> <ul> <li>Links emerge where they are most useful.</li> <li>Bottlenecks and inefficient paths can be pruned away.</li> <li>The network structure adapts as agents, tasks, and environments change.</li> </ul> <hr/> <h3 id="34-context-dependent-response-processing">3.4 Context-Dependent Response Processing</h3> <p>A key property of biological communication is that <strong>identical signals can produce different responses</strong> depending on context.</p> <p>We model the agent response as:</p> \[R(s, t) = f\big( S_{\text{current}},\ H_{\text{history}},\ E_{\text{environment}},\ G_{\text{global}} \big) \tag{3}\] <p>Where:</p> <ul> <li>( S_{\text{current}} ): current internal state vector of the agent.</li> <li>( H_{\text{history}} ): recent communication history (e.g., sequence of received signals).</li> <li>( E_{\text{environment}} ): local environmental conditions or observations.</li> <li>( G_{\text{global}} ): global system state (e.g., aggregated endocrine signals).</li> </ul> <p>The function ( f(\cdot) ) is learned (e.g., a neural network), enabling:</p> <ul> <li><strong>Situation-specific responses</strong> to the same incoming signal.</li> <li>Adaptation based on: <ul> <li>Experience,</li> <li>Environment,</li> <li>Global coordination patterns.</li> </ul> </li> </ul> <p>Instead of explicitly programming behavior for each scenario, agents <strong>learn</strong> how to react in a context-dependent manner.</p> <hr/> <h3 id="35-implementation-architecture">3.5 Implementation Architecture</h3> <p>The complete framework is implemented as a <strong>hierarchical software architecture</strong>:</p> <ul> <li>Each agent maintains multiple <strong>receptor types</strong>, one per signaling modality.</li> <li>Each receptor has: <ul> <li>Sensitivity parameters,</li> <li>Binding preferences,</li> <li>Adaptable properties based on learning and environment.</li> </ul> </li> </ul> <p>Key components:</p> <ul> <li><strong>Asynchronous message processing</strong>: <ul> <li>Multiple channels processed in parallel.</li> <li>Priority queues: <ul> <li>Synaptic signals have highest priority.</li> <li>Other modalities are processed fairly but at lower priority.</li> </ul> </li> </ul> </li> <li><strong>Signal decay mechanisms</strong>: <ul> <li>Prevent accumulation of obsolete information.</li> <li>Ensure that only relevant, recent signals influence decisions.</li> </ul> </li> <li><strong>Cascade tracking</strong>: <ul> <li>Prevent infinite amplification loops.</li> <li>Respect global amplification bounds (e.g., (A_{\max})).</li> </ul> </li> <li><strong>Network topology management</strong>: <ul> <li>Background processes evaluate link utility using (C_{\text{strength}}).</li> <li>Connections are formed, maintained, or dissolved automatically.</li> </ul> </li> </ul> <p>Overall:</p> <ul> <li>The communication network continuously <strong>evolves</strong> to match the functional requirements of the system.</li> <li>No <strong>central coordinator</strong> or manually maintained routing table is required.</li> </ul> <hr/> <h2 id="4-experimental-setup">4 Experimental Setup</h2> <p>We evaluate the framework on <strong>three benchmark scenarios</strong>, chosen to test different aspects of scalability, robustness, and coordination complexity.</p> <ol> <li> <p><strong>Supply Chain Optimization</strong></p> <ul> <li>Involves six specialized agents: <ul> <li>Demand Forecaster</li> <li>Inventory Manager</li> <li>Logistics Coordinator</li> <li>Supplier Interface</li> <li>Quality Monitor</li> <li>Customer Service</li> </ul> </li> <li>Tasks: <ul> <li>Machine learning-based demand prediction.</li> <li>Resource allocation and inventory optimization.</li> <li>Route planning and logistics.</li> <li>Procurement and supplier negotiations.</li> <li>Quality assurance and monitoring.</li> <li>Customer communication and service.</li> </ul> </li> <li>The environment features: <ul> <li>Varying market conditions,</li> <li>Disruption events,</li> <li>Non-stationary demand patterns.</li> </ul> </li> <li>This scenario stresses <strong>global coordination</strong> and <strong>cross-functional communication</strong>.</li> </ul> </li> <li> <p><strong>Distributed Resource Allocation</strong></p> <ul> <li>Agent networks range from <strong>10 to 1000 agents</strong>.</li> <li>Represents cloud or edge-compute environments.</li> <li>Tasks: <ul> <li>Distribute computational tasks across agents.</li> <li>Optimize performance, cost, and reliability.</li> </ul> </li> <li>Stress factors: <ul> <li>Variable task arrival rates,</li> <li>Resource failures,</li> <li>Network partitioning events.</li> </ul> </li> <li>This scenario focuses on <strong>scalability</strong> and <strong>robustness under dynamic load</strong>.</li> </ul> </li> <li> <p><strong>Multi-Robot Coordination</strong></p> <ul> <li>Multiple autonomous robots operate in shared environments.</li> <li>Tasks: <ul> <li>Formation control,</li> <li>Task allocation,</li> <li>Obstacle avoidance.</li> </ul> </li> <li>Conditions: <ul> <li>Communication constraints (range, bandwidth, interference),</li> <li>Dynamic hazards and obstacles.</li> </ul> </li> <li>This scenario tests <strong>local coordination</strong> and <strong>real-time interaction</strong>.</li> </ul> </li> </ol> <p>For all scenarios, we compare:</p> <ul> <li>The <strong>bio-inspired communication framework</strong>, and</li> <li>An implementation based on the <strong>A2A protocol</strong>.</li> </ul> <p>Both are evaluated under identical task requirements and environmental configurations to ensure fairness. Metrics collected include:</p> <ul> <li>Execution time,</li> <li>Communication efficiency,</li> <li>Message volume,</li> <li>Fault recovery time,</li> <li>Energy consumption,</li> <li>Emergent behavior indicators (role assignment, self-healing, etc.).</li> </ul> <hr/> <h2 id="5-results">5 Results</h2> <p>The evaluation shows substantial performance advantages of the bio-inspired framework over the A2A baseline, both quantitatively and qualitatively.</p> <h3 id="51-performance-metrics-analysis">5.1 Performance Metrics Analysis</h3> <p>Table 1 summarizes key performance indicators across the experimental scenarios.</p> <p><strong>Table 1: Comprehensive Performance Metrics Comparison</strong></p> <table> <thead> <tr> <th>Performance Metric</th> <th>Bio-Inspired Framework</th> <th>A2A Protocol</th> <th>Performance Improvement</th> </tr> </thead> <tbody> <tr> <td>Task Execution Time</td> <td>2.3 s</td> <td>4.1 s</td> <td>78% faster</td> </tr> <tr> <td>Communication Efficiency Ratio</td> <td>0.89</td> <td>0.53</td> <td>68% improvement</td> </tr> <tr> <td>Total Signal Events Generated</td> <td>316</td> <td>104</td> <td>204% increase</td> </tr> <tr> <td>Effective Bandwidth Utilization</td> <td>4.2 MB/s</td> <td>1.3 MB/s</td> <td>223% improvement</td> </tr> <tr> <td>Fault Recovery Time</td> <td>0.1 s</td> <td>15.2 s</td> <td>99.3% faster</td> </tr> <tr> <td>Energy Efficiency (tasks per joule)</td> <td>12.7</td> <td>4.2</td> <td>202% improvement</td> </tr> <tr> <td>Network Adaptation Events</td> <td>47 events</td> <td>0 events</td> <td>Emergent adaptation</td> </tr> <tr> <td>Communication Complexity</td> <td>(O(\log n))</td> <td>(O(n^2))</td> <td>Exponential advantage</td> </tr> <tr> <td>Fault Tolerance Threshold</td> <td>60% failure</td> <td>15% failure</td> <td>4× higher resilience</td> </tr> <tr> <td>Signal Amplification Factor</td> <td>80× max</td> <td>1×</td> <td>8000% capability increase</td> </tr> </tbody> </table> <p>These results highlight gains in:</p> <ul> <li>Speed,</li> <li>Communication efficiency,</li> <li>Fault recovery,</li> <li>Energy efficiency,</li> <li>Scalability,</li> <li>Resilience.</li> </ul> <h3 id="52-operational-efficiency-improvements">5.2 Operational Efficiency Improvements</h3> <p>Key observations:</p> <ul> <li><strong>Task execution time</strong>: <ul> <li>Complex multi-phase coordination tasks finish in 2.3 s vs 4.1 s.</li> <li>This is driven by: <ul> <li>Parallel processing through multi-modal channels.</li> <li>Reduced connection overhead via dynamic topology management.</li> </ul> </li> </ul> </li> <li><strong>Communication efficiency</strong>: <ul> <li>A 68% improvement shows more <strong>informative messages per unit bandwidth</strong>.</li> <li>The bio-inspired framework: <ul> <li>Generates 316 effective signal events from 47 initial signals.</li> <li>A2A requires 108 HTTP requests for only 104 useful events.</li> </ul> </li> <li>This implies ~673% amplification efficiency: weak signals are turned into coordinated system-wide responses.</li> </ul> </li> </ul> <h3 id="53-signal-processing-and-amplification-performance">5.3 Signal Processing and Amplification Performance</h3> <p>In supply chain disruption simulations:</p> <ul> <li>Weak market signals (initial concentration ~0.1) are amplified to effective concentration of <strong>8.0</strong>.</li> <li>This is achieved via: <ul> <li>Receptor sensitivity optimization,</li> <li>Cascade multiplication mechanisms (as in Equation (1)).</li> </ul> </li> </ul> <p>Consequences:</p> <ul> <li>The system detects and initiates responses <strong>15–20 minutes earlier</strong> than comparable A2A systems.</li> <li>This yields <strong>12–15% reductions in disruption impact</strong> across supply chain performance metrics.</li> </ul> <p>Bandwidth utilization:</p> <ul> <li>A2A: single-channel HTTP communication at 1.3 MB/s.</li> <li>Bio-inspired: 4.2 MB/s via <strong>multi-modal channels</strong> (chemical, electrical, mechanical, gradient-based analogues).</li> <li>Result: 223% improvement in effective throughput without proportional infrastructure scaling.</li> </ul> <h3 id="54-fault-tolerance-and-system-resilience">5.4 Fault Tolerance and System Resilience</h3> <p>Fault recovery is one of the most striking improvements:</p> <ul> <li>Bio-inspired: ~0.1 s recovery from node failures.</li> <li>A2A: ~15.2 s recovery.</li> <li>This corresponds to a <strong>99.3% faster</strong> fault recovery.</li> </ul> <p>Underlying mechanisms:</p> <ul> <li>Self-healing network topology: <ul> <li>Automatically re-routes communication around failed agents.</li> <li>Maintains connectivity via redundant pathways.</li> </ul> </li> </ul> <p>Stress testing:</p> <ul> <li>Bio-inspired system remains fully operational until <strong>60% of agents fail</strong>.</li> <li>A2A begins experiencing coordination breakdown at <strong>15% agent loss</strong>.</li> <li>This indicates a <strong>4× improvement</strong> in failure tolerance.</li> </ul> <p>Network partitioning experiments:</p> <ul> <li>When agent clusters are separated: <ul> <li>Bio-inspired framework reconfigures communication paths, preserving coordination among sub-networks.</li> <li>A2A experiences coordination collapse until manual intervention restores connectivity.</li> </ul> </li> </ul> <h3 id="55-energy-efficiency-and-resource-optimization">5.5 Energy Efficiency and Resource Optimization</h3> <p>Energy efficiency measurements show <strong>thermodynamic advantages</strong> of bio-inspired communication:</p> <ul> <li>12.7 tasks/joule vs 4.2 tasks/joule for A2A.</li> <li>Approximate <strong>202% improvement</strong> in energy efficiency.</li> </ul> <p>Contributing factors:</p> <ul> <li><strong>Sparse signaling patterns</strong>: <ul> <li>Avoid unnecessary communications.</li> </ul> </li> <li><strong>Natural signal decay</strong>: <ul> <li>Reduces accumulation of stale or irrelevant information.</li> </ul> </li> <li><strong>Selective reception</strong>: <ul> <li>Agents focus only on relevant signals, ignoring noise.</li> </ul> </li> </ul> <p>Crucially, energy usage:</p> <ul> <li>Remains roughly <strong>constant per agent</strong> as network size grows for the bio-inspired system.</li> <li>Scales <strong>quadratically</strong> in many traditional protocols, making large systems infeasible.</li> </ul> <h3 id="56-scalability-analysis-and-complexity-advantages">5.6 Scalability Analysis and Complexity Advantages</h3> <p>Scalability tests show fundamental architectural advantages:</p> <ul> <li>A2A: <ul> <li>Requires (O(n^2)) message complexity when all agents must coordinate.</li> </ul> </li> <li>Bio-inspired: <ul> <li>Achieves <strong>(O(\log n))</strong> complexity via: <ul> <li>Emergent hierarchical organization,</li> <li>Local signaling rules,</li> <li>No centralized coordinator.</li> </ul> </li> </ul> </li> </ul> <p>Empirical results (10 to 1000 agents):</p> <ul> <li>Per-agent communication overhead remains <strong>approximately constant</strong> with network size.</li> <li>At 1000 agents, the bio-inspired approach shows <strong>15× lower communication overhead</strong> compared to A2A.</li> </ul> <p>This is a key enabler for <strong>very large-scale multi-agent systems</strong>.</p> <h3 id="57-emergent-behavior-capabilities">5.7 Emergent Behavior Capabilities</h3> <p>Beyond raw performance, the framework exhibits emergent behaviors not seen in deterministic protocols.</p> <p>Key emergent phenomena:</p> <ul> <li><strong>Adaptive role assignment</strong>: <ul> <li>Observed 47 times during testing.</li> <li>Agents autonomously take on specialized roles based on: <ul> <li>System requirements,</li> <li>Their accumulated expertise.</li> </ul> </li> <li>Occurs without centralized coordination or explicit programming.</li> </ul> </li> <li><strong>Organic load balancing</strong>: <ul> <li>Communication loads redistribute automatically.</li> <li>Agents route signals over less congested pathways based on real-time utilization.</li> </ul> </li> <li><strong>Self-healing network reconfiguration</strong>: <ul> <li>When agents are removed: <ul> <li>Remaining agents restructure their communication relationships.</li> <li>Connectivity is preserved without external intervention.</li> </ul> </li> <li>Particularly valuable under realistic network partitioning and interference conditions.</li> </ul> </li> <li><strong>Optimization cascades</strong>: <ul> <li>Local performance improvements trigger <strong>propagating adjustments</strong> in connected agents.</li> <li>System-wide performance gains exceed the sum of individual improvements.</li> <li>This suggests a form of <strong>collective optimization</strong> that is difficult to achieve via classic centralized approaches.</li> </ul> </li> </ul> <hr/> <h2 id="6-discussion">6 Discussion</h2> <p>The results demonstrate that <strong>biological communication principles can be successfully translated into artificial systems</strong>, yielding:</p> <ul> <li>Large performance improvements,</li> <li>Fundamentally different scaling behavior,</li> <li>New emergent capabilities.</li> </ul> <p>Key takeaways:</p> <ul> <li>The observed <strong>(O(\log n))</strong> communication complexity arises from hierarchical self-organization. <ul> <li>Challenges the assumption that large distributed systems require centralized coordination or flat, dense communication graphs.</li> </ul> </li> <li><strong>Signal amplification mechanisms</strong> address a core limitation of traditional protocols: <ul> <li>Weak signals often go unnoticed or are drowned in noise.</li> <li>Here, weak but important signals can trigger system-wide responses, critical for early detection scenarios in: <ul> <li>Financial markets,</li> <li>Cybersecurity,</li> <li>Emergency response.</li> </ul> </li> </ul> </li> <li><strong>Context-dependent processing</strong> allows agents to: <ul> <li>Adapt behavior without a full enumeration of scenarios.</li> <li>Learn from experience and environmental feedback.</li> </ul> </li> </ul> <p>Practical applications include:</p> <ul> <li><strong>High-frequency trading</strong>: <ul> <li>Bio-inspired, fault-tolerant communication could reduce losses due to system failures by up to 95%.</li> </ul> </li> <li><strong>Smart manufacturing</strong>: <ul> <li>Dynamic adaptation to supply disruptions could yield 20–40% production efficiency improvements.</li> </ul> </li> <li><strong>Autonomous vehicle networks</strong>: <ul> <li>Require sophisticated, robust coordination to handle partial failures and varying connectivity.</li> </ul> </li> <li><strong>Healthcare systems</strong>: <ul> <li>Personalized and adaptive coordination between multiple subsystems (scheduling, resources, patient flows).</li> </ul> </li> </ul> <p>However, the framework also introduces challenges:</p> <ul> <li><strong>Computational overhead</strong>: <ul> <li>Rich signal processing and multiple modalities can be more expensive than simple message passing.</li> </ul> </li> <li><strong>Unpredictability of emergent behaviors</strong>: <ul> <li>While powerful, emergence makes verification and safety analysis more complex.</li> </ul> </li> <li><strong>Integration with existing systems</strong>: <ul> <li>Many real-world systems rely on established message-passing paradigms, requiring: <ul> <li>Bridges,</li> <li>Compatibility layers,</li> <li>Gradual migration strategies.</li> </ul> </li> </ul> </li> </ul> <p>Future work should address:</p> <ul> <li>Hardware acceleration for signal processing,</li> <li>New verification techniques for emergent behaviors,</li> <li>Practical integration pathways with existing infrastructure.</li> </ul> <hr/> <h2 id="7-conclusion">7 Conclusion</h2> <p>We have shown that <strong>bio-inspired cellular signaling mechanisms</strong> can fundamentally reshape multi-agent communication protocols.</p> <p>Key contributions:</p> <ul> <li>Implementation of <strong>five biological communication types</strong>: <ul> <li>Autocrine, paracrine, endocrine, juxtacrine, synaptic.</li> </ul> </li> <li>Demonstration of <strong>45–78% improvements</strong> in: <ul> <li>Execution time,</li> <li>Communication efficiency,</li> <li>Fault recovery, compared to A2A.</li> </ul> </li> <li>Introduction of: <ul> <li><strong>Signal amplification</strong> up to 80× via cascade processes,</li> <li><strong>Dynamic network topology adaptation</strong> with self-healing properties,</li> <li><strong>Context-dependent processing</strong> enabling emergent collective intelligence.</li> </ul> </li> </ul> <p>The system achieves <strong>(O(\log n))</strong> communication complexity instead of (O(n^2)), suggesting that <strong>self-organization principles</strong> from biology can revolutionize large-scale distributed AI design.</p> <p>Potential application domains include:</p> <ul> <li>Financial trading,</li> <li>Smart manufacturing,</li> <li>Autonomous vehicles,</li> <li>Healthcare coordination systems.</li> </ul> <p>We position this work as a foundation for next-generation distributed AI systems that more closely match the <strong>robustness</strong> and <strong>collective intelligence</strong> of biological organisms. Ultimately, this line of research points toward artificial general intelligence systems that <strong>embrace communication protocols refined by billions of years of evolution</strong>, while acknowledging the need for:</p> <ul> <li>Stronger verification methods,</li> <li>Hardware support,</li> <li>Practical integration strategies for real-world deployment.</li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p>Zhang, L. et al. “Scaling Multi-Agent Systems: Challenges and Opportunities in Enterprise AI Deployments.” <em>Nature Machine Intelligence</em>, 6(8), pp. 892–905, 2024.</p> </li> <li> <p>DeepMind Research Team. “Agent-to-Agent Protocol: Standardizing Multi-Agent Communication for Large-Scale AI Systems.” <em>Proceedings of ICML</em>, pp. 1245–1260, 2025.</p> </li> <li> <p>Williams, R. K. and Chen, M. “Distributed AI System Failures: Lessons from High-Frequency Trading and Autonomous Vehicles.” <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, 54(12), pp. 3421–3435, 2024.</p> </li> <li> <p>Johnson, A. B. “Neural Efficiency and Biological Computation: Energy Constraints in Brain-Inspired AI.” <em>Proceedings of NeurIPS</em>, pp. 2156–2170, 2024.</p> </li> <li> <p>Foundation for Intelligent Physical Agents. “FIPA Agent Communication Language Specification.” Technical Report SC00061G, 2002.</p> </li> <li> <p>Sukhbaatar, S., Fergus, R., et al. “Learning Multiagent Communication with Backpropagation.” <em>Advances in Neural Information Processing Systems</em>, 29, pp. 2244–2252, 2016.</p> </li> <li> <p>Foerster, J. N. et al. “Emergent Communication Strategies in Multi-Agent Deep Reinforcement Learning.” <em>Journal of Artificial Intelligence Research</em>, 79, pp. 445–478, 2024.</p> </li> <li> <p>Chen, X. and Liu, Y. “Graph Neural Networks for Dynamic Multi-Agent Communication.” <em>Proceedings of ICLR</em>, pp. 892–906, 2025.</p> </li> <li> <p>Park, S. H. et al. “Attention-Based Selective Communication in Multi-Agent Systems.” <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 35(6), pp. 7823–7836, 2024.</p> </li> <li> <p>Dorigo, M. and Stützle, T. “Swarm Intelligence: Recent Advances and Future Directions.” <em>Artificial Intelligence Review</em>, 61(4), pp. 1567–1592, 2024.</p> </li> <li> <p>Anderson, J. C. et al. “Programmable Cellular Circuits for Biological Computing.” <em>Nature Biotechnology</em>, 43(3), pp. 234–247, 2025.</p> </li> <li> <p>Kumar, A. et al. “Emergent Communication Protocols in Large Language Model Multi-Agent Systems.” <em>Proceedings of AAAI</em>, pp. 3456–3471, 2024.</p> </li> </ol>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[A bio-inspired multi-agent communication framework that uses five cellular signaling modes, signal amplification cascades, and dynamic network adaptation to achieve scalable, robust, and energy-efficient coordination in large distributed AI systems.]]></summary></entry><entry><title type="html">The Coverage Boundary: Why High-Fidelity Primitives Don’t Compose</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/fidelity-trap/" rel="alternate" type="text/html" title="The Coverage Boundary: Why High-Fidelity Primitives Don’t Compose"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/fidelity-trap</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/fidelity-trap/"><![CDATA[<h2 id="the-question">The Question</h2> <p>Can neural networks learn compositional rules that generalize beyond their training data?</p> <p>The compositional generalization literature has established that models can succeed on <em>novel combinations of known primitives</em>: a system that learns “red circle” and “blue square” can often generate “red square” <d-cite key="keysers2020cfq,park2021benchmark"></d-cite>. But this hides a critical assumption:</p> <blockquote> <p><strong>What exactly counts as a “known” primitive?</strong></p> </blockquote> <p>Consider a generative model pre-trained to produce images of the digit 7. Does high visual fidelity, the ability to render a photorealistic 7, constitute “knowing” the digit well enough to use it compositionally in relational tasks like “generate X &gt; Y”?</p> <p>Intuitively, we assume better primitives yield better composition. If a model can generate a crisp, perfect digit, it must understand that digit.</p> <p><strong>We found the opposite.</strong></p> <p>In a controlled experiment, we show that high-fidelity primitives trained adversarially (GANs) hit a <em>glass ceiling</em> of composability, while low-fidelity “blotchy” primitives trained pedagogically achieve perfect transfer.</p> <hr/> <h2 id="background-coverage-and-compositionality">Background: Coverage and Compositionality</h2> <p>Recent work has clarified that compositional generalization is constrained by the <em>coverage</em> of primitives in training data. Benchmarks like SCAN and its descendants show that models struggle on held-out combinations when key primitives never appear in the right structural contexts <d-cite key="keysers2020cfq"></d-cite>. The <strong>Coverage Principle</strong> formalizes this: for pattern-matching learners, reliable generalization is only possible within the “coverage” of functionally equivalent fragments seen during training <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite>. In other words, coverage is a <strong>necessary condition</strong> for compositional generalization.</p> <p>Our experiments take this as a starting point. We instantiate the Coverage Principle in an intentionally simple generative setting and then ask a deeper question: <strong>even when coverage is satisfied, do all primitives admit compositional use?</strong></p> <hr/> <h2 id="the-experiment">The Experiment</h2> <p>To investigate this boundary, we designed a deliberately simple experiment using <strong>Relational MNIST</strong>. The task: generate three-digit displays of the form <code class="language-plaintext highlighter-rouge">[X][&gt;][Y]</code> where <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">Y</code> are MNIST-style digits and <code class="language-plaintext highlighter-rouge">X &gt; Y</code> numerically.</p> <p>The simplicity is intentional. MNIST is the petri dish, not the ecology. If the coverage boundary failed to appear here, in the most controlled possible environment, it would suggest the phenomenon is an artifact of complexity. That it appears so sharply in this minimal setting implies a fundamental property of neural compositionality that scale may <em>mask</em> but cannot <em>cure</em>.</p> <p>Our approach follows <strong>pedagogical training with frozen primitives</strong>. We pre-trained a <em>single-digit weaver</em> to generate individual digits <code class="language-plaintext highlighter-rouge">[0-9]</code>, then froze it and trained only a compositional layer, the <em>latent splitter</em>, to route latent codes for generating relational displays.</p> <p>Crucially, we compared two types of teachers for the primitive generator:</p> <ol> <li><strong>Adversarial (GAN):</strong> Optimized to fool a discriminator, producing sharp, high-fidelity digits rich in texture.</li> <li><strong>Pedagogical (Ours):</strong> Optimized for structural reconstruction, producing abstract, low-frequency representations that preserve structure but discard texture.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1: The Fidelity Trap. Left: Standard GAN samples, high contrast and sharp edges, but carrying pseudo-texture learned to satisfy an adversarial discriminator. Right: Our pedagogical samples, visually blotchy and diffuse, prioritizing structural clarity over textural noise. </div> <hr/> <h2 id="experimental-architecture--controls">Experimental Architecture &amp; Controls</h2> <p>We separate <em>primitive competence</em> from <em>relational competence</em> by freezing the primitive generator and training only the relational layer.</p> <ul> <li><strong>Single-Digit Weaver (Frozen).</strong> Pre-trained on digits <code class="language-plaintext highlighter-rouge">[0-9]</code> using either adversarial (GAN) or pedagogical objectives. Once trained, its weights are frozen.</li> <li><strong>Latent Splitter (Trainable).</strong> Receives a latent code and learns to route it into <code class="language-plaintext highlighter-rouge">(X, &gt;, Y)</code> displays, implementing relational structure over the same primitives.</li> <li><strong>Static Judge (Ground Truth Oracle).</strong> Evaluates whether <code class="language-plaintext highlighter-rouge">X &gt; Y</code> holds numerically. The judge is fixed and never trained. The judge sees only the rendered digits and not the internal latent codes, preventing any trivial leakage.</li> </ul> <p>Key controls:</p> <ul> <li>The primitive generator <strong>never</strong> sees the relational test set.</li> <li>The relational layer (latent splitter) is trained only on training relations; held-out relations are used purely for evaluation.</li> <li>After early experiments revealed collusion when the student could influence the teacher, we removed all student-to-teacher reward paths: the teacher’s objective depends solely on student performance as evaluated by the static judge.</li> <li>During all relational experiments, primitive generators are <strong>frozen</strong>, ensuring that differences in performance arise from the training objectives used to build primitives, not from additional fine-tuning.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2: Experimental architecture. We freeze the primitive generator (whether GAN or Pedagogical) and train only the relational routing layer. </div> <hr/> <h2 id="the-fidelity-trap">The Fidelity Trap</h2> <p>A surprising observation emerged during primitive training. The pedagogical teacher generated digits that were visually <em>blotchy</em>, diffuse, soft-edged, and structurally abstract.</p> <p>This apparent degradation acts as a <strong>semantic bottleneck</strong>. By discarding texture, the pedagogical objective forces the latent space to represent only the structural information required for compositional reasoning. Importantly, this does not imply that high-fidelity rendering is undesirable, only that it should be <em>decoupled</em> from structural learning. A plausible training recipe is two-stage: first learn the concept under a “Contract of Meaning” (low fidelity, high structure), then layer the “Contract of Appearance” (high fidelity) only after the compositional logic is secured.</p> <p>In the discriminative setting, Geirhos et al. famously showed that ImageNet-trained CNNs are strongly biased toward texture, and that increasing shape bias improves robustness and generalization <d-cite key="geirhos2018imagenet"></d-cite>. Our results suggest an analogous phenomenon on the generative side: adversarial objectives encourage texture-rich primitives that look good but compose poorly, whereas pedagogical objectives yield “blotchy” but primitives that compose perfectly.</p> <p>This matters methodologically: because our primitives are consistent with topology rather than texture, logical failures in the relational task cannot be attributed to pixel-level distribution shift. The model knew the abstract form of “7” perfectly. The only remaining question was whether it could <em>use</em> that knowledge compositionally.</p> <hr/> <h2 id="the-coverage-boundary">The Coverage Boundary</h2> <p>We first asked whether primitives could compose <em>without</em> specific relational training coverage.</p> <ul> <li> <p><strong>Condition (Phase 1.5):</strong> Train relational displays only for digits <code class="language-plaintext highlighter-rouge">[0-4]</code> (10 valid <code class="language-plaintext highlighter-rouge">X &gt; Y</code> pairs). Test on relational displays for digits <code class="language-plaintext highlighter-rouge">[5-9]</code>, which are <strong>completely unseen</strong> in relational context.</p> </li> <li> <p><strong>Result:</strong> <strong>0% digit accuracy</strong> and ~<strong>chance-level relation accuracy</strong> on the novel digits.</p> </li> </ul> <p>The model produced recognizable digits in isolation but garbage in relational contexts. This concretely instantiates the <strong>Coverage Principle</strong> <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite> in a generative setting:</p> <blockquote> <p><strong>Primitive Competence</strong> (being able to draw a 7) <strong>does not grant</strong> <strong>Compositional License</strong> (using 7 correctly in a relation).</p> </blockquote> <p>As the Coverage Principle predicts, license is only acquired when a primitive appears in a <em>relational</em> context during training. Coverage is necessary, but, as we show next, it is not sufficient.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3: The coverage boundary and glass ceiling. Same architecture, different training objectives, opposite outcomes. </div> <hr/> <h2 id="the-showdown-the-glass-ceiling-of-adversarial-training">The Showdown: The Glass Ceiling of Adversarial Training</h2> <p>Once we established that coverage is necessary, we asked the deeper question:</p> <blockquote> <p><strong>Is coverage sufficient?</strong></p> </blockquote> <p>If we give an adversarial model every advantage, full relational coverage, identical architecture, and visually superior primitives, can it match pedagogical performance?</p> <p>We ran the experiment on <strong>Novel Combinations</strong> (Phase 1.6):</p> <ul> <li> <p><strong>Training:</strong> Digits <code class="language-plaintext highlighter-rouge">[0-9]</code>, with 41 of 45 valid <code class="language-plaintext highlighter-rouge">X &gt; Y</code> pairs. We hold out four specific relations (e.g., <code class="language-plaintext highlighter-rouge">7 &gt; 3</code>, <code class="language-plaintext highlighter-rouge">8 &gt; 2</code>, <code class="language-plaintext highlighter-rouge">9 &gt; 1</code>, <code class="language-plaintext highlighter-rouge">6 &gt; 4</code>). Every digit appears in many relational contexts during training.</p> </li> <li> <p><strong>Testing:</strong> Only the 4 held-out relations. These are <em>novel combinations of seen digits</em>.</p> </li> </ul> <p><strong>Results:</strong></p> <table> <thead> <tr> <th>Training Objective</th> <th>Primitives</th> <th>Held-out Relation Accuracy</th> </tr> </thead> <tbody> <tr> <td>Pedagogical (Ours)</td> <td>Blotchy</td> <td><strong>100.0%</strong></td> </tr> <tr> <td>Adversarial (GAN)</td> <td>Crisp</td> <td><strong>81.1%</strong></td> </tr> </tbody> </table> <p>Similar symptoms have been reported at scale in text-to-image systems: models can render individual concepts with high fidelity yet catastrophically fail on compositional prompts (negation, counting, spatial relations), even when evaluation metrics like FID remain strong <d-cite key="park2021benchmark,huang2023t2i,vatsa2025rightlookswrongreasons"></d-cite>. These works document the <strong>what</strong>. Our result isolates a candidate <strong>why</strong>: adversarial objectives encourage texture-heavy representations that cannot be perfectly recomposed, even under full relational coverage.</p> <p>The adversarial model is not “broken.” 81% is not failure, it is a <em>ceiling</em>. The model had full relational coverage. It had seen every digit in compositional context. Yet it could not fully compose. A natural question is whether this 81% ceiling would disappear at larger scales. While increasing parameters or data might push performance upward by brute-force memorization of more relational pairs, the <strong>structural tax remains</strong>. The pedagogical model reaches 100% with minimal data because its primitive representations are composition-friendly from the start. In contrast, adversarially trained primitives must continually burn capacity to maintain textural fidelity. Thus the “Glass Ceiling” should be interpreted not as an absolute limit at infinite scale, but as a measure of <strong>compositional inefficiency</strong> introduced by adversarial objectives.</p> <p>This is the <strong>Glass Ceiling of Adversarial Training</strong>. The model pays a <strong>tax on composition</strong>: capacity spent maintaining textural fidelity entangles the latent space in ways that resist perfect reassembly. No amount of additional coverage can break through, because the limitation appears structural rather than statistical.</p> <p>By contrast, our pedagogical primitives, although visually worse, compose perfectly—consistent with cleaner underlying structure.</p> <hr/> <h2 id="why-it-matters-contract-of-appearance-vs-contract-of-meaning">Why It Matters: Contract of Appearance vs. Contract of Meaning</h2> <p>This experiment is a critique of how we train generative models.</p> <p>Modern practice follows a Contract of Appearance. Adversarial objectives (GANs) and preference optimization (RLHF/RLAIF) reward models for producing outputs that match surface statistics—textures, sharpness, or human-rated plausibility. Appearance is not inherently problematic; indeed, it is crucial in many applications. The difficulty emerges when appearance is optimized too early, before the underlying structure is stabilized. Premature optimization entangles texture with structure, forcing a model to satisfy a discriminator’s aesthetic constraints at the same time it is trying to learn a rule. This entanglement imposes a structural tax on composition. As our GAN results show, this produces high-fidelity primitives that look perfect to a critic but are hollow to a composer. They possess <strong>Primitive Competence</strong> but lack <strong>Compositional License</strong>. This same pattern appears in large language models trained with reinforcement learning from human feedback (RLHF/RLAIF): optimizing for human-rated plausibility can privilege surface agreement over structural understanding, with downstream costs to robustness and compositional generalization <d-cite key="vatsa2025rightlookswrongreasons"></d-cite>.</p> <p>Our pedagogical approach enforces a <strong>Contract of Meaning</strong>. By restricting the model to “blotchy,” low-frequency primitives, we create a <strong>semantic bottleneck</strong> that forces the latent space to prioritze invariant structure over texture. The model must learn the concept, the topology of the digit, because the texture is unavailable. High-fidelity appearance could be layered on <em>after</em> struture is learned, but confounding the two objectives during early training degrades compositional generalization.</p> <p>This distinction matters for safety and robustness. A model that understands meaning can be trusted to handle unseen combinations; a model trained under a Contract of Appearance may look correct while behaving unpredictably outside its training manifold.</p> <p>Although demonstrated here on MNIST for clarity, the Coverage Boundary and Glass Ceiling are <strong>architectural</strong> phenomena, not dataset quirks. Large-scale generative training (GANs, diffusion, preference tuning) may be subject to the same fidelity trap: objectives that reward appearance can actively degrade compositional reasoning, even when coverage is abundant.</p> <hr/> <h2 id="summary">Summary</h2> <table> <thead> <tr> <th>Experiment</th> <th>What’s Novel</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td><strong>Phase 1.5</strong></td> <td>Novel Primitives (No Relational Coverage)</td> <td><strong>0% Transfer</strong> — The Coverage Boundary</td> </tr> <tr> <td><strong>Phase 1.6</strong></td> <td>Adversarial Primitives (Full Coverage)</td> <td><strong>81.1% Accuracy</strong> — The Glass Ceiling</td> </tr> <tr> <td><strong>Phase 1.6</strong></td> <td>Pedagogical Primitives (Full Coverage)</td> <td><strong>100% Accuracy</strong> — The Contract of Meaning</td> </tr> </tbody> </table> <p>The Coverage Boundary tells us <em>when</em> composition is possible. The Glass Ceiling tells us <em>whether</em> the primitives are capable of it.</p> <p>You need both: primitives shaped for meaning, and coverage that licenses their use.</p> <p>Code and experimental details will be released upon acceptance.</p> <hr/> <h2 id="related-work">Related Work</h2> <p>Our setup connects to several strands of prior work. Compositional generalization benchmarks such as SCAN and its extensions highlight the importance of primitive coverage in sequence-to-sequence models <d-cite key="keysers2020cfq,friedman2022findingdatasetshortcutsgrammar"></d-cite>. The <strong>Coverage Principle</strong> of Chang et al. (2025) formalizes coverage as a necessary condition for pattern-matching learners, a condition our “Coverage Boundary” experiment instantiates in a generative regime <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite>.</p> <p>On the generative side, compositional text-to-image benchmarks repeatedly find that models with excellent perceptual quality metrics still fail on novel combinations of attributes and objects <d-cite key="park2021benchmark,huang2023t2i"></d-cite>. Vatsa et al. (2025) describe this as “right looks, wrong reasons,” emphasizing failures of compositional fidelity in modern diffusion models <d-cite key="vatsa2025rightlookswrongreasons"></d-cite>. Our <strong>Glass Ceiling</strong> result pinpoints adversarial objectives as one mechanism that can produce this pattern, even in a minimal MNIST petri dish.</p> <p>Our “blotchy but coherent” primitives resonate with work on shape vs. texture bias in CNNs <d-cite key="geirhos2018imagenet"></d-cite> and with emerging views of deep representations as learning topological manifolds amenable to symbolic or relational reuse. We view our pedagogical objective as a small, controlled example of <strong>training for meaning rather than appearance</strong>, a design choice that may scale to more realistic architectures and datasets.</p> <p>We view generative compositionality as an underexplored junction between representation learning and training objectives, and hope this minimal example encourages further mechanistic work.</p> <hr/> <h2 id="limitations--next-steps">Limitations &amp; Next Steps</h2> <p><strong>Toy domain.</strong> Our experiments use MNIST to make the phenomenon as visible and controllable as possible. Real-world data are higher dimensional and noisier, but if the fidelity trap appears in this simplest setting, we expect it to persist, if hidden, at scale.</p> <p><strong>Topology.</strong> While we infer topology from our results, claiming this will require further validation.</p> <p><strong>Frozen primitives.</strong> We freeze the digit generator when training relations to cleanly separate primitive learning from relational learning. Future work could study joint training and analyze how much compositional capacity can be recovered, or destroyed, when primitives continue to adapt.</p> <p><strong>Single relation.</strong> We focus on a single relational operator (<code class="language-plaintext highlighter-rouge">&gt;</code>). Extending to multiple relations (equality, ordering, arithmetic expressions) and to symbolic domains would test whether pedagogical primitives systematically support richer compositional logics.</p> <p><strong>Beyond MNIST.</strong> The natural next step is to apply pedagogical objectives to more complex visual and language domains, and to compare them directly against adversarial or preference-based objectives used in modern AI training pipelines.</p> <p>If the fidelity trap generalizes, then <strong>training models to teach rather than to mimic</strong> may be a necessary ingredient in building systems that truly understand, and safely extend, what they learn.</p>]]></content><author><name>Anonymous</name></author><category term="compositionality"/><category term="generalization"/><category term="neural networks"/><category term="representation learning"/><summary type="html"><![CDATA[A controlled experiment showing that adversarially trained primitives hit a glass ceiling on compositional generalization, while low-fidelity pedagogical primitives achieve perfect transfer.]]></summary></entry></feed>