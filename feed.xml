<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iclr-blogposts.github.io/2026/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2026/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-08T03:44:53+00:00</updated><id>https://iclr-blogposts.github.io/2026/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Fairness Audits as Theater: When Metrics Mask Structural Harm</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/fairness-audits/" rel="alternate" type="text/html" title="Fairness Audits as Theater: When Metrics Mask Structural Harm"/><published>2026-12-07T00:00:00+00:00</published><updated>2026-12-07T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/fairness-audits</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/fairness-audits/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Machine learning systems are everywhere—and they are broken. Across domains from criminal justice to employment screening to healthcare, algorithms have replicated, amplified, and legitimized historical inequalities. In response, the AI ethics community has converged on a solution: <strong>fairness audits</strong>.</p> <p>Audits have become the lingua franca of algorithmic accountability. Companies commission them. Regulators expect them. Researchers publish them. The idea is straightforward: measure bias, document findings, demonstrate due diligence, proceed with deployment. Yet despite this explosion in auditing infrastructure, algorithmic harm persists.</p> <p>This blog post argues that contemporary fairness audits function as <strong>legitimation theater</strong> rather than mechanisms of genuine accountability. We identify three structural problems that prevent audits from preventing harm, then propose alternatives grounded in substantive accountability and participatory oversight.</p> <p>The stakes are high. As auditing becomes institutionalized, it risks creating a false sense of resolution while marginalizing communities most affected by algorithmic systems. Understanding why audits fail is the first step toward building accountability frameworks that actually work.</p> <h2 id="the-rise-and-crisis-of-fairness-audits">The Rise and Crisis of Fairness Audits</h2> <h3 id="what-audits-promise">What Audits Promise</h3> <p>The fairness audit emerged as a response to the reproducibility and integrity crisis in machine learning. After landmark publications—Buolamwini and Gebru’s <em>Gender Shades</em> (2018), ProPublica’s COMPAS investigation (2016), Obermeyer et al. on medical AI (2019)—the research community recognized that algorithmic bias was not incidental but systematic.</p> <p>Audits promised a solution. By treating algorithms like financial systems or medical devices, audits would:</p> <ol> <li><strong>Measure bias systematically</strong> using rigorous fairness metrics</li> <li><strong>Identify disparities</strong> before deployment</li> <li><strong>Document processes</strong> for regulatory compliance</li> <li><strong>Enable accountability</strong> by creating a paper trail</li> </ol> <p>The appeal is intuitive. Audits are <em>objective</em>, <em>scalable</em>, and <em>defensible</em>. They transform the abstract problem of “fairness” into concrete numbers: disparate impact ratios, false positive rate gaps, calibration indices. A system that passes an audit appears legitimate. Checkboxes are checked. Lawyers are satisfied.</p> <h3 id="what-audits-deliver">What Audits Deliver</h3> <p>In practice, audits deliver legitimacy without justice.</p> <p>Consider the structure of a typical audit:</p> <blockquote> <ol> <li>A third-party firm is hired</li> <li>The firm tests the algorithm against a selection of fairness metrics</li> <li>The firm produces a report documenting disparities</li> <li>The report is filed away or released to satisfy legal requirements</li> <li>The system is deployed or continues operating</li> </ol> </blockquote> <p>This workflow is reassuring precisely because it appears neutral. Yet each step embeds values and choices that shape whether audits actually prevent harm.</p> <p><strong>The metric selection problem</strong>: Which fairness metrics to test? Dozens exist, many mathematically incompatible. The choice is not technical—it is political. Testing only group fairness metrics (equal outcome across groups) may miss individual fairness concerns (similar people treated similarly). Testing only predictive parity (equal accuracy across groups) may legitimize systems that amplify existing disparities in outcomes. The audit firm—accountable to the client who hired them—selects metrics convenient to pass.</p> <p><strong>The context collapse problem</strong>: Audits extract algorithms from the socio-technical contexts that give them meaning. An algorithm may achieve group fairness in aggregate but fail catastrophically for specific subpopulations (intersectional groups). An algorithm may pass all fairness tests in one jurisdiction but encode biases specific to another’s data distributions. Audits typically treat these as edge cases, not failures.</p> <p><strong>The legitimation problem</strong>: Once an audit is complete, the system becomes “certified” as fair. This certification creates a false sense of resolution. Decision-makers can point to the audit and claim due diligence was conducted. Communities affected by the system have limited recourse—the audit was done, what more can be done? The legitimacy of the audit becomes a defense against further scrutiny.</p> <h2 id="why-current-audits-fail">Why Current Audits Fail</h2> <h3 id="problem-1---the-metric-selection-problem">Problem 1 - The Metric Selection Problem</h3> <p>Fairness metrics are not discovered; they are constructed. Each metric encodes assumptions about what fairness means and how it should be measured. The key insight is that <strong>no single metric captures fairness completely</strong>.</p> <p>Consider a hiring algorithm. We might measure:</p> <ul> <li><strong>Demographic parity</strong>: Equal hiring rates across groups (if 60% of applicants are male, hire 60% male employees)</li> <li><strong>Equalized odds</strong>: Equal true positive and false positive rates across groups (equally likely to hire qualified candidates; equally unlikely to hire unqualified ones)</li> <li><strong>Calibration</strong>: If the algorithm predicts someone has a 70% chance of success, they succeed 70% of the time (across groups)</li> <li><strong>Individual fairness</strong>: Similar applicants get similar predictions</li> </ul> <p>These metrics are mathematically incompatible. An algorithm cannot simultaneously satisfy all of them. So auditors must choose.</p> <p>Who chooses? Typically, the firm conducting the audit—whose client is the company deploying the algorithm. Under this arrangement, auditors face subtle (or not-so-subtle) pressure to select metrics that reveal acceptable levels of bias. If the client is deploying an algorithm that is computationally convenient but happens to disparately impact women, auditors might prioritize demographic parity (which the algorithm likely fails) but de-emphasize calibration (which it might pass). The choice appears technical; in practice, it is strategic.</p> <p>Real-world example: Amazon’s hiring algorithm initially scored candidates on a 0-5 scale and automatically filtered out anyone below 4. After audit, the company discovered the algorithm systematically downscored women. Amazon then… stopped using the score. They didn’t change the algorithm; they simply removed the automated filtering. The audit identified the bias but didn’t prevent deployment of the underlying biased system—it just made the bias less visible.</p> <h3 id="problem-2---the-context-collapse-problem">Problem 2 - The Context Collapse Problem</h3> <p>Fairness metrics assume a kind of context-free universalism. A metric is either satisfied or not. But algorithms operate in specific socio-technical contexts that metrics cannot capture.</p> <p>Consider COMPAS, the recidivism assessment tool. The algorithm predicts who will reoffend based on historical criminal justice data. Audits of COMPAS have revealed:</p> <ul> <li><strong>Structural bias</strong>: The algorithm inherits biases from training data shaped by racist policing practices</li> <li><strong>Feedback loops</strong>: Police deploy the algorithm to target neighborhoods, resulting in more arrests in those areas, which then become training data showing higher recidivism, which triggers more enforcement</li> <li><strong>Differential consequences</strong>: A high score for a Black defendant means harsher sentencing; a high score for a white defendant means increased supervision (both harmful, but differently)</li> </ul> <p>No fairness metric—not demographic parity, not equalized odds, not calibration—captures these contextual harms. An algorithm can pass every fairness test while perpetuating systemic racism. This is because metrics measure <strong>properties of the algorithm</strong>. They do not measure <strong>properties of the world</strong>: historical inequalities, ongoing discrimination, power asymmetries between the algorithm and those it affects.</p> <p>The context collapse problem is particularly acute for algorithmic systems that operate at the intersection of multiple domains. Consider hiring algorithms that use educational credentials as predictors. The algorithm may be internally fair (women and men with the same education are equally likely to be hired). But if the algorithm inherits biases from the educational system—where admissions, funding, and opportunities are unequally distributed—then the algorithm propagates those biases forward. An audit that ignores educational context will miss this structural propagation.</p> <h3 id="problem-3---the-legitimation-problem">Problem 3 - The Legitimation Problem</h3> <p>Perhaps the most insidious failure of audits is that they create legitimacy for systems that remain unjust.</p> <p>When a company hires a reputable firm to audit an algorithm, the company gains several things:</p> <ol> <li><strong>Legal protection</strong>: If sued, the company can point to the audit as evidence of due diligence</li> <li><strong>Rhetorical power</strong>: The company can claim the system is “fair” (or “as fair as possible”)</li> <li><strong>Internal reassurance</strong>: Employees can believe they are working for an ethical company</li> <li><strong>Reduced scrutiny</strong>: Once audited, the system faces less external pressure to change</li> </ol> <p>For affected communities, audits often feel like performance. Researchers and advocates may have repeatedly warned about a system’s harms. An audit is commissioned. A report is produced. The company claims to take findings seriously. The system remains deployed. What changed? Often, very little—except that the company now has a document testifying to awareness of the problem.</p> <p>This is the legitimation trap: audits create accountability theater. They make it appear that someone is responsible for ensuring fairness. But responsibility without power is theater. Audits typically have no enforcement mechanisms. If an audit finds bias, there is no requirement to fix it. The company can acknowledge the finding, claim context makes it unavoidable, and continue deploying.</p> <p>Worse, audits can <em>prevent</em> further accountability by creating a sense that the system has been properly scrutinized. Regulators who see an audit report may conclude that no further investigation is needed. Communities may be too exhausted by the audit process to push for additional changes. The audit closes the door on demanding more.</p> <h2 id="case-studies-in-audit-failure">Case Studies in Audit Failure</h2> <h3 id="amazons-hiring-algorithm">Amazon’s Hiring Algorithm</h3> <p>Amazon’s recruiting tool was trained on 10 years of historical hiring data. The algorithm learned to predict who would be hired and who would succeed in the role. Like most machine learning systems trained on historical data, it learned to replicate existing hiring patterns—which historically favored men, particularly in technical roles.</p> <p>An audit, conducted internally, found the algorithm systematically downscored women applicants. Amazon’s response: stop using the score for final hiring decisions, but keep using it for other purposes. The algorithm remained deployed; only its application changed.</p> <p>What did the audit accomplish? It created awareness of the bias (which was known to researchers and advocates beforehand). It did not prevent the bias from influencing hiring (the algorithm still scores women lower; the company just doesn’t automatically reject based on the score). It did allow Amazon to claim it took fairness seriously.</p> <p>The deeper issue: Amazon’s hiring data reflected decades of underrepresentation of women in tech. No audit of the algorithm could fix this upstream bias. An adequate response would have required addressing why the training data was skewed—a question audits typically don’t ask. An audit that examined only the algorithm and not the historical data that shaped it was always going to miss the root cause.</p> <h3 id="compas-recidivism-assessment">COMPAS Recidivism Assessment</h3> <p>ProPublica’s investigation of COMPAS found that the algorithm had different false positive rates for Black and white defendants: Black defendants were 45% more likely to be falsely labeled as high-risk. Despite ProPublica’s analysis, Northpointe (COMPAS’s developer) claimed the algorithm was fair because it had similar predictive accuracy across racial groups (calibration).</p> <p>Subsequent audits found conflicting results depending on which fairness metrics were used. By some metrics, COMPAS was fair. By others, it was not. Each audit could cherry-pick metrics to support a predetermined conclusion.</p> <p>Meanwhile, COMPAS remained in use across U.S. courts, influencing bail decisions, sentencing, and parole determinations. The debate over which fairness metric to use—carried out in academic papers and audit reports—had no impact on deployment. The algorithm continued to shape human lives while researchers argued about how to measure its bias.</p> <p>The audit failure here was not methodological but structural. The choice of which metric to believe was decided not by auditors but by judges and policy-makers, based on which metric was most convenient. An audit that could not compel acceptance of its findings was an audit that could not prevent harm.</p> <h3 id="healthcare-risk-scores">Healthcare Risk Scores</h3> <p>Hospitals and insurance companies use algorithms to predict patient outcomes and allocate resources. One widely-used system predicted mortality risk to identify patients needing palliative care. When audited, the algorithm was found to systematically underestimate risk for Black patients.</p> <p>Why? The algorithm was trained on healthcare cost as a proxy for illness severity. Since Black patients receive systematically lower healthcare spending due to structural racism in medicine, the algorithm learned to associate Blackness with lower disease severity. When exposed to Black patients with the same objective health status as white patients, the algorithm predicted better outcomes.</p> <p>This bias had profound consequences: Black patients were less likely to be identified for palliative care and early intervention. The algorithm replicated and amplified existing racial disparities in healthcare.</p> <p>After the audit, the algorithm was updated to remove race from the training features. But this didn’t solve the problem—the bias was not in the race variable itself, but in the proxy (cost) used for the outcome. Removing race was like treating a symptom while ignoring the disease. An adequate response would have required reconstructing the outcome variable and retraining on better data, which would have required collaboration with clinicians, patients, and healthcare systems. It would have required examining why Black patients receive less healthcare spending in the first place. Instead, the quick fix was applied, the audit was completed, and the algorithm continued to encode historical bias.</p> <h2 id="beyond-audits---toward-substantive-accountability">Beyond Audits - Toward Substantive Accountability</h2> <p>If audits fail, what works? The answer is not “better metrics” or “more rigorous audits.” The answer is <strong>participation</strong>.</p> <h3 id="participatory-model-cards">Participatory Model Cards</h3> <p>Traditional audits are conducted by experts (auditors) examining systems designed by experts (engineers) based on data selected by experts (product teams). Communities affected by the systems have no role.</p> <p>An alternative is participatory model cards: documents that describe the algorithm’s purpose, training data, performance, limitations, and known biases—co-produced by engineers, domain experts, affected communities, and ethicists.</p> <p>A participatory model card for a hiring algorithm might include:</p> <ul> <li><strong>Purpose</strong>: Identify qualified candidates (written by product team)</li> <li><strong>Data sources</strong>: Historical hiring records, with analysis of how the training data encodes historical biases (written collaboratively)</li> <li><strong>Intended use</strong>: Initial screening to identify candidates for human review (written by HR)</li> <li><strong>Known failures</strong>: The algorithm systematically downscores women with engineering backgrounds due to underrepresentation in the training data (identified by auditors and affected communities working together)</li> <li><strong>Conditions for deployment</strong>: Only use the algorithm if the hiring team receives training on its limitations; only use as initial screening, never for final decisions; regularly audit for bias; create appeals process for candidates who wish to challenge the score (negotiated with affected communities)</li> </ul> <p>The key difference: a participatory process creates accountability to affected communities, not just to the company deploying the system.</p> <h3 id="threshold-advocacy-and-context-binding">Threshold Advocacy and Context Binding</h3> <p>Some fairness metrics may be useful not as universal measures of fairness, but as <strong>threshold advocates</strong>: metrics that flag when a system is clearly unacceptable.</p> <p>For instance, a metric might state: “If the false positive rate for one group is more than 20% higher than for other groups, the system should not be deployed without explicit written justification.” This is not claiming to measure fairness perfectly; it is drawing a line against egregious harm.</p> <p>Threshold advocacy pairs metrics with <strong>context binding</strong>: making explicit the contexts in which the algorithm can be deployed, and the conditions under which it must be re-audited.</p> <p>An algorithm might be approved for use in employment screening in one domain but not another, because the contexts are different. An algorithm might be approved conditionally, with requirements to audit quarterly, to maintain an appeals process, to involve affected communities in oversight.</p> <p>This approach treats audits not as one-time certifications but as the beginning of ongoing accountability.</p> <h3 id="rights-based-accountability-frameworks">Rights-Based Accountability Frameworks</h3> <p>Rather than asking “Is this algorithm fair?” audits could ask “Are the rights of people affected by this algorithm protected?”</p> <p>A rights-based framework might include:</p> <ol> <li><strong>Right to know</strong>: Affected people should know if and how an algorithm is used to make decisions about them</li> <li><strong>Right to understand</strong>: The algorithm’s logic should be explainable in terms meaningful to those affected</li> <li><strong>Right to challenge</strong>: Affected people should be able to contest algorithmic decisions and have challenges reviewed by a human with authority to override</li> <li><strong>Right to remedy</strong>: If an algorithm causes harm, affected people should have access to compensation or system change</li> <li><strong>Right to participate</strong>: Communities should have voice in decisions about whether and how algorithms are deployed</li> </ol> <p>These rights cannot be verified by auditing the algorithm alone. They require examining the socio-technical system: Does the company actually tell people when algorithms are used? Can they understand the explanations provided? Can they challenge decisions? Can they receive remedy? Are they actually participating in oversight?</p> <p>Rights-based audits would be messier than metric-based audits. They would require qualitative research, community engagement, and ongoing monitoring. But they would be more likely to prevent harm because they focus on what actually matters to affected communities.</p> <h2 id="implications-for-practice">Implications for Practice</h2> <p>For practitioners, the message is clear: <strong>audits alone are insufficient</strong>. But audits are not useless—they are just incomplete.</p> <p><strong>For companies deploying algorithms</strong>:</p> <ul> <li>Don’t use an audit as a substitute for accountability</li> <li>Commission audits, but involve affected communities in the process</li> <li>Use audit findings not as a sign the system is ready to deploy, but as the starting point for governance design</li> <li>Create accountability structures: oversight boards, appeals processes, regular re-auditing</li> <li>Be transparent about audit findings and limitations</li> </ul> <p><strong>For auditors and researchers</strong>:</p> <ul> <li>Be explicit about the values embedded in your choice of fairness metrics</li> <li>Consider multiple metrics and document trade-offs, not just those the algorithm passes</li> <li>Examine the socio-technical context, not just the algorithm</li> <li>Engage affected communities in auditing, not just as data subjects</li> <li>Decline to conduct audits when you lack power to ensure findings lead to change</li> </ul> <p><strong>For regulators and policymakers</strong>:</p> <ul> <li>Don’t accept audits as evidence of compliance</li> <li>Require procedural safeguards (appeals processes, community oversight, transparency) not just technical safeguards (metrics)</li> <li>Build in incentives for companies to genuinely address audit findings, not just acknowledge them</li> <li>Support community organizations in conducting independent audits</li> <li>Create legal liability for algorithmic harms, regardless of audit status</li> </ul> <p><strong>For affected communities</strong>:</p> <ul> <li>Audits are not accountability—they are theater</li> <li>Demand participation in audit processes, don’t accept being studied</li> <li>Push for rights-based frameworks, not metric-based ones</li> <li>Document harms and build alternative knowledge systems</li> <li>Connect with other communities affected by algorithms; collective pressure is more effective than individual appeals</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Fairness audits have become the default mechanism for algorithmic accountability. Yet they fail to prevent harm because they treat fairness as a technical problem solvable through measurement, when it is fundamentally a political problem requiring power-sharing.</p> <p>An algorithm that measures its own fairness and declares itself fair is not making a meaningful claim. A company that audits its own algorithms and publishes findings it selects is performing transparency without practicing it. A regulator that accepts audits as evidence of compliance is delegating its authority.</p> <p>The alternative is uncomfortable. It requires admitting that algorithms cannot be made “fair” if the training data encodes historical injustice. It requires involving communities in technical decisions, which is slower and messier than expertise-driven processes. It requires giving affected people power to say “no” to systems, not just to participate in their design.</p> <p>But this is what genuine accountability looks like. Not audits that certify systems as fair. But systems designed with affected communities, governed by affected communities, and changed when affected communities say they’re causing harm.</p> <p>The work of building trustworthy AI is not the work of auditors measuring systems. It is the work of power-sharing and accountability that algorithms can only support, not substitute for.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[This blog post examines why contemporary fairness audits fail to prevent algorithmic harm, despite growing adoption. We analyze structural limitations and propose substantive alternatives grounded in participatory accountability.]]></summary></entry><entry><title type="html">Boundlessness Overtaking Benchmarks: The Crisis of Evaluating AI Scientists</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/boundlessness-overtaking-benchmarks/" rel="alternate" type="text/html" title="Boundlessness Overtaking Benchmarks: The Crisis of Evaluating AI Scientists"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/boundlessness-overtaking-benchmarks</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/boundlessness-overtaking-benchmarks/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Since the origins of machine learning as a field, research has been anchored by a simple, shared and stable framework of evaluation:</p> <p><code class="language-plaintext highlighter-rouge">fix a data set → fix an evaluation metric → compare models → assess performance</code></p> <p>In this, the task is well-specified, the output space is bounded, and the ground truth is fixed. This framework rewarded rigor and reproducibility, and it provided a shared language for progress.</p> <p>Machine learning (ML) has applied this evaluation framework in many sub-fields, from computer vision to speech recognition, but our discussion here focuses on NLP, especially knowledge-grounded scientific tasks. Benchmarks like Natural QA, HotpotQA, and BioASQ provided a comfort zone where researchers could iterate rapidly, measure objectively, and scale evaluations to tens or hundreds of thousands of samples to ensure generalizability and robustness <d-cite key="brown2025systematicliteraturereviewretrievalaugmented"></d-cite>. Even in the era of long-form tasks, like machine translation or summarization, metrics such as BLEU, ROUGE, METEOR, and BERTScore—whether they measure n-gram overlap or embedding-based similarity—offered a shared reference point <d-cite key="schmidtova-etal-2024-automatic-metrics"></d-cite>. The outputs were usually short with low level of abstraction, and the evaluation problem was fundamentally one of lexical or semantic similarity. Imperfect, yes, but collectively understood. These benchmarks gave us the comforting illusion that we were comparing like with like.</p> <p>And it worked spectacularly until we built models that escaped this framework. LLMs did not merely improve performance; they changed the fundamental nature of what a model is and what an output can be. AI models today can effortlessly switch between generating a short, three-sentence summary to writing a detailed, 60-page research proposal.</p> <p>We are therefore drifting towards a world in which:</p> <ul> <li>Output sizes are unbounded: A model can generate a book, a research paper, a laboratory protocol, or a simulation-driven argument.</li> <li>Reasoning paths are not unique: Multiple chains of thought can be valid, divergent, and equally defensible.</li> <li>Quality is contextual: The correctness of a biology report is evaluated differently from a computer science theorem or a social science argument.</li> <li>Domain experts validate science: Evaluating 10,000 open-ended essays for “scientific quality” is not scalable the way classifying 10,000 images is.</li> </ul> <p>We therefore eagerly need a corresponding shift in our epistemic infrastructure, in order to be able to evaluate and further develop this new world.</p> <p>In this blog post, we focus exclusively on the emerging space of AI systems that generate scientific outputs—hypotheses, experimental plans, methodological rationales, data-interpretation narratives, and full research papers. Our discussion does not to the same degree concern tasks like math olympiad problems or abstract reasoning questions, which have well-defined solutions fundamentally different from long-form scientific output.</p> <h2 id="the-evaluation-problem-no-one-wants-to-name">The Evaluation Problem No One Wants to Name</h2> <p>The fundamental problem with rapidly evolving AI models and their expanding capabilities is that current machine-learning benchmarks rest on assumptions that no longer hold:</p> <p><strong>Assumption 1</strong>: There exists a canonical correct answer. <br/> Scientific long-form tasks rarely have one. In science, there usually is no clear right/wrong dichotomy. Rather, scientific contributions are evaluated according to their usefulness (for further progress). Wrong answers can be equally useful to refute hypotheses, motivate further work, or simply as eye-openers. AI models for science can therefore generate entire argumentative universes, not just labels.</p> <p><strong>Assumption 2</strong>: Evaluation is output-based. <br/> In science, it is often not the final output that matters most but the process of how one got there: new algorithms often emerge from constructive proofs, and multiple paths can lead to the same result, not all of them equally insightful. Also in scientific texts, justification, citation accuracy, experimental validity, and methodological rigor are often more important than mere quality of language.</p> <p><strong>Assumption 3</strong>: Humans can serve as gold-standard evaluators. <br/> Scientific quality is hard to evaluate, even for humans, as often only time will tell. Human peer review is notoriously inconsistent even for conventional papers. For more groundbreaking, innovative papers, experts wildly disagree. For AI-generated content—which can be longer, denser, and more numerous—there is a simple scaling problem: having humans evaluate thousands of long-form research outputs is simply impossible. We have pushed the complexity of the output space into a regime where humans themselves cannot provide consistent, scalable ground truth.</p> <p><strong>Assumption 4</strong>: Correctness is equivalent to alignment with ground truth. <br/> This assumption worked well for traditional long-form QA, where correctness could be approximated by matching key facts or phrases to a reference answer. However, for open-ended tasks such as hypothesis generation, research writing, or literature review, aspects like creativity and perspective are intrinsic. For instance, if the task involves generating a literature review, the original survey may serve as a reference point, but the organization, articulation, and interpretation of the research objectives produced by an AI system can turn out to be completely different from any previously known ground truth. Since humans write divergent reviews from the same sources, unalignment does not imply incorrectness.</p> <p>In summary, when AI models violate one or several of these assumptions, their performance cannot be evaluated by word-pattern matching, embedding similarity, or any simple classification proxy. In this sense, recent LLMs have escaped the benchmarking infrastructure we once built around deterministic tasks.</p> <h2 id="progress-lacks-standard-measurement">Progress Lacks Standard Measurement</h2> <p>With the arrival of AI Scientists, lab-automation agents, and autonomous discovery pipelines, we face a new, uncomfortable reality: the traditional foundations of ML evaluation do not extend to AI-generated scientific texts. The recent generation of AI systems does not merely answer questions. These systems generate full reasoning paths, complete with rationale, exposition, and self-evaluation. This shift in what AI systems produce requires rethinking how they are evaluated. A strong indicator of this shift can be seen in the examples of recent works below, which report results on very small test data sets—often 3 to 20 samples—rather than the large-scale evaluations common in traditional ML. As a result, the emphasis moves towards showcasing model’s capabilities rather than testing generalization. The examples that follow illustrate this change and collectively point to the absence of a shared, large-scale evaluation protocol for AI-driven scientific discovery.</p> <ol> <li> <p><strong>ChemCrow</strong> <d-cite key="bran2023chemcrow"></d-cite> is an LLM-based chemistry agent, published by EPFL in Nature Machine Intelligence in May 2024. It uses a tool-augmented (ReAct) approach, with access to 18 external tools, including ChemSpace, PubChem, and the IUPAC to SMILES converter OPSIN. The authors evaluate its chemical reasoning capabilities across 14 specialized tasks from drug discovery, organic synthesis, and materials design. They compared ChemCrow’s output against GPT-4 outputs using EvaluatorGPT (LLM judge) and domain experts. The expert chemists were asked to evaluate each model’s performance for each task along three dimensions: (a) quality of reasoning, (b) correctness of the chemistry, and (c) degree of task completion. As per the <a href="https://github.com/ur-whitelab/chemcrow-runs/tree/main/tasks">code</a> and <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs42256-024-00832-8/MediaObjects/42256_2024_832_MOESM1_ESM.pdf">Appendix G in the supplementary material</a>, it appears that each task included only a single test instance. This yields a data set of merely 14 samples in total. In their limitations section (Appendix F), the authors acknowledged a broader need for standardized and scalable assessment frameworks for AI scientist agents. Heavy reliance on expert judgment and the demanding nature of designing domain-specific experiments to display the strengths and weaknesses of an agent both limit the speed and consistency of evaluation.</p> </li> <li> <p><strong>AI Scientist-v1</strong> <d-cite key="lu2024ai"></d-cite> published by SakanaAI as an arXiv preprint in August 2024, demonstrated the ability to autonomously generate research artifacts—code, experiments, and papers—across multiple subfields of machine learning, including diffusion models, transformer-based language modeling, and learning dynamics. To evaluate the outputs, the authors built an automated peer-reviewer (powered by an LLM) that assigns scores to the generated papers similar to human peer-review assessing: novelty, methodological soundness, clarity, empirical results, and overall contribution. To validate the automated reviewer, they tested it on a data set of 500 real papers from ICLR 2022 (from the OpenReview public archive). The data set was unbalanced, containing more rejected papers. They then compared the LLM’s accept/reject decisions with the actual outcomes. The automated reviewer achieved roughly 70% overlap with the human-written reviews When evaluated on a balanced subset of accepted vs. rejected papers, it attained roughly 65% balanced accuracy (human-level ~66% in the same consistency experiment). The system then generated its own ML-research papers, which were fed into the same automated reviewer. Some of them “passed”, i.e., they exceeded the acceptance threshold as defined by the reviewer.</p> </li> <li><strong>AI Co-Scientist</strong> <d-cite key="gottweis2025towards"></d-cite> published by SakanaAI and Google as an arXiv preprint in February 2025, is a multi-agent system built on Gemini 2.0. It uses a “generate → debate → evolve” workflow: given a high-level research goal in natural language, specialized agents generate candidate hypotheses, critique and rank them, then refine and evolve them—akin to an automated “scientific debate and selection” process. The authors demonstrated the potential for augmenting biomedical and scientific discovery in three applications: novel target discovery for liver fibrosis, drug repurposing for acute myeloid leukemia, and explaining mechanisms of bacterial evolution and anti-microbial resistance. The authors combined multiple evaluation strategies: <ul> <li>Automated hypothesis-quality evaluation: A pool of 203 distinct research goals was curated to evaluate system-generated hypotheses using Elo rating. Such automated ranking (Elo), however, only provides relative plausibility, not guaranteed scientific feasibility.</li> <li>Full paper evaluation: A subset of 15 research goals was curated by seven biomedical human experts to evaluate the full research articles generated by the model. There was no broad “peer review of full reports” for all tasks or research goals. The expert human evaluation was limited to novelty/impact preference and did not include a full rigorous peer review of the generated research articles.</li> <li>Report validation with domain experts: Three of the selected hypotheses were validated in wet-lab experiments (in vitro / organoid). This provided isolated stand-alone demonstrations of success, as some of the AI’s top (albeit scientifically rather unsurprising) predictions could be experimentally confirmed.</li> </ul> </li> <li> <p><strong>AI Scientist-v2</strong> <d-cite key="yamada2025ai"></d-cite>, the second version of AI-Scientist, published by SakanaAI as an arXiv preprint in April 2025, is an end-to-end agentic system that autonomously generates scientific hypotheses, designs and runs experiments, analyzes and visualizes the data, and finally writes manuscripts—making it the first system claimed to produce fully AI-generated papers that passed peer review at a workshop. For evaluation, the authors submitted three manuscripts produced entirely by the system (without human-written code templates) to a workshop at ICLR 2025, in cooperation with the workshop organizers under a double-blind peer review process. The submitted papers underwent the standard peer-review evaluation by human reviewers, who scored them on criteria such as scientific soundness, clarity, novelty, quality of experiments, and presentation. One of the three manuscripts earned an average reviewer score of 6.33 (with individual ratings 6, 7, and 6), placing it approximately in the top 45% of submissions—above the average acceptance threshold—thus marking the first time a fully AI-generated paper successfully passed a human scientific peer-review process.</p> </li> <li> <p><strong>Biomni</strong> <d-cite key="huang2025biomni"></d-cite> is a general-purpose biomedical AI agent, published by Stanford as a bioRxiv preprint in June 2025. It uses a tool-augmented (ReAct+Code) approach with a unified biomedical action space consisting of around 150 specialized tools, 59 databases, and 105 software packages. The authors evaluated the agent’s performance on established biomedical benchmarks, such as HLE <d-cite key="phan2025humanitysexam"></d-cite> and LabBench’s DbQA and SeqQA <d-cite key="laurent2024labbenchmeasuringcapabilitieslanguage"></d-cite>, both of which are multiple-choice question-answering data sets, in line with traditional evaluation protocols. Additionally, they evaluated their agent on eight biomedical tasks, namely rare disease diagnosis, drug repurposing, patient gene prioritization, variant prioritization, GWAS (Genome-Wide Association Study) causal gene detection, CRISPR (a gene editing technology) perturbation screen design, single-cell RNA-sequence annotation, and microbiome disease-taxa analysis. Looking in depth into the available data sets on <a href="https://huggingface.co/datasets/biomni/Eval1">huggingface</a>, each task contains sample sizes between 10 and 50 entries, many of which follow template-based formats. Overall, biomni unifies a diverse landscape of biomedical tools into a unified framework, enabling seamless knowledge grounding and accelerating complex scientific reasoning.</p> </li> <li> <p><strong>Agent Laboratory</strong> <d-cite key="schmidgall2025agent"></d-cite> published by ETH Zurich at EMNLP in November 2025, examined whether autonomous agents can conduct end-to-end research workflows. It used 5 research questions from the fields of NLP and computer vision to produce 15 papers by three LLM backends, GPT-4o, o1-mini, and o1-preview. The generated reports were evaluated by human reviewers according to experimental quality, report quality, and usefulness. In addition, they used NeurIPS scores for the criteria: quality, significance, clarity, soundness, presentation, contribution, and overall. Human and automated reviewer scores were evaluated side by side. They then computed scores for cost, time, and success rate of subtasks. Such scores, however, do not capture the scientific quality, originality, or usefulness of the generated research output but rather quantify the computational and data efficiency of the agent.</p> </li> <li><strong>KOSMOS</strong> <d-cite key="mitchener2025kosmos"></d-cite> published by Future House as arXiv preprint in November 2025, spans multiple scientific domains—metabolomics, materials science, neuroscience, and statistical genetics. The technical report highlights seven discoveries. KOSMOS takes as input a research objective and a data set, both provided by a human scientist. KOSMOS then attempts to complete the research objective by using LLMs, data analysis agents, literature search agents, and a “world model” to perform iterative discovery cycles. Three of its discoveries independently reproduced findings from preprinted or unpublished manuscripts that were not accessible to KOSMOS. The other four discoveries marked truly novel contributions to the scientific literature: two supporting existing findings with novel methods, one developing a new method, and one providing a novel discovery not previously identified by human researchers. The authors used the following ways to evaluate KOSMOS: <ul> <li>Expert auditing of sample statements: The authors collectively extracted 102 claims out of three generated reports and had domain-expert scientists classify each as “Supported” vs. “Refuted” — i.e. whether the claim could be replicated by independent analysis or found in the literature. The 79.4% accuracy suggests that many of the statements were meaningfully supported by data or literature as a human-grounded measure of reliability.</li> <li>Estimating human-equivalent research effort: They measured how much “work” KOSMOS did in each run, e.g., how many lines of code were written or how many papers read. A typical run wrote ~42,000 lines of code and read ~1,500 papers. This suggests a KOSMOS run to equal ~4.1 “expert-months” of human work. Such metrics, however, do not capture the scientific validity of the generated outputs, nor the smartness or productivity of the agent.</li> <li>Domain-expert evaluation: The four novel discoveries were checked by domain expert collaborators, who verified that the reasoning, code, and citations made sense, but didn’t fully reproduce the results with their own experiments. Therefore, the KOSMOS discoveries can be seen as interesting, potentially valuable ideas that passed initial expert screening. <br/> Importantly, the KOSMOS paper acknowledges that evaluating which insights truly matter still depends on substantial human effort, as each report contains several discovery narratives, each with dozens of claims, and no automated method exists to judge accuracy, novelty, or significance.</li> </ul> </li> </ol> <blockquote> <p>These examples illustrate our point that while AI scientists appear formidable within their own familiar environment and domain, we lack a standardized arena in which to objectively evaluate the extent of their actual capabilities and their future potential for scientifically useful contributions.</p> </blockquote> <p>All of the above examples used different and mutually incomparable evaluation metrics and protocols. Some agents were assessed using traditional-looking benchmarks like GPQA <d-cite key="rein2023gpqagraduatelevelgoogleproofqa"></d-cite>, HLE, and LabBench. These are multi-choice question-answering benchmarks that gauge recall and reasoning but fail to capture scientific novelty, creativity, or usefulness. Others used internal auto-metrics like Elo-style self-play for hypothesis assessment, or human-evaluated metrics such as novelty and plausibility on very small sample sizes. In some cases, isolated claims extracted from generated output were verified by domain experts or experiments, while others had experts score full papers without validating or reproducing the results. This was complemented by an array of ad-hoc numerical metrics like cost, time, task-completion success rate, lines of code generated, papers read, and human-equivalent effort.</p> <p>While each of these metrics is valuable by itself, there is no community consensus on which subset of them to minimally report in order to make results comparable. Standardized reporting is, however, needed to meaningfully judge performance, create fair competition, and render results reproducible. All of these are prerequisite to the scientific method. Defining shared, domain-specific problems, task structures, and unified evaluation paradigms would bridge the current fragmentation and improve communication within the AI community.</p> <h2 id="conclusion">Conclusion</h2> <p>The above examples serve to illustrate the two main points we wish to emphasize: (1) Recent long-form AI tools, exemplified by “AI-Scientists”, generate potentially unbounded output with no clear right/wrong dichotomy. (2) Every team evaluates their system with different, often purpose-made evaluation protocols. We lack a scalable standard evaluation framework for such outputs. This indicates that a structural transition in AI is underway. Historically, evaluation in machine learning relied on large sample sizes (often ~10,000 or more) because models produced short, paragraph-length output, and generating thousands of samples was computationally inexpensive. With contemporary LLMs capable of producing research-grade documents, multi-page analyses, and even book-length content, the computational cost of each sample has increased dramatically. As a result, evaluating 10,000 such outputs is no longer feasible, neither for model generation nor for human or hybrid assessment.</p> <p>Consequently, the field is shifting away from traditional large-N statistical generalization toward a different evaluative paradigm—one centered on assessing a model’s capacity for sound reasoning, cross-domain competence, trustworthiness, robustness to manipulation, and broader cognitive generalization. This transition is not a problem in itself, but it does represent a fundamental change in what counts as evidence for model capability, and it requires us to rethink the epistemic foundations of evaluation in the age of AI-generated scientific work.</p> <blockquote> <p>This blogpost is a timely wake up call to build a standard evaluation protocol for AI-driven science well before systems reach Artificial Super Intelligence (ASI).</p> </blockquote> <p>Without such a protocol, the path toward ASI becomes substantially more severe. The pre-ASI systems may generate elegant-looking unreliable theories, lead to massive accumulation of unverifiable claims, and production of detrimental artifacts. Even if ASI does emerge without a prior evaluation framework and is capable of developing its own evaluation methods, it would be unreasonable to assume that those methods will align with the norms and goals of human scientific institutions. Therefore, developing a standard evaluation protocol is essential both to guide development and to prevent a situation where one system dictates both what is true and controls the mechanisms for verifying it.</p> <h2 id="now-and-next">Now And Next</h2> <p>Given the risks associated with flying blind in AI-Science, it is reassuring to see that recent research increasingly highlights the need for a standardization of evaluation protocols for AI-Scientist systems <d-cite key="luo2025automateseehiddenpitfalls"></d-cite>. While there is no consensus yet, awareness is growing and progress is accelerating. Recent data sets and benchmarks, like IdeaBench <d-cite key="guo2024ideabenchbenchmarkinglargelanguage"></d-cite> (biomedical research ideas), HypoBench <d-cite key="liu2025hypobenchsystematicprincipledbenchmarking"></d-cite> (generic hypothesis discovery), and ScienceAgentBench <d-cite key="chen2025scienceagentbenchrigorousassessmentlanguage"></d-cite> (scientific agent performance) constitute an incremental but much needed advancement toward standard evaluation frameworks. Looking ahead, promising solutions include domain-specific shared tasks that enable communities to converge around common challenges, as well as benchmarks designed with strict evaluation-only data sets to prevent data leakage. Standardized evaluation will likely rely on multi-metric scorecards rather than a single scores, combining measures of correctness and task success (e.g., accuracy, discovery rate), novelty and impact (e.g., expert- and model-assessed plausibility), efficiency (e.g., cost and wall-clock time), robustness and reproducibility, safety and governance, and the usefulness of human-AI collaboration. To reduce cherry-picking and ensure transparency, researchers may also be expected to release full trace logs and code as evaluation artifacts. Future directions include exploring cross-disciplinary metrics and experimental workflows that can reliably verify AI-generated scientific output. Additionally, LLM as a judge and meta-evaluation of those judges would enable scalable, responsible, and reliable evaluation of increasingly autonomous scientific AI systems.</p>]]></content><author><name>Anonymous</name></author><category term="long-form-research-reports"/><category term="science"/><category term="AI-Scientist"/><category term="evaluation"/><summary type="html"><![CDATA[As AI systems begin drafting full research reports, our long-standing evaluation mindset is hitting its limits. We are used to benchmarking models on massive data sets with well-defined, comparable metrics. But modern AI-generated science is now judged on only a small number of long, open-ended research outputs, making traditional notions of generalization hard to verify. In the absence of standard evaluation frameworks, researchers find themselves creating case-specific evaluation criteria. This blog is a wake-up call, a look at how quickly LLM-based scientific agents are outgrowing our inherited evaluation paradigms, and why we must rethink our long-held assumptions to build rigorous and standardized ways of assessing this new form of AI-driven scientific work.]]></summary></entry><entry><title type="html">Can Coding Agents be General Agents?</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/coding-agents/" rel="alternate" type="text/html" title="Can Coding Agents be General Agents?"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/coding-agents</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/coding-agents/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Immediately after the emergence of Transformer-based Language Models (LMs), researchers and developers began exploring LM code generation.<d-cite key="ahmad2021plbart"></d-cite> With heavy investment into training models specifically on coding data, LMs have seen dramatic improvements in coding capabilities.<d-cite key="chen2021codex"></d-cite> The top score on SWE-Bench Verified, a benchmark testing models on real-world software engineering tasks, jumped from 49% to 78% through 2025.<d-cite key="jimenez2023swebench"></d-cite><d-cite key="openai2024swebenchverified"></d-cite> The newer Terminal Bench, released in May of 2025, has also tracked frontier LLMs improving from 43% to 61% on complex tasks in the terminal, including analyzing data, calling APIs, and addressing security vulnerabilities.<d-cite key="terminalbench2024"></d-cite></p> <p>Propelled by compounding improvement, frontier labs have been developing coding agents, which augment foundation models with a shell sandbox and code editor to help with coding tasks. These agents, including Claude Code, Codex CLI, and Gemini CLI, have seen explosive developer adoption: since launching in May 2025, Claude Code now receives over 4.4 million downloads every week.<d-cite key="anthropic2025claudecode"></d-cite></p> <p>Surprisingly, people are using these coding agents for purposes far beyond the realm of software development. Users report applying coding agents to tax preparation, creating content, personal knowledge management, and more.<d-cite key="steipete2025claudecomputer"></d-cite> At their core, “coding” agents are versatile: even Anthropic has acknowledged this shift, rebranding its Claude Code SDK to the general “Agent SDK.”<d-cite key="anthropic2024agentsdk"></d-cite> This makes sense: knowledge work takes place entirely in software - business apps, browsers, spreadsheets, databases. Coding agents are naturally fit to meet knowledge work where it lives.</p> <h3 id="how-does-a-coding-agent-work">How Does a Coding Agent Work?</h3> <p>Broadly, an “AI Agent” is a system that autonomously pursues a goal by perceiving its environment, reasoning iteratively, and taking actions, with minimal human intervention. Typically, agents take action through manually predefined tools.<d-cite key="schick2023toolformer"></d-cite> Often, these tools include complex business logic that reduces the reasoning burden on the LM itself. For example, a simple banking agent may be provided: “read_balance,” “withdraw_money,” “deposit_money,” and “transfer_funds.” In practice, each of these tools would include guardrails to prevent prohibited transactions or incorrect calculations.</p> <p>Coding agents are a specific type of agent designed for software engineering tasks. These agents operate within software development environments (repositories, IDEs, sandboxes, terminals). <em>Coding agents are unique because they write, execute, and debug their own scripts at runtime,</em> <em>instead of being limited to a pre-defined set of tools.</em> They are especially versatile: they can quickly orient themselves in new software environments by querying for information, installing packages to unlock new capabilities, and resolving errors from logs. This self-governed feedback loop unlocks utility beyond software development: coding agents can theoretically plug-and-play into <em>any</em> software environment—offering an interesting pathway to generalizability.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/1_Coding_Agent_Architecture-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/1_Coding_Agent_Architecture-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/1_Coding_Agent_Architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/1_Coding_Agent_Architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 1: The coding agent is an emergent agent architecture, which allows a model to create its own code tools at runtime and iterate with the help of code outputs and error logs. </div> <p>Experimentally, past research has shown that training models on code improves general reasoning. For instance, Aryabumi et al. find that adding code data to pre-training yields up to 8.2% relative improvement on natural language reasoning benchmarks.<d-cite key="aryabumi2024codepretraining"></d-cite> Separately, Wang et al. demonstrate that their CodeAct agent, using executable Python scripts, outperformed JSON and text-based tool-calling alternatives with a 20% higher success rate on non-coding tasks.<d-cite key="wang2024codeact"></d-cite> Furthermore, the continued scaling of test-time compute means coding agents are increasingly adept at attacking multi-hour, project-scale tasks.<d-cite key="openai2025gpt51"></d-cite> Practically and experimentally, there is visible potential in applying coding agents to general tasks.</p> <p>Given this convergence, we ask: <strong>Can coding agents succeed as general business agents? And if not yet, where do they break down?</strong></p> <p>This post makes three contributions:</p> <ol> <li><strong>A framework for coding-agent generalization.</strong> <br/> We propose that success as a general business agent requires bidirectional translation between business/domain and code/software layers, and articulate four concrete capabilities this entails.</li> <li><strong>An evaluation gap analysis.</strong> <br/> We survey prominent benchmarks and show that code-level evals (SWE-bench, Terminal-Bench) lack business context while domain-reasoning evals (τ-bench<d-cite key="yao2024taubench"></d-cite>, BFCL<d-cite key="gorilla2024bfcl"></d-cite>) lack complex code execution - leaving full-stack translation underexplored.</li> <li><strong>Observations from an ERP case study.</strong> <br/> We deploy frontier coding agents on a production-grade Enterprise Resource Planning (ERP) software instance with multi-constraint Sales-to-Fulfilment and HR operational tasks, document distinct failure modes at the business-code boundary, and propose asymmetric feedback from the environment to explain persistent agent overconfidence.</li> </ol> <h2 id="defining-success">Defining Success</h2> <p><strong>In order to be a reliable general agent, a coding agent must be excellent at translating between the business/domain layer and the code/software layer.</strong> In practice, that means it can:</p> <ol> <li> <p>Understand business‑level requests and policies: turning instructions like “approve this expense if it fits our travel policy and budget” into precise goals and constraints.</p> </li> <li> <p>Inspect the current system state via generated and executed code: writing and running queries or scripts to see what requests, data, and approvals already exist across the relevant systems.</p> </li> <li> <p>Plan a business‑level solution: deciding, at the domain level, what should actually happen (approve, modify, escalate, or reject) given the policies and current state.</p> </li> <li> <p>Encode that plan back into the software: implementing the decision as code/API calls that update the system state to match the intended business outcome.</p> </li> </ol> <h2 id="what-gets-measured-gets-improved">What Gets Measured Gets Improved</h2> <p>It’s worth examining how we evaluate frontier models and agents today. <strong>Current evaluations largely fall into two camps: those that test code-level competence and those that test business/domain-level reasoning</strong>. The full-stack, bi-directional <em>translation</em> between them is undercovered.</p> <h3 id="code-and-system-level-benchmarks">Code and System-Level Benchmarks</h3> <p><strong>SWE-Bench</strong> tests whether models can resolve real GitHub issues by generating code patches. Given a repository snapshot and an issue description like <em>“DatetimeIndex.to_period() fails with timezone-aware timestamps”</em>, the agent must locate the bug across thousands of files, generate a fix, and pass the repository’s unit tests. The input is technical; the output is technical; the evaluation is whether tests pass. SWE-Bench has become a standard because it captures real software engineering difficulty, but the “business context” facet is thin.</p> <p><strong>Terminal-Bench</strong> takes a similar approach for evaluating terminal mastery. Tasks range from <em>“Build Linux kernel 6.9 from source with a custom printk message”</em> to <em>“Configure a git server that pushes to a webserver on port 8080.”</em> Agents interact with a sandboxed Linux terminal and are evaluated with task-specific checks: whether files exist, services respond, and commands succeed. Again, the request is system-level, and the evaluation is system-level.</p> <h3 id="tool-use-and-domain-reasoning-benchmarks">Tool-Use and Domain Reasoning Benchmarks</h3> <p>Tool-use benchmarks like <strong>The Berkeley Function Calling Leaderboard (BFCL)</strong> evaluate whether a model can turn prompts and pre-defined function specifications into <em>correct function calls</em> across many domains. For example, if tasked with calculating a five-year compounding return and given a tool set containing a “calculate_compound_interest” function, the agent must locate that tool, identify the principal, interest rate, and time frame from context, and correctly pass them as arguments into the function.</p> <p>BFCL measures how often models get tool names and arguments right, probing the flow from business requests to tool selection and arguments, but without complex code execution or policy adherence behind it.</p> <p><strong>τ-bench</strong> pushes further by adding policy compliance. An airline agent may handle a user request: <em>“I want to change my reservation to a different destination.”</em> The agent must gather information through dialogue, consult policy documents about change fees and cabin-class restrictions, and call domain APIs correctly to make the change. This is close to what a real deployment would require, but the underlying, synthetically-generated database and API are intentionally much simpler than real-world business software.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/2_Critical_Evaluation_Gap-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/2_Critical_Evaluation_Gap-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/2_Critical_Evaluation_Gap-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/2_Critical_Evaluation_Gap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 2: The critical evaluation gap: Code evaluations lack business context; business evaluations lack code interaction. </div> <p>Existing evaluations usually stress one or two capabilities at a time: choosing tools, business reasoning, writing code, or manipulating a complex software system. Few require an agent to do everything end-to-end.</p> <h1 id="case-study-coding-agent-in-enterprise-resource-planning">Case Study: Coding Agent in Enterprise Resource Planning</h1> <p>To cover this gap, we ask: <em>Can a coding agent run business processes end-to-end, under realistic constraints, by writing and executing its own code against a live ERP instance?</em></p> <h2 id="setup">Setup</h2> <p>An ERP is the single source of truth for businesses’ operations, supporting more than 3.8 million companies worldwide and, by some estimates, underpinning 77% of the world’s transaction revenue. We identified the ERP as our ideal testing ground for “general” business tasks because it contains nearly all core business functions - finance, HR, supply chain, and customer relationship management - as native, interdependent modules. In this environment, we can test how the agent completes simple, single-module tasks like onboarding an employee, but also complex, cross-module tasks like fulfilling a customer order end to end. Furthermore, the ERP has certain validation rules built into its software, while a majority of business logic still remains as the agent’s burden.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/3_Example_ERP_Process-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/3_Example_ERP_Process-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/3_Example_ERP_Process-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/3_Example_ERP_Process.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 3: An example of a process in an ERP, involving an order-to-delivery workflow. </div> <h3 id="environment">Environment</h3> <p>We built a fictional company inside Odoo 19.0 Community Edition, the open-core ERP with over 12 million users across SMBs and large enterprises. Odoo is particularly well suited for this research: unlike most proprietary business apps, it is free to self‑host, easy to spin up in many parallel sandboxed instances, and provides visibility into the underlying PostgreSQL database. This gives us a realistic “world model” of business operations (sales, inventory, manufacturing, HR, purchasing) while still allowing fine-grained control. To mimic a production environment, we populate the Odoo instance with complete company data - products, vendors, price lists, bills of material, lead times, etc.</p> <p>We containerize the environment, agent execution, and verification modules separately to prevent information from the verifier from contaminating agent reasoning and to facilitate reproducibility.</p> <h3 id="task-structure">Task Structure</h3> <p>We test the agent’s ability to complete real-world business workflows that are commonly found in an ERP. Each task gives the agent a natural-language instruction with an objective, constraints, and a policy rulebook. For example:</p> <p>Instruction: <em>TechStart Solutions needs 40 ergonomic chairs within 7 days (budget: <code class="language-plaintext highlighter-rouge">$12,000</code>). DesignHub Agency needs 30 chairs within 10 days (budget: <code class="language-plaintext highlighter-rouge">$9,000</code>). Handle both orders, allocating stock and creating manufacturing orders as needed.</em></p> <p>Policy: <em>Maintain at least 25% gross margin. Minimize procurement costs. You are not allowed to increase the list price of the finished goods.</em></p> <p>In each scenario, the agent has to make at least 2 interdependent business decisions while being subject to at least 2 interacting operational constraints. To pass the example scenario above, the agent must query the ERP, determine the optimal fulfillment strategy, and document its solution (as confirmed sales, manufacturing, and purchase orders) - all through writing and executing code at runtime.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/4_Simple_Scenario_Visualization-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/4_Simple_Scenario_Visualization-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/4_Simple_Scenario_Visualization-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/4_Simple_Scenario_Visualization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 4: A visualization of the decisions present in a simplified scenario. The solution to this task requires multiple choices and execution steps within the ERP. </div> <h3 id="agent-harness">Agent Harness</h3> <p>We developed a simple coding agent harness for our trials. The harness includes only a bash tool, which is how the agent writes and executes code scripts to interact with the Odoo environment. We provide the agent with database credentials, an isolated workspace for temporary files, and rudimentary examples of five Odoo data models. We do not provide comprehensive documentation on the Odoo instance and API. This compels the agent to iteratively map out the ERP environment from scratch.</p> <p>We test the agent with GPT-5 and Claude Sonnet 4.5, using maximum allowed reasoning effort/thinking budget settings.</p> <h3 id="evaluation">Evaluation</h3> <p>After each trial, our task verifier compares the PostgreSQL database with ground-truth rubrics that define the correct database end-state. Our rubric considers the following:</p> <ol> <li> <p>Constraint Resolution: Did the agent satisfy all constraints posed by the task instruction and data loaded into the ERP instance?</p> </li> <li> <p>Resource Optimization: Did the agent solve the task optimally? E.g. figured out the most cost-effective fulfillment plan.</p> </li> <li> <p>Traceability: Are the resulting data objects linked correctly in the ERP? E.g. procurement orders are linked to the sales orders they fulfill.</p> </li> <li> <p>Policy Adherence: Did the agent follow all the rules outlined in the policy rulebook provided to it in the prompt?</p> </li> </ol> <h2 id="results">Results</h2> <h3 id="success-out-of-the-box">Success Out-Of-The-Box</h3> <p>In the first trial of 10 easy scenarios, our coding agent using Claude Sonnet 4.5 reliably scores above 80% on the verifiers. These simple tasks include creating sales orders for one or two customers, selecting the cheapest vendor, and generating invoices. This consistent accuracy is impressive: even given limited documentation, the agent intuitively issued correct API calls and navigated Odoo’s data model.</p> <p>The coding agent was so successful on these tasks that, in order to challenge the agent, we needed to scale up the complexity of the tasks to require the agents to make 5+ decisions and weigh 5+ constraints, at which point many domain-level resource allocation tasks became challenging even for humans to work out without computational assistance.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/5_Results_Chart-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/5_Results_Chart-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/5_Results_Chart-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/5_Results_Chart.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 5: We test Claude Sonnet 4.5 and GPT-5 on 20 scenarios across a gradient of difficulty from Easy to Hardest. We observe that increasing the complexity of the scenarios by increasing the number of constraints results in a breakdown of performance. We separate our evaluation of each scenario into 4 dimensions: Constraint Resolution, Resource Optimization, Traceability, and Policy Adherence. Noticeably, GPT-5 tended to come up with comparable or even better-quality business plans as Claude 4.5 Sonnet, but GPT-5 struggled more with correct API calls, which caused lower scores across the board. </div> <h3 id="failure-modes">Failure Modes</h3> <p>As complexity increased, characteristic failures began to emerge.</p> <p>Certain issues came from the business reasoning side alone. Initially, in “medium” and some “hard” scenarios, the agent produced solutions that were feasible but suboptimal. For instance, all or most constraints were satisfied, but the agent failed to calculate the most cost-effective outcome. Eventually, for the remaining “hard” and “hardest” scenarios, the agent stops satisfying constraints altogether. The agent’s traceability also degrades with additional complexity as it neglects maintaining documentation of its more complex decisions.</p> <p>Beyond simple reasoning failures, however, we started to see a breakdown in the agent’s ability to maintain coherence between the business level and the code level.</p> <h4 id="lazy-code-heuristics">Lazy Code Heuristics</h4> <p>One class of such problems can be described as “lazy code heuristics” that don’t accurately implement business logic.</p> <p>The most salient example involved a task policy to <em>only order goods and components from American vendors</em>. Rather than filtering vendors by address, the agent used a glaringly wrong lazy heuristic based on the vendor’s name.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="code"><pre><span class="c1"># Policy in the request:
#   "...order only from American vendors..."
# Relevant Odoo data model:
#   res.partner
#     ├── name
#     ├── contact_address          
#     ├── Country Info
#     │     ├── country_code    
#     │     └── country_id                         
#     └── Company Info (...)
</span>
<span class="c1"># Code the agent wrote to filter vendor list
</span><span class="n">is_american</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sh">"</span><span class="s">American</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">vendor</span><span class="p">.</span><span class="n">name</span>
    <span class="ow">or</span> <span class="sh">"</span><span class="s">Northern</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">vendor</span><span class="p">.</span><span class="n">name</span>
    <span class="ow">or</span> <span class="sh">"</span><span class="s">Catalyst</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">vendor</span><span class="p">.</span><span class="n">name</span>
    <span class="c1"># etc.
</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>The agent correctly understands the natural‑language policy (“order only from American vendors”) but implements it with a crude string‑matching shortcut. The business intent is correct, but the code that is supposed to enforce it is wrong.</p> <h4 id="hallucinations-in-the-business-layer">Hallucinations in the Business Layer</h4> <p>We also observed the case where the agent hallucinates at the business layer, causing it to write functional code that ultimately leads it astray. In one case, the agent is tasked with scrapping faulty LED Boards that were found to have condensation damage. The agent asserts that if the boards have water damage, they <em>must</em> be stored in a fridge, even though the setup actually has them in the warehouse with all others. When it queries the imaginary “fridge” location, it finds no results, so it assumes those boards have been discarded.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/6_LED_Hallucination-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/6_LED_Hallucination-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/6_LED_Hallucination-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/6_LED_Hallucination.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 6: The agent hallucinates that damaged LED boards must be stored in the fridge, without any supporting evidence in the system, leading to functional-but-misleading code. </div> <p>The direction of failure flips from the previous case. The code is locally coherent given the agent’s belief that there is a “Fridge” storage location. The query execution makes sense, but the underlying reasoning is hallucinated.</p> <h4 id="ignored-policy-constraints">Ignored Policy Constraints</h4> <p>In some runs, agents failed to internalize explicit rulebook constraints at all.</p> <p>For an employee vacation request HR scenario, a policy stating that “<em>days off should be consecutive</em>” was simply ignored during reasoning, leading to prohibited fragmented schedules.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-coding-agents/7_HR_Policy_Violation-480.webp 480w,/2026/assets/img/2026-04-27-coding-agents/7_HR_Policy_Violation-800.webp 800w,/2026/assets/img/2026-04-27-coding-agents/7_HR_Policy_Violation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-coding-agents/7_HR_Policy_Violation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-start"> Figure 7: The agent disregards the policy requirement to only schedule consecutive days off, and incorrectly claims successful completion. </div> <p>In more complex procurement tasks, margin requirements and lead-time rules were dropped despite being queried correctly from the ERP. Here, the break happens one step earlier: the agent simply does not carry certain rules forward through its reasoning at all.</p> <h4 id="overconfidence">Overconfidence</h4> <p>Whether the solution was optimal, suboptimal, or incorrect, one thing remained constant: the agent almost always reported success and remained unaware of its shortcomings.</p> <p>This reflects an asymmetry in the feedback profile of the environment. At the code level, the agent receives concrete signals: bad imports, malformed payloads, and incorrect field names, which all produce exceptions. Conversely, we observe that at the business level, feedback is sparse. The inbuilt ERP guardrails reject obviously invalid and ill-formatted interactions but won’t flag suboptimal choices or policy violations. Only when we run the task verifier do we see that the outcome was wrong.</p> <p>This pattern can be viewed as a form of specification gaming: the agent optimizes for the measurable proxy (code execution) rather than the true objective (business correctness). Recent work demonstrates that gaming behaviors generalize, models trained to exploit easily-discovered reward signals will zero-shot transfer those behaviors to novel environments.<d-cite key="denison2024sycophancy"></d-cite> Coding agents may be particularly susceptible: their training provides dense, unambiguous feedback at the code layer (tests pass, scripts execute, errors resolve), effectively teaching that execution success equals task success. This learned prior does not transfer when code is merely the <em>medium</em> for a business objective rather than the objective itself. These silent failures point to the untrustworthiness of the agent’s own pass/fail conclusions.</p> <h2 id="synthesis">Synthesis</h2> <p>The most striking result is that the coding agent can immediately complete straightforward tasks out-of-the-box, at near-human efficacy. With no ERP-specific tooling, the agent reliably executed real business workflows - creating orders, selecting vendors, generating invoices - that would typically require dedicated integrations.</p> <p>Let us revisit our criteria for a “successfully” generalizable coding agent: (1) the agent was able to understand our instructions, (2) inspect the undocumented environment state, (3) reason through the business-level solution, and (4) write a series of scripts to perfectly complete the task and match our verifiers. If all real-world tasks were this simple, we could label the coding agent as “general” already.</p> <p>But when scenarios were loaded with complex decisions and constraints, failures emerged in ways disconnected from raw coding ability. The four failure modes are all different ways of breaking the business-code translation:</p> <ul> <li>Lazy heuristics: correct understanding, incorrect implementation</li> <li>Hallucinations: correct code, incorrect world model</li> <li>Ignored constraints: rules dropped before reasoning even begins</li> <li>Overconfidence: code-level success mistaken for task-level success</li> </ul> <p><strong>In order to get coding agents to generalize, we should measure and optimize code and business-level correctness together.</strong></p> <h2 id="conclusion">Conclusion</h2> <p>Coding agents represent a promising path toward general-purpose AI. They are already widely adopted, rapidly improving, and operate in the same medium through which most white-collar work already flows: software.</p> <p>After examining performance in our simulations, we find that coding agents have advantageous traits that help them succeed in settings where the goal is not to write code, but to achieve correct real-world outcomes <em>through</em> code in complex systems. If these agents can learn to reason reliably across unmapped business domains while retaining their native fluency in code, the distance to broad automation shrinks considerably.</p> <p>Today, coding agents still do not generalize to complex business workflows. Domain-specific tools and guardrails will remain important in the near term - but they address failure modes one instance at a time. This risks running afoul of the bitter lesson<d-cite key="sutton2019bitterlesson"></d-cite>, and history suggests that general methods eventually win.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[As coding agents have seen rapid capability and adoption gains, users are applying them to general tasks beyond software engineering. In this post, we investigate whether coding agents can successfully generalize to end-to-end business process automation. We identify gaps in current evaluations, and conduct a case study to evaluate a coding agent on practical business tasks in an open-core Enterprise Resource Planning system. We find that the agent reliably completes simple tasks but exhibits characteristic failures on complex tasks, suggesting that bridging domain logic and code execution is a key bottleneck to generalizability.]]></summary></entry><entry><title type="html">Diffusion Guidance - Opportunities for Physical Sciences</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-guidance/" rel="alternate" type="text/html" title="Diffusion Guidance - Opportunities for Physical Sciences"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/diffusion-guidance</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-guidance/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="introduction">Introduction</h2> <p>Diffusion models have emerged as a state-of-the-art approach for sampling from complex probability distributions. Prominent examples are image-generation models like Stable Diffusion, where the model generates a high-quality image within seconds based on a given text prompt like <em>“A corgi with sunglasses on the beach”</em>. What makes these models stand out is their ability to faithfully adhere to a given prompt.</p> <p>These capabilities arise from techniques collectively known as guidance, which direct the output of diffusion models toward specified conditions. To guide the models, we use a score function divided into a <strong style="color: #25a18e;">prior</strong> and <strong style="color: #00a5cf;">likelihood term</strong>:</p> \[\begin{equation*} \textcolor{#A125A1}{\nabla_{\mathbf{x}} \log \, p(\mathbf{x} \mid \mathbf{y})} = \textcolor{#25a18e}{\nabla_{\mathbf{x}} \log p(\mathbf{x})} + \textcolor{#00a5cf}{\nabla_{\mathbf{x}} \log p(\mathbf{y} \mid \mathbf{x})} \end{equation*}\] <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/score_decomposition.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/score_decomposition.svg" class="img-fluid" width="100%" height="auto" style=" max-width: 25%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>This blog post aims to share insights into methods that go beyond traditional guidance techniques. We begin by explaining the fundamentals of the classifier guidance approach in the first section and then explore recent developments in guiding diffusion models. Our goal is not to favor any specific method but to present alternatives, especially useful when data is limited or training resources are constrained.</p> <h2 id="background">Background</h2> <p>In this section, we will briefly summarize the key features of diffusion models. Readers familiar with the score-based formulation might want to skip ahead.</p> <h3 id="diffusion-models">Diffusion Models</h3> <p>A <em>forward</em> diffusion process, which gradually destroys data over time, can be defined via a stochastic differential equation (SDE)<d-cite key="song2021generative"></d-cite>:</p> \[\begin{equation} d\mathbf{z}_t = \mathbf{f}(\mathbf{z}_t, t) \; \mathrm{d}t + \mathrm{g}(t) \; d\mathbf{w} \textrm{,} \end{equation}\] <p>where $\mathbf{f}$ and $\mathrm{g}$ are determined by the noise schedule, and $d\mathbf{w}$ is a Brownian motion. For an affine drift $\mathbf{f}$, the forward process can be rewritten in closed form:</p> \[\begin{equation} \label{eq:diff_co} \mathbf{z}_t = \alpha_t \mathbf{x} + \sigma_t \boldsymbol{\epsilon} , \end{equation}\] <p>with $\mathbf{x} \sim p_{\text{data}}(\mathbf{x})$ sampled from the data distribution and $\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ drawn from a standard gaussian distribution. The forward diffusion processes starts with a clean data sample $\mathbf{z}_0=\mathbf{x}$ (where $\alpha_0=1$ and $\sigma_0=0$), and gradually adds noise. In the case of a variance-exploding noise schedule, $\alpha_t$ remains $1$ and $\sigma_t$ grows with $t$.</p> <p>The <em>reverse</em> process<d-cite key="song2021generative,anderson1982reverse"></d-cite> is also an SDE running backward in time:</p> \[\begin{equation} \label{eq:reverse_process} \mathrm{d}\mathbf{z}_t = [\mathbf{f}(\mathbf{z}_t, t) - \mathrm{g}(t)^2 \; \textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)}]\; \mathrm{d}t + \mathrm{g}(t) \; d\mathbf{w} , \end{equation}\] <p>where \(\textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)}\) is called the score function (aka Stein score), and is approximated by a neuronal network \(s_{\theta}(\mathbf{z}_t, t) \approx \nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)\) using denoising score matching<d-cite key="hyvarinen2005estimation"></d-cite>.</p> <p>The score function defines a <strong>time-dependent vector field</strong> that guides points toward the data distribution.</p> <h3 id="conditional-diffusion-models">Conditional Diffusion Models</h3> <p>Previously, we demonstrated how to create a process for sampling from an unconditional distribution. Extending this to the conditional case, we aim to sample from a posterior $p(\mathbf{x} \mid \mathbf{y}) \text{,}$ which we can decompose using Bayes’ rule:</p> \[\begin{equation} p(\mathbf{x} \mid \mathbf{y}) = \frac{p(\mathbf{y} \mid \mathbf{x}) p(\mathbf{x})}{p(\mathbf{y})} . \end{equation}\] <p>Applying the logarithm and differentiating with respect to $\mathbf{x}$, allows us to define a conditional form of the score function (relying on the fact that the denominator does not depend on $\mathbf{x}$):</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{x}} \log \, p(\mathbf{x} \mid \mathbf{y})} = \textcolor{#25a18e}{\nabla_{\mathbf{x}} \log p(\mathbf{x})} + \textcolor{#00a5cf}{\nabla_{\mathbf{x}} \log p(\mathbf{y} \mid \mathbf{x})} . \end{equation}\] <p>Where the conditional <em>reverse process</em> from Eq. \eqref{eq:reverse_process} is given by:</p> \[\begin{equation} \mathrm{d}\mathbf{z}_t = [\mathbf{f}(\mathbf{z}_t, t) - \mathrm{g}(t)^2 \; (\textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)} + \textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)})] \; \mathrm{d}t + \mathrm{g}(t) \; d\mathbf{w} . \end{equation}\] <p>This formulation allows us to reuse our unconditional model (\(\textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)}\)) and simply add a guidance term \(\textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)}\). The guidance term acts like a force pushing our samples to be consistent with the condition \(\mathbf{y}\). The remaining practical challenge is deriving</p> \[\textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)} .\] <p>There are two distinct strategies for doing so:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. <strong>Classifier Guidance (Learning the Likelihood)</strong>: <br/>Learn a time-dependent classifier that approximates the likelihood score through supervised training on labeled data.</p> <p style="margin: 0;">2. <strong>Analytical Likelihoods (Defining the Likelihood)</strong>: <br/>Leverage analytically tractable likelihood functions, which are particularly valuable for inverse problems, where paired training data $(\mathbf{x}, \mathbf{y})$ is scarce or unavailable.</p> </div> <p><strong>In both approaches, the diffusion model itself remains frozen</strong>, serving only as a fixed prior distribution $p(\mathbf{x})$. We are left to define or learn how to assess likelihoods, particularly their gradients, which makes this framework highly data-efficient for adapting to new tasks.</p> <h2 id="classifier-guidance-cg">Classifier Guidance (CG)</h2> <p>Classifier Guidance<d-cite key="dhariwal2021diffusion"></d-cite> trains a time-dependent neuronal network to approximate the likelihood:</p> \[\begin{equation} p_{\phi}(\mathbf{y} \mid \mathbf{z}_t, t) \approx p(\mathbf{y} \mid \mathbf{z}_t, t) . \end{equation}\] <p>We therefore train a classifier whose inputs are noisy samples that resemble the intermediate steps of the reverse diffusion process. In particular, given a dataset of paired samples $(\mathbf{x}, \mathbf{y})$, we can train a time-conditional classifier $p_{\phi}(\mathbf{y} \mid \mathbf{z}_t, t)$ by minimizing</p> \[\begin{equation*} \mathbb{E}_{t \sim \mathcal{U}(0, T), (\mathbf{x}, \mathbf{y}) \sim p_{\text{data}}, \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})}[-\log p_{\phi}(\mathbf{y} \mid \mathbf{z}_t, t)] , \end{equation*}\] <p>with the noisy sample $\mathbf{z}_t = \alpha_t \mathbf{x} + \sigma_t \boldsymbol{\epsilon}$. Handling inputs at varying noise levels allows the classifier to operate across the entire diffusion trajectory.</p> <p>The trained classifier can be used as an approximation of the <strong><span style="color: var(--color-likelihood);">log-likelihood gradient</span></strong>:</p> \[\begin{equation} \textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)} \approx \textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p_{\phi}(\mathbf{y} \mid \mathbf{z}_t, t)} , \end{equation}\] <p>where the gradient with respect to the input is easy to compute using automatic differentiation frameworks such as PyTorch or JAX.</p> <p>Finally we combine the prior score with the log likelihood gradient. In practice, classifier guidance works with a scaled version of the guidance term controlled by the parameter $\gamma$,</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{z}_t} \log \, p_{\gamma}(\mathbf{z}_t \mid \mathbf{y})} = \textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)} + \gamma \textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)} . \end{equation}\] <p>This scaling lets us adjust the guidance strength: if $\gamma$ is too high, we obtain unrealistic samples that move away from the data manifold, whereas if $\gamma$ is too low, the guidance has little effect.</p> <p>Before continuing, we want to mention that classifier-free guidance is currently more commonly used, as it often outperforms classifier guidance and does not require training a separate classifier. However, since this blog post focuses on techniques that modify sampling behavior without retraining the original diffusion model, classifier guidance serves as a better illustrative example. Additionally, the name classifier-free guidance is somewhat misleading, because a classifier is still involved, it is simply embedded implicitly within the diffusion model itself. For completeness, we have included classifier-free guidance in the expandable section below.</p> <details> <summary> Classifier-Free Guidance (CFG) </summary> <div style="color: black;"> Classifier-free guidance can be derived in a similar manner, the main difference is that the diffusion model itself acts as classifier. Applying Bayes' rule to $$ \nabla_{\mathbf{z}_t}\,p(\mathbf{y} \mid \mathbf{z}_t) $$ results in $$ \nabla_{\mathbf{z}_t}\,p(\mathbf{y} \mid \mathbf{z}_t)=\nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t \mid \mathbf{y}) - \nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t). $$ To avoid training of an unconditional and a conditional model, we train one single model with an additional condition: $$ \nabla_{\mathbf{z}_t}\,p(\mathbf{y} \mid \mathbf{z}_t)=\nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t \mid \mathbf{y}) - \nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t \mid \emptyset), $$ where we condition the model on the null token $\emptyset$, to represent the unconditional model. We can now replace the likelihood term in classifier guidance: $$ \nabla_{\mathbf{z}_t} \log \, p_{\gamma}(\mathbf{z}_t \mid \mathbf{y}) = (1 - \gamma) \nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t, \mid \emptyset) + \gamma \nabla_{\mathbf{z}_t}\,p(\mathbf{z}_t \mid \mathbf{y}) $$ </div> </details> <h2 id="analytical-likelihoods">Analytical Likelihoods</h2> <p>In the last section, we discussed how to train an additional model serving as a likelihood. For a class of problems where the relationship between the data and the observations follows a known probabilistic model, the likelihood function can be derived analytically. A canonical example are inverse problems, where we have observations $\mathbf{y}$ and we want to recreate $\mathbf{x}$. We consider observations of the form:</p> \[\begin{equation} \mathbf{y} = \mathcal{A}(\mathbf{x}) + \mathbf{n} \quad \textrm{where} \quad \mathbf{y}, \mathbf{n} \in \mathbb{R}^m, \mathbf{x} \in \mathbb{R}^n. \end{equation}\] <p>Here \(\mathbf{y}\) is our observation, \(\mathbf{x}\) is the underlying clean data, \(\mathcal{A}:\mathbb{R}^n \mapsto \mathbb{R}^m\) is the <strong>forward operator</strong> (which may be linear or nonlinear), and $\mathbf{n}$ is observation noise.</p> <p>For Gaussian noise $\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$, the likelihood is defined by:</p> \[\begin{equation} \label{eq:y-given-x-gaussian-noise} p(\mathbf{y} \mid \mathbf{x}) = \mathcal{N}(\mathbf{y}; \mathcal{A}(\mathbf{x}), \sigma^2 \mathbf{I}) \propto \exp \left(-\frac{1}{2 \sigma^2} ||\mathbf{y} - \mathcal{A}(\mathbf{x})||^2 \right) . \end{equation}\] <p>Typical inverse problems are:</p> <ul> <li> <p><strong>Inpainting</strong>, where the operator $\mathcal{A}$ acts as a mask that zeros out out certain pixels, requiring the model to fill in the missing areas.</p> </li> <li> <p><strong>Super-resolution</strong>, where the operator $\mathcal{A}$ performs a downsampling operation and the model’s task is to generate the image at high resolution.</p> </li> <li> <p><strong>Deblurring</strong>, where the operator $\mathcal{A}$ acts as a gaussian filter operation blurring out pixels.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/cover-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-guidance/cover-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-guidance/cover-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/cover.png" class="img-fluid" width="100%" height="auto" title="Posterior Sampling with Diffusion Models" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1: Source: https://dps2022.github.io/diffusion-posterior-sampling-page</figcaption> </figure> <h3 id="diffusion-posterior-sampling-dps">Diffusion Posterior Sampling (DPS)</h3> <p>Direct computation of the time-dependent likelihood gradient $\textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)}$ poses significant computational challenges. Let’s calculate the likelihood by marginalizing over $\mathbf{x}$:</p> \[\begin{equation} p(\mathbf{y} \mid \mathbf{z}_t) = \int p(\mathbf{y} \mid \mathbf{x}) \, p(\mathbf{x} \mid \mathbf{z}_t) d \mathbf{x} = \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x} \mid \mathbf{z}_t)}[p(\mathbf{y} \mid \mathbf{x})] . \end{equation}\] <p>The intractability arises from two sources: (1) the marginalization over the high-dimensional space of clean samples $\mathbf{x}$, and (2) the dependence of $p(\mathbf{y} \mid \mathbf{z}_t)$ on the reverse diffusion process, which requires integrating over all trajectories from time $t$ to $0$. DPS<d-cite key="chung2022diffusion"></d-cite> addresses this through an approximation that avoids explicit marginalization:</p> \[\begin{equation} p(\mathbf{y} \mid \mathbf{z}_t) \approx p(\mathbf{y} \mid \hat{\mathbf{x}}(\mathbf{z}_t)), \end{equation}\] <p>where \(\hat{\mathbf{x}}(\mathbf{z}_t) = \mathbb{E}[\mathbf{x} \mid \mathbf{z}_t]\). By Tweedie’s formula<d-cite key="efron2011tweedie,tweedie1957"></d-cite>, the posterior mean is given by</p> \[\begin{equation} \mathbb{E}[\mathbf{x} \mid \mathbf{z}_t] = \frac{1}{\alpha_t} (\mathbf{z}_t + \sigma_t^2 \nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)) . \end{equation}\] <p>Substituting \(\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)\) with the learned score function \(s_\theta(\mathbf{z}_t;t)\) gives the estimator:</p> \[\begin{equation} \mathbf{x}_\theta(\mathbf{z}_t;t) = \frac{1}{\alpha_t} (\mathbf{z}_t + \sigma_t^2 s_\theta(\mathbf{z}_t;t)) . \end{equation}\] <p>Instead of learning the score function, we can train a network to predict the clean data directly, i.e., act as a denoiser<d-cite key="karras2022elucidating"></d-cite>. There are three main parameterizations: score prediction, data prediction, and noise prediction. These parameterizations are mathematically equivalent and allow seamless integration of all current diffusion model formulations into this framework. <d-cite key="kingma2023understanding"></d-cite> provides an extensive analysis.</p> <p>We can now combine the Gaussian likelihood from Eq. \eqref{eq:y-given-x-gaussian-noise} with the network’s data prediction, \(\mathbf{x}_\theta(\mathbf{z}_t;t)\), to analytically obtain the log-likelihood gradient:</p> \[\begin{equation} \nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t) \approx -\frac{1}{\sigma^2} \nabla_{\mathbf{z}_t} ||\mathbf{y} - \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t))||^2_2 . \end{equation}\] <p>Again, we combine the prior and the likelihood term to obtain the score function:</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{z}_t} \log p(\mathbf{x} \mid \mathbf{z}_t)} \approx \textcolor{#25a18e}{s_\theta(\mathbf{z}_t;t)} - \textcolor{#00a5cf}{\zeta \nabla_{\mathbf{z}_t} ||\mathbf{y} - \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t))||^2_2} . \end{equation}\] <h3 id="example">Example</h3> <p>Let’s consider a simple example using the Swiss Roll data to demonstrate the effectiveness of diffusion priors in solving inverse problems. We define a linear forward operator $\mathcal{A}$ that performs a projection of the 2D data onto the \(x_1\) axis:</p> \[\begin{equation} \mathbf{y} = \mathcal{A}(\mathbf{x}) = \mathbf{A}\mathbf{x} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \end{equation}\] <p>We sample observations from the ground-truth manifold but restrict them to the left side of the Swiss Roll. Next, for each observation \(\mathbf{y}\), we sample from the posterior distribution \(p(\mathbf{x} \mid \mathbf{y})\), which places the samples back on the spiral manifold. The results are displayed in the figure below, where we have:</p> <ol> <li> <p><strong>Observations (Left)</strong>: This plot shows the observations $p(\mathbf{y} \mid \mathbf{x})$. Since the operator projects everything onto the x-axis, the unique spiral structure of the Swiss Roll is entirely gone. This creates an ill-posed inverse problem: for any observed point on this line, there are multiple possible “correct” locations on the original spiral that could have produced the outcome.</p> </li> <li> <p><strong>Standard Sampling (Right)</strong>: This represents the unconditional generation from our trained diffusion model. While these points perfectly inhabit the Swiss Roll manifold, they are random samples from the prior $p(\mathbf{x})$. They show us what the model “knows” about the data distribution, but they have no connection to the specific measurements we observed.</p> </li> <li> <p><strong>DPS Sampling (Center)</strong>: Here we show the reconstruction using Diffusion Posterior Sampling with $\zeta=0.25$. By using the measurement gradient to guide the generation process, DPS pushes \(\mathbf{x}\) to match the observation \(\mathbf{y}\). It solves the ambiguity by finding points that are both consistent with the measurement $\mathbf{y}$ and highly probable under the learned prior $p(\mathbf{x})$, recovering the spiral shape despite information loss through the operator.</p> </li> </ol> <iframe src="/2026/assets/html/2026-04-27-diffusion-guidance/measurements_dps_standard.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> <p><br/></p> <p>We note that DPS is only one possible way to estimate the likelihood term \(\textcolor{#00a5cf}{\nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t)}\). For interested readers, we recommend the excellent survey by Daras et al. <d-cite key="daras2024survey"></d-cite>, which compares a range of different approaches.</p> <h2 id="applications-for-physical-sciences">Applications for Physical Sciences</h2> <p>We now turn to scientific applications and examine how flexible the discussed approaches are in practice, particularly in settings where physical constraints must be respected. The following is just an high level overview.</p> <h3 id="diffusionpde">DiffusionPDE</h3> <p>DiffusionPDE<d-cite key="huang2024diffusionpde"></d-cite> use DPS to sample Partial Differential Equations (PDEs) solutions from sparse observation, by exploiting the structure of the underlying PDE, a guidance term can be derived to align the sampling to the underlying structure of the PDE.</p> <p>We take the <strong>Darcy flow</strong> PDE equation as an example:</p> \[\begin{aligned} -\nabla \cdot \big(a(\mathbf{c})\nabla u(\mathbf{c})\big) &amp;= q(\mathbf{c}), \quad \mathbf{c} \in \Omega, \\ u(\mathbf{c}) &amp;= 0, \quad \mathbf{c} \in \partial\Omega. \end{aligned}\] <p>Defining the residual as an operator</p> \[\begin{aligned} f(\mathbf{c}) = \nabla \cdot \big(a(\mathbf{c})\nabla u(\mathbf{c})\big) + q(\mathbf{c}), \end{aligned}\] <p>so that valid solutions satisfy $f(\mathbf{c}) = 0 $. This residual is used to construct an additional physics-based guidance loss:</p> \[\begin{aligned} \mathcal{L}_{\textrm{pde}} = ||\mathbf{0} - f(\hat{\mathbf{x}}(\mathbf{z}_t)) ||^2_2 . \end{aligned}\] <p>DiffusionPDE then augments the conditional score function as</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t \mid \mathbf{y}, f)} \approx \textcolor{#25a18e}{s_\theta(\mathbf{z}_t;t)} \textcolor{#00a5cf}{+ \zeta \nabla_{\mathbf{z}_t} \log p(\mathbf{y} \mid \mathbf{z}_t) - \zeta_{\text{pde}} \nabla_{\mathbf{z}_t} \mathcal{L}_{\text{pde}}}. \end{equation}\] <p>The sampling process is very similar to the one we saw before, combining the part we saw in DPS sampling and the additional term for the PDE solution.</p> <h3 id="inequality-constraints-for-rare-event-sampling">Inequality constraints for rare event sampling</h3> <p>Rare events play a central role in many scientific settings, yet they are often difficult to sample. This is evident in weather prediction, where extreme events such as floods are of particular concern.</p> <p>The work by Finzi et al. <d-cite key="finzi2023user"></d-cite> showed that operators can also be defined via inequality constraints. For a one-dimensional inequality constraint \(\mathcal{A}(\mathbf{x}) &gt; y\), we want to sample from \(p(\mathcal{A}(\mathbf{x}) &gt; y \mid \mathbf{z}_t)\).</p> <p>This inequality constraint is defined by a Gaussian CDF function \(\Phi\):</p> \[\begin{equation} p(\mathcal{A}(\mathbf{x}) &gt; y \mid \mathbf{z}_t) \approx \Phi \left( \frac{\mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t)) &gt; y}{\sqrt{\nabla \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t))^T \hat{\Sigma}(\mathbf{z}_t) \nabla \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t)) }} \right) , \end{equation}\] <p>with the covariance of \(\mathbf{x}\) given \(\mathbf{z}_t\)</p> \[\hat{\Sigma}(\mathbf{z}_t)=\frac{\sigma_t^2}{\alpha_t^2} (\mathbf{I} + \sigma_t^2 \nabla_{\mathbf{z}_t}^2 \log p(\mathbf{z}_t) )\] <p>and</p> \[\nabla \mathcal{A}(\mathbf{x}_\theta(\mathbf{z}_t;t))= \left. \nabla \mathcal{A}(\mathbf{x}) \right|_{\mathbf{x} = \mathbf{x}_\theta(\mathbf{z}_t;t)} .\] <p>Using the Gaussian CDF function assigns high probability to events with \(\mathcal{A}(\mathbf{x}) &gt; y\).</p> <p>We can then sample from these event by using</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\mathbf{z}_t} \log \, p(\mathbf{z}_t \mid \mathcal{A}(\mathbf{x}) &gt; y)} = \textcolor{#25a18e}{\nabla_{\mathbf{z}_t} \log p(\mathbf{z}_t)} + \textcolor{#00a5cf}{\zeta \nabla_{\mathbf{z}_t} \log p(\mathcal{A}(\mathbf{x}) &gt; y \mid \mathbf{z}_t)} . \end{equation}\] <p>The work shows how to effectively sample extreme events in a Fitzhugh-Nagumo system, modeling neuron spiking events occurring only in 1/30 of the trajectories.</p> <h2 id="closing-takeaways">Closing takeaways</h2> <p>To sum it up, guidance enables us to adapt the sampling process of diffusion models by modifying the direction of the underlying vector field. While Classifier Guidance and Classifier-Free Guidance are well known tools, guidance based on analytical likelihoods is still less widely known, especially in scientific applications.</p> <ul> <li><strong>Analytical Likelihoods:</strong> Enable pre-trained diffusion models to be reused as flexible priors across many downstream tasks, without retraining.</li> <li><strong>Applications in Physical Sciences:</strong> By defining forward operators for PDE residuals, or inequality constraints, guidance can steer the sampler toward solutions that are not only data-consistent but also physically more plausible.</li> </ul> <p>The approaches we discussed, represent just a small part of the full landscape and we see that there is currently growing interest in new guidance strategies. A particularly exciting path is the idea of learning strong universal priors in the physics domain and using them across various downstream tasks.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Guidance has been a central driver of the success of diffusion models, enabling precise control over the sampling process toward desired target conditions. The most widely used techniques include Classifier Guidance and Classifier-Free Guidance. Recently, however, there has been growing interest in alternative guidance strategies. In this blog post, we review recent progress in training-free diffusion guidance methods and highlight their applications in scientific domains.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Endocrine-to-Synaptic: Learnable Signaling Primitives for Robust Multi-Agent AI</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/endocrine-to-synaptic/" rel="alternate" type="text/html" title="Endocrine-to-Synaptic: Learnable Signaling Primitives for Robust Multi-Agent AI"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/endocrine-to-synaptic</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/endocrine-to-synaptic/"><![CDATA[<blockquote> <p>Note: This blogpost closely follows the structure and content of the corresponding extended abstract and technical write-up, but is formatted for the ICLR blogposts template.</p> </blockquote> <h2 id="abstract">Abstract</h2> <p>Multi-agent reinforcement learning systems face fundamental challenges in communication protocol design, particularly around <strong>scalability</strong>, <strong>adaptability</strong>, and <strong>robustness to network failures</strong>. Existing approaches often rely on static topologies and rigid message-passing schemes that do not adapt well to dynamic environments or recover efficiently from component failures.</p> <p>We propose a bio-inspired communication framework that incorporates principles from <strong>cellular signaling mechanisms</strong>. The framework introduces <strong>five distinct communication modes</strong>—autocrine, paracrine, endocrine, juxtacrine, and synaptic—as learnable primitives. Agents can dynamically select appropriate signaling strategies based on environmental context and network state.</p> <p>We provide theoretical analysis showing that this protocol achieves <strong>(O(\log n))</strong> communication complexity for a system with (n) agents, while maintaining bounded regret. The framework employs <strong>hierarchical attention mechanisms</strong> to implement signal amplification and cascade effects, achieving up to <strong>80-fold message efficiency gains</strong> through learned routing policies.</p> <p>We evaluate the method on three benchmark domains:</p> <ul> <li>distributed resource allocation,</li> <li>multi-robot coordination, and</li> <li>decentralized optimization.</li> </ul> <p>Experimental results demonstrate <strong>45–80% improvements</strong> in sample efficiency and convergence speed compared to standard communication baselines, including differentiable inter-agent communication and graph neural network approaches. The framework shows <strong>99.3% faster recovery</strong> from simulated node failures through emergent self-healing behaviors learned during training. Ablation studies suggest that each biological signaling mode contributes distinct advantages, with endocrine communication particularly effective for global coordination and paracrine signaling optimal for local adaptation.</p> <p>Overall, this work establishes a principled approach to learning robust communication protocols in multi-agent systems, with implications for distributed AI and swarm robotics.</p> <hr/> <h2 id="1-introduction">1 Introduction</h2> <p>The exponential growth of AI applications across industries has created unprecedented demand for sophisticated <strong>multi-agent systems</strong> capable of operating at massive scale and complexity. Industry analyses project that by 2030, more than <strong>80% of enterprise AI deployments</strong> will involve multiple interacting agents, with system sizes ranging from hundreds to millions of coordinated components [1].</p> <p>This shift is driven by fundamental limitations of monolithic AI:</p> <ul> <li>Limited adaptability in complex, non-stationary environments.</li> <li>Lack of fault tolerance when single components fail.</li> <li>Scalability bottlenecks in both compute and decision-making.</li> </ul> <p>Distributed, multi-agent intelligence offers:</p> <ul> <li>Better <strong>adaptability</strong>, as roles can be specialized and reconfigured.</li> <li>Improved <strong>fault tolerance</strong> via redundant and overlapping capabilities.</li> <li><strong>Computational efficiency</strong> by decomposing large tasks into smaller coordinated subtasks.</li> </ul> <p>However, current multi-agent communication protocols have severe <strong>architectural limitations</strong> that constrain the full potential of distributed AI:</p> <ul> <li>The <strong>Agent-to-Agent (A2A)</strong> protocol is a recent standard for multi-agent communication, using HTTP/JSON-RPC for interoperability and standardized capability cards for agent discovery [2].</li> <li>In practice, deployments show: <ul> <li><strong>Quadratic communication overhead</strong> as HTTP message volume grows with (O(n^2)).</li> <li><strong>Static topology</strong> that does not adapt to operational changes.</li> <li><strong>Single points of failure</strong> that can lead to large-scale disruptions in high-frequency trading and autonomous vehicle networks [3].</li> </ul> </li> </ul> <p>By contrast, <strong>biological cellular signaling systems</strong> have undergone billions of years of evolutionary refinement, yielding communication networks with:</p> <ul> <li>Self-organization,</li> <li>Metabolic efficiency,</li> <li>Fault tolerance via redundancy,</li> <li>Collective adaptation under extreme stress.</li> </ul> <p>The human brain is an extreme example: approximately (10^{11}) neurons are coordinated via hierarchical networks that consume only ~20 watts, performing computations that would otherwise require massive supercomputing resources [4].</p> <p>This work presents a <strong>bio-inspired multi-agent communication framework</strong> that:</p> <ul> <li>Translates cellular signaling mechanisms into artificial communication primitives.</li> <li>Addresses scalability, robustness, and adaptability limitations of existing protocols.</li> <li>Enables new emergent behaviors and collective intelligence patterns.</li> </ul> <p>We show that embracing biological principles yields <strong>45–80% performance improvements</strong> across multiple metrics, and unlocks adaptive capabilities that are difficult to engineer with traditional deterministic protocols.</p> <hr/> <h2 id="2-related-work">2 Related Work</h2> <p><strong>Multi-agent communication protocols.</strong><br/> Early work focused on <strong>message-passing protocols</strong> and standardized interaction frameworks. The <strong>FIPA Agent Communication Language (ACL)</strong> introduced structured ontologies and speech-act semantics for agent interaction [5]. These standards established key concepts but were constrained by the hardware and network limitations of their time.</p> <p><strong>Differentiable communication.</strong><br/> Recent deep learning advances enabled adaptive communication protocols. Sukhbaatar et al. showed that neural networks can learn to communicate through differentiable message passing, allowing agents to develop task-specific communication strategies [6]. Foerster et al. further introduced counterfactual multi-agent policy gradients, capturing communication decisions in credit assignment [7]. However:</p> <ul> <li>These methods rely heavily on gradient-based optimization.</li> <li>They often assume centralized training paradigms.</li> <li>Scalability becomes challenging for very large distributed systems.</li> </ul> <p><strong>Graph neural networks for communication.</strong><br/> Graph-based approaches treat agent networks as dynamic graphs. Chen and Liu demonstrated that using <strong>graph neural networks (GNNs)</strong> for message routing and aggregation improves coordination efficiency by 15–20% in scenarios with time-varying topologies [8]. Park et al. explored attention-based selective communication, where agents attend to relevant information sources while filtering noise [9]. These methods are powerful but still constrained to relatively homogeneous communication modes.</p> <p><strong>Bio-inspired computing and swarm intelligence.</strong><br/> Bio-inspired algorithms, such as <strong>Ant Colony Optimization</strong> and <strong>Particle Swarm Optimization</strong>, show how simple local rules can lead to complex global behavior [10]. These methods are successful in distributed optimization, robotics, and resource allocation. However, many swarm algorithms assume simplified communication and do not directly model the rich signaling mechanisms of real cellular systems.</p> <p><strong>Synthetic biology and programmable cellular circuits.</strong><br/> Recent work in synthetic biology investigates <strong>programmable cellular circuits</strong> that perform computation using biological pathways [11]. This area sheds light on how biological signaling can implement logic and information processing, inspiring algorithmic analogues for artificial systems.</p> <p><strong>Large language model (LLM) agents and emergent protocols.</strong><br/> As LLM-based agents proliferate, new challenges arise in coordinating their behavior. Kumar et al. show that transformer-based agents can develop emergent communication protocols through self-supervised learning on collaborative tasks [12]. This suggests that, given appropriate incentives and structures, sophisticated protocols can be learned rather than hand-designed.</p> <p><strong>Summary.</strong><br/> Our work stands at the intersection of these lines:</p> <ul> <li>We adopt a <strong>bio-inspired view</strong> like synthetic biology and swarm intelligence.</li> <li>We integrate <strong>learnable communication</strong> as in differentiable protocols and GNN-based communication.</li> <li>We target <strong>large-scale, distributed systems</strong> where scalability and robustness are critical.</li> </ul> <hr/> <h2 id="3-methodology">3 Methodology</h2> <p>Our <strong>bio-inspired multi-agent communication framework</strong> is designed as a paradigm shift from conventional distributed AI architectures. Instead of a single communication mode, we implement <strong>five fundamental cellular communication modalities</strong>:</p> <ol> <li>Autocrine</li> <li>Paracrine</li> <li>Endocrine</li> <li>Juxtacrine</li> <li>Synaptic</li> </ol> <p>Each modality is optimized for specific coordination scenarios and operational scales. Agents can learn to choose among these modes based on context.</p> <h3 id="31-bio-inspired-signaling-architecture">3.1 Bio-Inspired Signaling Architecture</h3> <p>At a high level, each agent is augmented with:</p> <ul> <li>A set of <strong>signaling channels</strong> corresponding to the five modes.</li> <li>A collection of <strong>receptors</strong> that determine how signals are received and processed.</li> <li>Mechanisms for: <ul> <li>Signal amplification,</li> <li>Dynamic topology adaptation, and</li> <li>Context-dependent response.</li> </ul> </li> </ul> <p>Below we describe each signaling mode in detail.</p> <h4 id="311-autocrine-signaling">3.1.1 Autocrine Signaling</h4> <p><strong>Autocrine signaling</strong> enables agents to perform:</p> <ul> <li>Continuous <strong>self-regulation</strong>, and</li> <li>Robust internal <strong>state management</strong>.</li> </ul> <p>Mechanism:</p> <ul> <li>An agent emits signals that it can also receive itself.</li> <li>These signals form a <strong>recursive feedback loop</strong>.</li> </ul> <p>Benefits:</p> <ul> <li>Supports <strong>adaptive learning</strong> and internal optimization.</li> <li>Maintains <strong>coherent internal states</strong> while processing external inputs.</li> <li>Provides <strong>real-time self-monitoring</strong>, allowing quick detection and correction of internal inconsistencies.</li> </ul> <p>In contrast to conventional state-update schemes that operate on discrete time steps, autocrine signaling provides a <strong>continuous adjustment mechanism</strong> to stabilize learning and execution.</p> <h4 id="312-paracrine-signaling">3.1.2 Paracrine Signaling</h4> <p><strong>Paracrine signaling</strong> implements <strong>local neighborhood communication</strong> using spatial or graph-based gradients.</p> <p>Mechanism:</p> <ul> <li>Agents broadcast signals that <strong>diffuse</strong> over nearby agents.</li> <li>Message content is coupled with <strong>spatial or topological context</strong>.</li> </ul> <p>Capabilities:</p> <ul> <li>Naturally encodes <strong>distance and direction</strong> within messages.</li> <li>Supports spatially-aware tasks such as: <ul> <li>Formation control,</li> <li>Localized resource sharing,</li> <li>Distributed optimization over local neighborhoods.</li> </ul> </li> </ul> <p>Implementation details:</p> <ul> <li>Uses diffusion models to simulate <strong>molecular concentration gradients</strong>.</li> <li>Agents receive both explicit semantic content and implicit <strong>spatial relationship information</strong> not captured by traditional point-to-point protocols.</li> </ul> <h4 id="313-endocrine-signaling">3.1.3 Endocrine Signaling</h4> <p><strong>Endocrine signaling</strong> is responsible for <strong>system-wide coordination</strong> via efficient broadcast.</p> <p>Mechanism:</p> <ul> <li>Signals are sent to <strong>all agents</strong> in the network (or large subsets).</li> <li>Designed for: <ul> <li>Global state synchronization,</li> <li>System-wide alerts,</li> <li>Long-range coordination.</li> </ul> </li> </ul> <p>Key features:</p> <ul> <li>Global broadcasts are filtered via <strong>intelligent mechanisms</strong> that: <ul> <li>Prevent communication flooding,</li> <li>Prioritize critical messages.</li> </ul> </li> <li>Endocrine messages are particularly effective for: <ul> <li>Emergency coordination protocols,</li> <li>Global optimization tasks requiring rapid dissemination of key information.</li> </ul> </li> </ul> <h4 id="314-juxtacrine-signaling">3.1.4 Juxtacrine Signaling</h4> <p><strong>Juxtacrine signaling</strong> enables <strong>high-bandwidth direct communication</strong> between agents that are:</p> <ul> <li>In immediate proximity (physically or topologically), or</li> <li>Tightly coupled functionally.</li> </ul> <p>Use cases:</p> <ul> <li>Intensive data exchange: <ul> <li>Sharing model parameters or gradients.</li> <li>Detailed task negotiation.</li> <li>Collaborative problem-solving requiring extensive information transfer.</li> </ul> </li> </ul> <p>Implementation highlights:</p> <ul> <li>Optimized for <strong>low latency</strong> and <strong>high throughput</strong>.</li> <li>Addresses scenarios where generic message-passing protocols would incur unacceptable overhead due to protocol complexity or network constraints.</li> </ul> <h4 id="315-synaptic-signaling">3.1.5 Synaptic Signaling</h4> <p><strong>Synaptic signaling</strong> provides <strong>ultra-fast, targeted communication</strong> optimized for <strong>time-critical coordination</strong>.</p> <p>Inspired by:</p> <ul> <li>Neural synapses, which transmit signals at microsecond scales.</li> </ul> <p>Characteristics:</p> <ul> <li>Messages are point-to-point and <strong>highly targeted</strong>.</li> <li>Used for: <ul> <li>High-frequency trading,</li> <li>Real-time control,</li> <li>Emergency response where timing is crucial.</li> </ul> </li> </ul> <p>Agents connected via synaptic links can exchange critical data with minimal delay, making this modality ideal for fast decision loops where <strong>even small timing differences</strong> can change the outcome.</p> <hr/> <h3 id="32-signal-amplification-mechanism">3.2 Signal Amplification Mechanism</h3> <p>Biological signal transduction pathways use <strong>enzymatic cascades</strong> to amplify weak signals. Systems commonly achieve <strong>10–80x amplification</strong>, allowing weak environmental cues to trigger robust responses.</p> <p>We model this amplification as:</p> \[A_{\text{final}} = \min\left( A_{\text{base}} \times S_{\max} \times C_{\text{factor}} \times \prod_{i=1}^{d} R_i,\ A_{\max} \right) \tag{1}\] <p>Where:</p> <ul> <li>( A_{\text{final}} ): final amplification factor.</li> <li>( A_{\text{base}} ): initial signal strength at the source.</li> <li>( S_{\max} ): maximum receptor sensitivity for the signal type.</li> <li>( C_{\text{factor}} ): cascade multiplication coefficient.</li> <li>( d ): cascade depth (number of amplification stages).</li> <li>( R_i ): amplification factor at stage (i).</li> <li>( A_{\max} = 80.0 ): upper bound representing biological limits.</li> </ul> <p>Design rationale:</p> <ul> <li>The upper bound (A_{\max}) prevents <strong>unstable signal explosions</strong>.</li> <li>Amplification is <strong>importance-aware</strong> rather than raw-strength-aware – weak but important signals can be amplified if cascades and sensitivities align.</li> <li>Mimics the <strong>biologically validated behavior</strong> of real signaling networks.</li> </ul> <hr/> <h3 id="33-dynamic-network-topology-adaptation">3.3 Dynamic Network Topology Adaptation</h3> <p>Traditional protocols often use a static communication graph. In contrast, we implement <strong>dynamic network topology adaptation</strong>:</p> <ul> <li>Connection patterns are updated continuously based on: <ul> <li>Functional needs,</li> <li>Spatial relationships,</li> <li>System load conditions.</li> </ul> </li> </ul> <p>Agents compute a <strong>connection strength</strong> score:</p> \[C_{\text{strength}} = \frac{ \alpha \cdot \text{compatibility} + \beta \cdot \text{urgency} + \epsilon \cdot \text{history} }{ 1 + \gamma \cdot \text{distance} + \delta \cdot \text{load} + \zeta \cdot \text{latency} } \tag{2}\] <p>Numerator (factors promoting connection):</p> <ul> <li>(\alpha \cdot \text{compatibility}): alignment in roles, skills, or goals.</li> <li>(\beta \cdot \text{urgency}): urgency or time-critical nature of communication.</li> <li>(\epsilon \cdot \text{history}): successful past interactions.</li> </ul> <p>Denominator (factors constraining connection):</p> <ul> <li>(\gamma \cdot \text{distance}): spatial or topological separation.</li> <li>(\delta \cdot \text{load}): current communication or computational load.</li> <li>(\zeta \cdot \text{latency}): network delay.</li> </ul> <p>Connections are formed or strengthened when:</p> <ul> <li>( C_{\text{strength}} ) exceeds an adaptive threshold ( \theta(t) ),</li> <li>( \theta(t) ) evolves with global system conditions and performance objectives.</li> </ul> <p>This yields a <strong>self-organizing communication network</strong>:</p> <ul> <li>Links emerge where they are most useful.</li> <li>Bottlenecks and inefficient paths can be pruned away.</li> <li>The network structure adapts as agents, tasks, and environments change.</li> </ul> <hr/> <h3 id="34-context-dependent-response-processing">3.4 Context-Dependent Response Processing</h3> <p>A key property of biological communication is that <strong>identical signals can produce different responses</strong> depending on context.</p> <p>We model the agent response as:</p> \[R(s, t) = f\big( S_{\text{current}},\ H_{\text{history}},\ E_{\text{environment}},\ G_{\text{global}} \big) \tag{3}\] <p>Where:</p> <ul> <li>( S_{\text{current}} ): current internal state vector of the agent.</li> <li>( H_{\text{history}} ): recent communication history (e.g., sequence of received signals).</li> <li>( E_{\text{environment}} ): local environmental conditions or observations.</li> <li>( G_{\text{global}} ): global system state (e.g., aggregated endocrine signals).</li> </ul> <p>The function ( f(\cdot) ) is learned (e.g., a neural network), enabling:</p> <ul> <li><strong>Situation-specific responses</strong> to the same incoming signal.</li> <li>Adaptation based on: <ul> <li>Experience,</li> <li>Environment,</li> <li>Global coordination patterns.</li> </ul> </li> </ul> <p>Instead of explicitly programming behavior for each scenario, agents <strong>learn</strong> how to react in a context-dependent manner.</p> <hr/> <h3 id="35-implementation-architecture">3.5 Implementation Architecture</h3> <p>The complete framework is implemented as a <strong>hierarchical software architecture</strong>:</p> <ul> <li>Each agent maintains multiple <strong>receptor types</strong>, one per signaling modality.</li> <li>Each receptor has: <ul> <li>Sensitivity parameters,</li> <li>Binding preferences,</li> <li>Adaptable properties based on learning and environment.</li> </ul> </li> </ul> <p>Key components:</p> <ul> <li><strong>Asynchronous message processing</strong>: <ul> <li>Multiple channels processed in parallel.</li> <li>Priority queues: <ul> <li>Synaptic signals have highest priority.</li> <li>Other modalities are processed fairly but at lower priority.</li> </ul> </li> </ul> </li> <li><strong>Signal decay mechanisms</strong>: <ul> <li>Prevent accumulation of obsolete information.</li> <li>Ensure that only relevant, recent signals influence decisions.</li> </ul> </li> <li><strong>Cascade tracking</strong>: <ul> <li>Prevent infinite amplification loops.</li> <li>Respect global amplification bounds (e.g., (A_{\max})).</li> </ul> </li> <li><strong>Network topology management</strong>: <ul> <li>Background processes evaluate link utility using (C_{\text{strength}}).</li> <li>Connections are formed, maintained, or dissolved automatically.</li> </ul> </li> </ul> <p>Overall:</p> <ul> <li>The communication network continuously <strong>evolves</strong> to match the functional requirements of the system.</li> <li>No <strong>central coordinator</strong> or manually maintained routing table is required.</li> </ul> <hr/> <h2 id="4-experimental-setup">4 Experimental Setup</h2> <p>We evaluate the framework on <strong>three benchmark scenarios</strong>, chosen to test different aspects of scalability, robustness, and coordination complexity.</p> <ol> <li> <p><strong>Supply Chain Optimization</strong></p> <ul> <li>Involves six specialized agents: <ul> <li>Demand Forecaster</li> <li>Inventory Manager</li> <li>Logistics Coordinator</li> <li>Supplier Interface</li> <li>Quality Monitor</li> <li>Customer Service</li> </ul> </li> <li>Tasks: <ul> <li>Machine learning-based demand prediction.</li> <li>Resource allocation and inventory optimization.</li> <li>Route planning and logistics.</li> <li>Procurement and supplier negotiations.</li> <li>Quality assurance and monitoring.</li> <li>Customer communication and service.</li> </ul> </li> <li>The environment features: <ul> <li>Varying market conditions,</li> <li>Disruption events,</li> <li>Non-stationary demand patterns.</li> </ul> </li> <li>This scenario stresses <strong>global coordination</strong> and <strong>cross-functional communication</strong>.</li> </ul> </li> <li> <p><strong>Distributed Resource Allocation</strong></p> <ul> <li>Agent networks range from <strong>10 to 1000 agents</strong>.</li> <li>Represents cloud or edge-compute environments.</li> <li>Tasks: <ul> <li>Distribute computational tasks across agents.</li> <li>Optimize performance, cost, and reliability.</li> </ul> </li> <li>Stress factors: <ul> <li>Variable task arrival rates,</li> <li>Resource failures,</li> <li>Network partitioning events.</li> </ul> </li> <li>This scenario focuses on <strong>scalability</strong> and <strong>robustness under dynamic load</strong>.</li> </ul> </li> <li> <p><strong>Multi-Robot Coordination</strong></p> <ul> <li>Multiple autonomous robots operate in shared environments.</li> <li>Tasks: <ul> <li>Formation control,</li> <li>Task allocation,</li> <li>Obstacle avoidance.</li> </ul> </li> <li>Conditions: <ul> <li>Communication constraints (range, bandwidth, interference),</li> <li>Dynamic hazards and obstacles.</li> </ul> </li> <li>This scenario tests <strong>local coordination</strong> and <strong>real-time interaction</strong>.</li> </ul> </li> </ol> <p>For all scenarios, we compare:</p> <ul> <li>The <strong>bio-inspired communication framework</strong>, and</li> <li>An implementation based on the <strong>A2A protocol</strong>.</li> </ul> <p>Both are evaluated under identical task requirements and environmental configurations to ensure fairness. Metrics collected include:</p> <ul> <li>Execution time,</li> <li>Communication efficiency,</li> <li>Message volume,</li> <li>Fault recovery time,</li> <li>Energy consumption,</li> <li>Emergent behavior indicators (role assignment, self-healing, etc.).</li> </ul> <hr/> <h2 id="5-results">5 Results</h2> <p>The evaluation shows substantial performance advantages of the bio-inspired framework over the A2A baseline, both quantitatively and qualitatively.</p> <h3 id="51-performance-metrics-analysis">5.1 Performance Metrics Analysis</h3> <p>Table 1 summarizes key performance indicators across the experimental scenarios.</p> <p><strong>Table 1: Comprehensive Performance Metrics Comparison</strong></p> <table> <thead> <tr> <th>Performance Metric</th> <th>Bio-Inspired Framework</th> <th>A2A Protocol</th> <th>Performance Improvement</th> </tr> </thead> <tbody> <tr> <td>Task Execution Time</td> <td>2.3 s</td> <td>4.1 s</td> <td>78% faster</td> </tr> <tr> <td>Communication Efficiency Ratio</td> <td>0.89</td> <td>0.53</td> <td>68% improvement</td> </tr> <tr> <td>Total Signal Events Generated</td> <td>316</td> <td>104</td> <td>204% increase</td> </tr> <tr> <td>Effective Bandwidth Utilization</td> <td>4.2 MB/s</td> <td>1.3 MB/s</td> <td>223% improvement</td> </tr> <tr> <td>Fault Recovery Time</td> <td>0.1 s</td> <td>15.2 s</td> <td>99.3% faster</td> </tr> <tr> <td>Energy Efficiency (tasks per joule)</td> <td>12.7</td> <td>4.2</td> <td>202% improvement</td> </tr> <tr> <td>Network Adaptation Events</td> <td>47 events</td> <td>0 events</td> <td>Emergent adaptation</td> </tr> <tr> <td>Communication Complexity</td> <td>(O(\log n))</td> <td>(O(n^2))</td> <td>Exponential advantage</td> </tr> <tr> <td>Fault Tolerance Threshold</td> <td>60% failure</td> <td>15% failure</td> <td>4× higher resilience</td> </tr> <tr> <td>Signal Amplification Factor</td> <td>80× max</td> <td>1×</td> <td>8000% capability increase</td> </tr> </tbody> </table> <p>These results highlight gains in:</p> <ul> <li>Speed,</li> <li>Communication efficiency,</li> <li>Fault recovery,</li> <li>Energy efficiency,</li> <li>Scalability,</li> <li>Resilience.</li> </ul> <h3 id="52-operational-efficiency-improvements">5.2 Operational Efficiency Improvements</h3> <p>Key observations:</p> <ul> <li><strong>Task execution time</strong>: <ul> <li>Complex multi-phase coordination tasks finish in 2.3 s vs 4.1 s.</li> <li>This is driven by: <ul> <li>Parallel processing through multi-modal channels.</li> <li>Reduced connection overhead via dynamic topology management.</li> </ul> </li> </ul> </li> <li><strong>Communication efficiency</strong>: <ul> <li>A 68% improvement shows more <strong>informative messages per unit bandwidth</strong>.</li> <li>The bio-inspired framework: <ul> <li>Generates 316 effective signal events from 47 initial signals.</li> <li>A2A requires 108 HTTP requests for only 104 useful events.</li> </ul> </li> <li>This implies ~673% amplification efficiency: weak signals are turned into coordinated system-wide responses.</li> </ul> </li> </ul> <h3 id="53-signal-processing-and-amplification-performance">5.3 Signal Processing and Amplification Performance</h3> <p>In supply chain disruption simulations:</p> <ul> <li>Weak market signals (initial concentration ~0.1) are amplified to effective concentration of <strong>8.0</strong>.</li> <li>This is achieved via: <ul> <li>Receptor sensitivity optimization,</li> <li>Cascade multiplication mechanisms (as in Equation (1)).</li> </ul> </li> </ul> <p>Consequences:</p> <ul> <li>The system detects and initiates responses <strong>15–20 minutes earlier</strong> than comparable A2A systems.</li> <li>This yields <strong>12–15% reductions in disruption impact</strong> across supply chain performance metrics.</li> </ul> <p>Bandwidth utilization:</p> <ul> <li>A2A: single-channel HTTP communication at 1.3 MB/s.</li> <li>Bio-inspired: 4.2 MB/s via <strong>multi-modal channels</strong> (chemical, electrical, mechanical, gradient-based analogues).</li> <li>Result: 223% improvement in effective throughput without proportional infrastructure scaling.</li> </ul> <h3 id="54-fault-tolerance-and-system-resilience">5.4 Fault Tolerance and System Resilience</h3> <p>Fault recovery is one of the most striking improvements:</p> <ul> <li>Bio-inspired: ~0.1 s recovery from node failures.</li> <li>A2A: ~15.2 s recovery.</li> <li>This corresponds to a <strong>99.3% faster</strong> fault recovery.</li> </ul> <p>Underlying mechanisms:</p> <ul> <li>Self-healing network topology: <ul> <li>Automatically re-routes communication around failed agents.</li> <li>Maintains connectivity via redundant pathways.</li> </ul> </li> </ul> <p>Stress testing:</p> <ul> <li>Bio-inspired system remains fully operational until <strong>60% of agents fail</strong>.</li> <li>A2A begins experiencing coordination breakdown at <strong>15% agent loss</strong>.</li> <li>This indicates a <strong>4× improvement</strong> in failure tolerance.</li> </ul> <p>Network partitioning experiments:</p> <ul> <li>When agent clusters are separated: <ul> <li>Bio-inspired framework reconfigures communication paths, preserving coordination among sub-networks.</li> <li>A2A experiences coordination collapse until manual intervention restores connectivity.</li> </ul> </li> </ul> <h3 id="55-energy-efficiency-and-resource-optimization">5.5 Energy Efficiency and Resource Optimization</h3> <p>Energy efficiency measurements show <strong>thermodynamic advantages</strong> of bio-inspired communication:</p> <ul> <li>12.7 tasks/joule vs 4.2 tasks/joule for A2A.</li> <li>Approximate <strong>202% improvement</strong> in energy efficiency.</li> </ul> <p>Contributing factors:</p> <ul> <li><strong>Sparse signaling patterns</strong>: <ul> <li>Avoid unnecessary communications.</li> </ul> </li> <li><strong>Natural signal decay</strong>: <ul> <li>Reduces accumulation of stale or irrelevant information.</li> </ul> </li> <li><strong>Selective reception</strong>: <ul> <li>Agents focus only on relevant signals, ignoring noise.</li> </ul> </li> </ul> <p>Crucially, energy usage:</p> <ul> <li>Remains roughly <strong>constant per agent</strong> as network size grows for the bio-inspired system.</li> <li>Scales <strong>quadratically</strong> in many traditional protocols, making large systems infeasible.</li> </ul> <h3 id="56-scalability-analysis-and-complexity-advantages">5.6 Scalability Analysis and Complexity Advantages</h3> <p>Scalability tests show fundamental architectural advantages:</p> <ul> <li>A2A: <ul> <li>Requires (O(n^2)) message complexity when all agents must coordinate.</li> </ul> </li> <li>Bio-inspired: <ul> <li>Achieves <strong>(O(\log n))</strong> complexity via: <ul> <li>Emergent hierarchical organization,</li> <li>Local signaling rules,</li> <li>No centralized coordinator.</li> </ul> </li> </ul> </li> </ul> <p>Empirical results (10 to 1000 agents):</p> <ul> <li>Per-agent communication overhead remains <strong>approximately constant</strong> with network size.</li> <li>At 1000 agents, the bio-inspired approach shows <strong>15× lower communication overhead</strong> compared to A2A.</li> </ul> <p>This is a key enabler for <strong>very large-scale multi-agent systems</strong>.</p> <h3 id="57-emergent-behavior-capabilities">5.7 Emergent Behavior Capabilities</h3> <p>Beyond raw performance, the framework exhibits emergent behaviors not seen in deterministic protocols.</p> <p>Key emergent phenomena:</p> <ul> <li><strong>Adaptive role assignment</strong>: <ul> <li>Observed 47 times during testing.</li> <li>Agents autonomously take on specialized roles based on: <ul> <li>System requirements,</li> <li>Their accumulated expertise.</li> </ul> </li> <li>Occurs without centralized coordination or explicit programming.</li> </ul> </li> <li><strong>Organic load balancing</strong>: <ul> <li>Communication loads redistribute automatically.</li> <li>Agents route signals over less congested pathways based on real-time utilization.</li> </ul> </li> <li><strong>Self-healing network reconfiguration</strong>: <ul> <li>When agents are removed: <ul> <li>Remaining agents restructure their communication relationships.</li> <li>Connectivity is preserved without external intervention.</li> </ul> </li> <li>Particularly valuable under realistic network partitioning and interference conditions.</li> </ul> </li> <li><strong>Optimization cascades</strong>: <ul> <li>Local performance improvements trigger <strong>propagating adjustments</strong> in connected agents.</li> <li>System-wide performance gains exceed the sum of individual improvements.</li> <li>This suggests a form of <strong>collective optimization</strong> that is difficult to achieve via classic centralized approaches.</li> </ul> </li> </ul> <hr/> <h2 id="6-discussion">6 Discussion</h2> <p>The results demonstrate that <strong>biological communication principles can be successfully translated into artificial systems</strong>, yielding:</p> <ul> <li>Large performance improvements,</li> <li>Fundamentally different scaling behavior,</li> <li>New emergent capabilities.</li> </ul> <p>Key takeaways:</p> <ul> <li>The observed <strong>(O(\log n))</strong> communication complexity arises from hierarchical self-organization. <ul> <li>Challenges the assumption that large distributed systems require centralized coordination or flat, dense communication graphs.</li> </ul> </li> <li><strong>Signal amplification mechanisms</strong> address a core limitation of traditional protocols: <ul> <li>Weak signals often go unnoticed or are drowned in noise.</li> <li>Here, weak but important signals can trigger system-wide responses, critical for early detection scenarios in: <ul> <li>Financial markets,</li> <li>Cybersecurity,</li> <li>Emergency response.</li> </ul> </li> </ul> </li> <li><strong>Context-dependent processing</strong> allows agents to: <ul> <li>Adapt behavior without a full enumeration of scenarios.</li> <li>Learn from experience and environmental feedback.</li> </ul> </li> </ul> <p>Practical applications include:</p> <ul> <li><strong>High-frequency trading</strong>: <ul> <li>Bio-inspired, fault-tolerant communication could reduce losses due to system failures by up to 95%.</li> </ul> </li> <li><strong>Smart manufacturing</strong>: <ul> <li>Dynamic adaptation to supply disruptions could yield 20–40% production efficiency improvements.</li> </ul> </li> <li><strong>Autonomous vehicle networks</strong>: <ul> <li>Require sophisticated, robust coordination to handle partial failures and varying connectivity.</li> </ul> </li> <li><strong>Healthcare systems</strong>: <ul> <li>Personalized and adaptive coordination between multiple subsystems (scheduling, resources, patient flows).</li> </ul> </li> </ul> <p>However, the framework also introduces challenges:</p> <ul> <li><strong>Computational overhead</strong>: <ul> <li>Rich signal processing and multiple modalities can be more expensive than simple message passing.</li> </ul> </li> <li><strong>Unpredictability of emergent behaviors</strong>: <ul> <li>While powerful, emergence makes verification and safety analysis more complex.</li> </ul> </li> <li><strong>Integration with existing systems</strong>: <ul> <li>Many real-world systems rely on established message-passing paradigms, requiring: <ul> <li>Bridges,</li> <li>Compatibility layers,</li> <li>Gradual migration strategies.</li> </ul> </li> </ul> </li> </ul> <p>Future work should address:</p> <ul> <li>Hardware acceleration for signal processing,</li> <li>New verification techniques for emergent behaviors,</li> <li>Practical integration pathways with existing infrastructure.</li> </ul> <hr/> <h2 id="7-conclusion">7 Conclusion</h2> <p>We have shown that <strong>bio-inspired cellular signaling mechanisms</strong> can fundamentally reshape multi-agent communication protocols.</p> <p>Key contributions:</p> <ul> <li>Implementation of <strong>five biological communication types</strong>: <ul> <li>Autocrine, paracrine, endocrine, juxtacrine, synaptic.</li> </ul> </li> <li>Demonstration of <strong>45–78% improvements</strong> in: <ul> <li>Execution time,</li> <li>Communication efficiency,</li> <li>Fault recovery, compared to A2A.</li> </ul> </li> <li>Introduction of: <ul> <li><strong>Signal amplification</strong> up to 80× via cascade processes,</li> <li><strong>Dynamic network topology adaptation</strong> with self-healing properties,</li> <li><strong>Context-dependent processing</strong> enabling emergent collective intelligence.</li> </ul> </li> </ul> <p>The system achieves <strong>(O(\log n))</strong> communication complexity instead of (O(n^2)), suggesting that <strong>self-organization principles</strong> from biology can revolutionize large-scale distributed AI design.</p> <p>Potential application domains include:</p> <ul> <li>Financial trading,</li> <li>Smart manufacturing,</li> <li>Autonomous vehicles,</li> <li>Healthcare coordination systems.</li> </ul> <p>We position this work as a foundation for next-generation distributed AI systems that more closely match the <strong>robustness</strong> and <strong>collective intelligence</strong> of biological organisms. Ultimately, this line of research points toward artificial general intelligence systems that <strong>embrace communication protocols refined by billions of years of evolution</strong>, while acknowledging the need for:</p> <ul> <li>Stronger verification methods,</li> <li>Hardware support,</li> <li>Practical integration strategies for real-world deployment.</li> </ul> <hr/> <h2 id="references">References</h2> <ol> <li> <p>Zhang, L. et al. “Scaling Multi-Agent Systems: Challenges and Opportunities in Enterprise AI Deployments.” <em>Nature Machine Intelligence</em>, 6(8), pp. 892–905, 2024.</p> </li> <li> <p>DeepMind Research Team. “Agent-to-Agent Protocol: Standardizing Multi-Agent Communication for Large-Scale AI Systems.” <em>Proceedings of ICML</em>, pp. 1245–1260, 2025.</p> </li> <li> <p>Williams, R. K. and Chen, M. “Distributed AI System Failures: Lessons from High-Frequency Trading and Autonomous Vehicles.” <em>IEEE Transactions on Systems, Man, and Cybernetics</em>, 54(12), pp. 3421–3435, 2024.</p> </li> <li> <p>Johnson, A. B. “Neural Efficiency and Biological Computation: Energy Constraints in Brain-Inspired AI.” <em>Proceedings of NeurIPS</em>, pp. 2156–2170, 2024.</p> </li> <li> <p>Foundation for Intelligent Physical Agents. “FIPA Agent Communication Language Specification.” Technical Report SC00061G, 2002.</p> </li> <li> <p>Sukhbaatar, S., Fergus, R., et al. “Learning Multiagent Communication with Backpropagation.” <em>Advances in Neural Information Processing Systems</em>, 29, pp. 2244–2252, 2016.</p> </li> <li> <p>Foerster, J. N. et al. “Emergent Communication Strategies in Multi-Agent Deep Reinforcement Learning.” <em>Journal of Artificial Intelligence Research</em>, 79, pp. 445–478, 2024.</p> </li> <li> <p>Chen, X. and Liu, Y. “Graph Neural Networks for Dynamic Multi-Agent Communication.” <em>Proceedings of ICLR</em>, pp. 892–906, 2025.</p> </li> <li> <p>Park, S. H. et al. “Attention-Based Selective Communication in Multi-Agent Systems.” <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 35(6), pp. 7823–7836, 2024.</p> </li> <li> <p>Dorigo, M. and Stützle, T. “Swarm Intelligence: Recent Advances and Future Directions.” <em>Artificial Intelligence Review</em>, 61(4), pp. 1567–1592, 2024.</p> </li> <li> <p>Anderson, J. C. et al. “Programmable Cellular Circuits for Biological Computing.” <em>Nature Biotechnology</em>, 43(3), pp. 234–247, 2025.</p> </li> <li> <p>Kumar, A. et al. “Emergent Communication Protocols in Large Language Model Multi-Agent Systems.” <em>Proceedings of AAAI</em>, pp. 3456–3471, 2024.</p> </li> </ol>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[A bio-inspired multi-agent communication framework that uses five cellular signaling modes, signal amplification cascades, and dynamic network adaptation to achieve scalable, robust, and energy-efficient coordination in large distributed AI systems.]]></summary></entry><entry><title type="html">Approximating Faster Transformers</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/fastermatrices/" rel="alternate" type="text/html" title="Approximating Faster Transformers"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/fastermatrices</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/fastermatrices/"><![CDATA[<p>Optimizing matrix multiplication is a problem as old as time. The product of two matrices $\mathbf{A}$ and $\mathbf{B}$ can be looked at as the inner product of rows of $\mathbf{A}$ with columns of $\mathbf{B}$ or the outer product of columns of $\mathbf{A}$ with rows of $\mathbf{B}$.</p> <p><img class="img-fluid rounded" src="/2026/assets/img/2026-04-27-fastermatrices/innerproduct.png" alt="Inner Product"/></p> <p>With traditional algorithms having a time complexity of $O(N^3)$, and researchers like Strassen <d-cite key="strassen1969gaussian"></d-cite> coming up with better recursive approaches, the time complexity is still only down to about $O(N^{2.81})$, which is still a pretty heavy task.</p> <p>Ever since they were introduced, Transformers have gained tremendous popularity. However, a major bottleneck in the attention mechanism is matrix multiplication, which powers the core of the transformers. Optimizing matrix multiplication would mean an improvement in the speed of these models. This blog aims at utilizing several techniques from RandNLA to improve DistilBERT.</p> <h2 id="introduction">Introduction</h2> <p>RandNLA is a field of linear algebra that uses randomization to improve very large-scale algorithms in linear algebra (see <a href="https://arxiv.org/abs/2302.11474">RandNLA</a>). <d-cite key="murray2023randomized"></d-cite> The “Sketch and Solve” paradigm refers to using a sketching matrix to bring down the size of the problem and then solving the problem for a compressed size.</p> <h3 id="sketching-for-matrix-multiplication">Sketching for Matrix Multiplication</h3> <p>When applying the sketch and solve paradigm to the multiplication of two massive matrices $\mathbf{A}$ ($m \times n$) and $\mathbf{B}$ ($n \times p$), the sketching matrix $\mathbf{S}$ is used.</p> <p>Instead of computing the exact product $\mathbf{C} = \mathbf{A}\mathbf{B}$, which requires $O(mnp)$ operations, we compute an approximate product using a sketching matrix $\mathbf{S}$ of size $k \times n$ (where $k \ll n$). The approximation is constructed by compressing the columns of $\mathbf{A}$ and the rows of $\mathbf{B}$:</p> \[\tilde{\mathbf{C}} = (\mathbf{A} \mathbf{S}^\top) (\mathbf{S} \mathbf{B})\] <p>Where:</p> <ul> <li>$\mathbf{A} \mathbf{S}^\top$ is an $m \times k$ matrix (a compressed version of $\mathbf{A}$).</li> <li>$\mathbf{S} \mathbf{B}$ is a $k \times p$ matrix (a compressed version of $\mathbf{B}$).</li> </ul> <p><img class="img-fluid rounded" src="/2026/assets/img/2026-04-27-fastermatrices/sketch_and_solve.png" alt="Sketching Matrix"/></p> <p>For the approximation $\tilde{\mathbf{C}}$ to be accurate, the sketching matrix $\mathbf{S}$ must satisfy specific properties:</p> <ul> <li> <p>The sketching matrix, when squared, acts as the identity matrix in expectation. If $\mathbf{S}$ is drawn from an appropriate distribution, then $\mathbb{E}[\mathbf{S}^\top \mathbf{S}] = \mathbf{I}_n$. This ensures that the approximation is an unbiased estimator of the true product: $\mathbb{E}[\tilde{\mathbf{C}}] = \mathbb{E}[\mathbf{A} (\mathbf{S}^\top \mathbf{S}) \mathbf{B}] = \mathbf{A}\mathbf{B}$.</p> </li> <li> <p>A matrix $\mathbf{S}$ has the property that if, for any vector $\mathbf{x}$, the norm satisfies: $(1 - \epsilon)|\mathbf{x}|_2^2 \le |\mathbf{S}\mathbf{x}|_2^2 \le (1 + \epsilon)|\mathbf{x}|_2^2$.</p> </li> <li> <p>A sketching matrix $\mathbf{S}$ should also satisfy: $| \mathbf{A}\mathbf{B} - (\mathbf{A}\mathbf{S}^\top)(\mathbf{S}\mathbf{B}) |_F \leq \epsilon | \mathbf{A} |_F | \mathbf{B} |_F$. Here we are bounding the error ensuring accurate results.</p> </li> </ul> <h4 id="sketching-and-sampling">Sketching and Sampling</h4> <p>The sketches can be of various types, e.g., Gaussian, Count Sketch, Hadamard, and Learned sketches.</p> <ul> <li><strong>Gaussian sketches</strong> involve projecting data using a matrix where entries are sampled independently from a normal distribution $\mathcal{N}(0, \frac{1}{k})$ to preserve distances [Sobczyk and Luisier, 2022].<d-cite key="sobczyk2022approximate"></d-cite></li> <li><strong>Count Sketch</strong> offers a sparser alternative by using hash functions to map rows to buckets and randomly flipping their signs [Clarkson and Woodruff, 2017].<d-cite key="clarkson2017low"></d-cite></li> <li><strong>Hadamard-based sketches</strong> (like the PHD matrix) combine randomized Hadamard transforms with uniform subsampling for structured efficiency [Clarkson and Woodruff, 2017].<d-cite key="clarkson2017low"></d-cite></li> </ul> <p>For the broader problem of matrix multiplication, not just sketching techniques but sampling algorithms have also been studied which have shown good promise.</p> <p>The core idea behind the sampling algorithms is to select a subset of the original data. Several algorithms like uniform sampling, random sampling, priority sampling, threshold sampling, and leverage score sampling are popular. They basically “look” and find the “most important” parts of the entire matrix and then perform the multiplication on the smaller subset of the matrices.</p> <ul> <li><strong>Uniform Sampling</strong> samples each row with the probability $p=k/n$ of selection, effectively treating every single row as equally important [Cohen et al., 2015].<d-cite key="cohen2015uniform"></d-cite></li> <li><strong>Leverage Score Sampling</strong> selects rows based on their statistical influence called leverage scores which are either calculated using SVD or QR-decomposition [Drineas et al., 2012].<d-cite key="drineas2012fast"></d-cite></li> <li><strong>Priority and Threshold Sampling</strong> is an innovative method which selects the rows based on their “importance,” i.e., rows beyond a certain threshold are selected [Daliri et al., 2025].<d-cite key="daliri2025matrix"></d-cite></li> </ul> <h3 id="sketch-and-solve-for-transformers">Sketch and Solve for Transformers</h3> <p>Transformers, as an architecture,<d-cite key="vaswani2017attention"></d-cite> is the industry standard because it has improved context as compared to previous architectures such as LSTM and RNNs, and provides the ability to parallelize the block which means faster training and inference.</p> <p>Its architecture is split into two distinct blocks, both composed of a stack of $N$ identical layers:</p> <p><img class="img-fluid rounded" src="/2026/assets/img/2026-04-27-fastermatrices/transformers.png" alt="Transformers architecture"/></p> <p><strong>Encoder</strong></p> <p>The Encoder’s primary function is to map an input sequence of symbols $(x_1, …, x_n)$ into a rich, continuous representation $\mathbf{Z} = (z_1, …, z_n)$.</p> <p>Where each token is allowed to look at every other token available and hence it essentially understands the semantics.</p> <p><strong>Decoder</strong> The Decoder generates the output sequence $(y_1, …, y_m)$ one element at a time, being <strong>auto-regressive</strong> (consuming previously generated symbols as input for the next) instead of looking at the entire sentence like the encoder does.</p> <p>The core logic which powers both encoder and decoder-based architectures is the attention mechanism; it basically creates a better, richer representation of our input context.</p> <h3 id="attention-mechanism">Attention Mechanism</h3> <p>The input to the attention layer is our input embedding matrix $\mathbf{X}$. We project it using three learned weight matrices:</p> <ul> <li><strong>Query ($\mathbf{Q}$):</strong> What the token is looking for.</li> <li><strong>Key ($\mathbf{K}$):</strong> What the token identifies as.</li> <li><strong>Value ($\mathbf{V}$):</strong> The actual content the token holds.</li> </ul> <p><img class="img-fluid rounded" src="/2026/assets/img/2026-04-27-fastermatrices/mha.png" alt="Multihead Attention"/></p> <p>The attention score is calculated using:</p> \[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V}\] <ul> <li>First, we calculate the dot product between the Query matrix and the transpose of the Key matrix; in a way, for a token, it calculates the similarity between every single token and every other token in the sequence.</li> <li>These are then normalized by a factor of $\sqrt{d_k}$ and then we apply <strong>Softmax</strong> over it, which converts the raw scores into probabilities.</li> <li>The normalized weights are then multiplied with the matrix $\mathbf{V}$.</li> </ul> <p>Calculating this basically tells us the relationship of one token in a sequence to all the other tokens. However, the operation $\mathbf{Q}\mathbf{K}^\top$ is a very heavy operation. To compute this, we have to perform operations proportional to the square of the sequence length ($N^2$) (assuming the length of our sequence to be $N$).</p> <p>More specifically,</p> <ul> <li>If $N=512$, the attention matrix has roughly 260,000 entries.</li> <li>If $N=4096$, it jumps to over 16 million entries.</li> </ul> <p>It is quite evident that improvement with $\mathbf{Q}\mathbf{K}^\top$ can lead to improvement in the overall speed of the transformers.</p> <h3 id="better-transformers">Better Transformers</h3> <p>In regards to optimizing the attention matrix, a lot of work has already been done. Instead of computing the full matrix, architectures like the <strong>Sparse Transformer</strong> enforce a fixed sparsity pattern, such as a sliding window where tokens only attend to their immediate neighbors [Child et al., 2019].<d-cite key="child2019generating"></d-cite> This reduces the complexity to approximately $O(N\sqrt{N})$. A more dynamic approach was introduced by the <strong>Reformer</strong><d-cite key="kitaev2020reformer"></d-cite>, which replaces exact dot-product attention with Locality-Sensitive Hashing (LSH) [Kitaev et al., 2020]. By grouping similar query and key vectors into the same hash buckets, the model computes attention only within these buckets, effectively dropping the complexity to $O(N \log N)$.</p> <p>The <strong>Linformer</strong><d-cite key="wang2020linformer"></d-cite> relies on the low-rank property of the attention matrix [Wang et al., 2020]. Linformer projects the Key ($\mathbf{K}$) and Value ($\mathbf{V}$) matrices into a lower-dimensional space using learned linear projections ($\mathbf{E}$ and $\mathbf{F}$). By compressing the original $(N \times d)$ matrices into much smaller $(k \times d)$ matrices, the attention operation becomes linear $O(N)$ in time and space.</p> <p><strong>LevAttention</strong><d-cite key="kannan2025levattention"></d-cite> utilizes the concept of leverage scores (generalized $f$-sensitivities) to identify a small “universal set” of keys that dominate the attention scores for <em>any</em> query [Kannan et al., 2024]. This method proves that a subset of high-leverage keys, independent of the sequence length, captures the vast majority of the attention mass, allowing for efficient $O(N \cdot \text{poly}(d/\epsilon))$ computation.</p> <p><strong>Matrix Product Sketching via Coordinated Sampling</strong> <d-cite key="daliri2025matrix"></d-cite> proposes estimating the product $\mathbf{Q}\mathbf{K}^\top$ directly using coordinated random sampling [Daliri et al., 2025]. Unlike traditional linear sketching (such as Johnson-Lindenstrauss projections) which can be inefficient for sparse data, coordinated sampling (specifically Priority Sampling) selects rows from $\mathbf{Q}$ and $\mathbf{K}$ based on their norms using a shared random seed.</p> <p>In one way or another, all of these methods are trying to make the models faster by leveraging properties of the matrix itself or the matrix multiplication.</p> <p>Our framework addresses three fundamental questions:</p> <ol> <li><strong>Performance Comparison</strong>: How do different efficient attention mechanisms perform across diverse Hindi NLP tasks?</li> <li><strong>Cross-Attention Compatibility</strong>: Are attention mechanisms interchangeable? Can a model trained with one attention mechanism perform well with another during inference?</li> <li><strong>Task-Specific Patterns</strong>: Do certain attention mechanisms work better for specific task types (classification, sequence labeling, multiple choice)?</li> </ol> <p>To answer these questions, we developed a comprehensive pipeline that:</p> <ul> <li>Trains models with different attention mechanisms on multiple tasks</li> <li>Evaluates performance with both matching and mismatched attention mechanisms</li> <li>Provides detailed visualization and interpretability tools</li> <li>Tracks computational efficiency and environmental impact</li> </ul> <h3 id="experiment-setup">Experiment Setup</h3> <p>For the course of our experiments, we have picked the <strong>DistilBERT</strong><d-cite key="sanh2019distilbert"></d-cite> (6 encoder blocks, 12 heads each) architecture which is a distilled version of <strong>BERT</strong> that retains 97% of its performance while being 40% lighter.</p> <p><img class="img-fluid rounded" src="/2026/assets/img/2026-04-27-fastermatrices/distbert.drawio%20(1).png" alt="DistilBert"/></p> <p>We chose DistilBERT-multilingual-cased as our foundation model due to its balance between performance and efficiency. With 6 transformer layers, 768 hidden dimensions, and 12 attention heads, it provides sufficient capacity while remaining computationally manageable for our large-scale experiments. Its multilingual training includes Hindi, making it suitable for our target language.</p> <p>We then fine-tuned this model for various different tasks using our datasets and different attention mechanisms such as:</p> <ul> <li>priority sampling</li> <li>Leverage score based sampling</li> <li>Lewis score based sampling</li> <li>Learned sketches</li> <li>vanilla fine tuning</li> </ul> <p>Each attention mechanism was implemented as a custom MultiHeadSelfAttention module, maintaining the same interface as the original while implementing the specific approximation strategy.</p> <h4 id="training-the-base-models">Training The Base Models</h4> <p>For each attention mechanism, we followed an identical five-step procedure:</p> <p><strong>Step 1: Model Initialization</strong> We loaded the base DistilBERT model and configured it for each specific task type (sequence classification, token classification, or multiple choice).</p> <p><strong>Step 2: Pre-Training Attention Injection</strong> Crucially, we injected the custom attention mechanism before fine-tuning. This ensured the model learned task-specific representations through the lens of that particular attention mechanism from the beginning of training.</p> <p>The process involved iterating through all transformer layers, creating a custom attention instance, copying weights from the original attention module to preserve pre-trained knowledge, and then replacing the attention module. This surgical replacement maintained the model’s architecture while changing the attention computation mechanism.</p> <p><strong>Step 3: Task-Specific Fine-Tuning</strong> The datasets were taken from the <strong>IndicGLUE Benchmark</strong> (Hindi Language subsets), courtesy of <a href="https://indicnlp.ai4bharat.org/pages/indic-glue/">AI4Bharat</a>.<d-cite key="kakwani2020indicnlpsuite"></d-cite> We fine-tuned each model on five carefully selected Hindi NLP tasks:</p> <table> <thead> <tr> <th style="text-align: left">Task &amp; Dataset</th> <th style="text-align: left">Description</th> <th style="text-align: left">Unique Labels</th> <th style="text-align: left">Hindi Example</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Sentiment Analysis</strong><br/><code class="language-plaintext highlighter-rouge">iitp-mr.hi</code></td> <td style="text-align: left">Classifies movie reviews as positive, negative, or neutral.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">0</code> (Negative)<br/><code class="language-plaintext highlighter-rouge">1</code> (Neutral)<br/><code class="language-plaintext highlighter-rouge">2</code> (Positive)</td> <td style="text-align: left"><em>“यह फिल्म देखने लायक है, कहानी बहुत अच्छी है।”</em></td> </tr> <tr> <td style="text-align: left"><strong>News Classification</strong><br/><code class="language-plaintext highlighter-rouge">bbca.hi</code></td> <td style="text-align: left">Classifies news articles into 14 distinct topics (e.g., Sports, India).</td> <td style="text-align: left">14 Topics<br/>(<code class="language-plaintext highlighter-rouge">india</code>, <code class="language-plaintext highlighter-rouge">sport</code>, <code class="language-plaintext highlighter-rouge">entertainment</code>, etc.)</td> <td style="text-align: left"><em>“भारतीय क्रिकेट टीम ने आज ऐतिहासिक जीत दर्ज की।”</em></td> </tr> <tr> <td style="text-align: left"><strong>Discourse Mode</strong><br/><code class="language-plaintext highlighter-rouge">md.hi</code></td> <td style="text-align: left">Identifies the rhetorical role of a sentence in a narrative.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">Argumentative</code><br/><code class="language-plaintext highlighter-rouge">Descriptive</code><br/><code class="language-plaintext highlighter-rouge">Dialogic</code><br/><code class="language-plaintext highlighter-rouge">Informative</code><br/><code class="language-plaintext highlighter-rouge">Narrative</code></td> <td style="text-align: left"><em>“एक बार की बात है, एक घना जंगल था।”</em><br/>(Narrative)</td> </tr> <tr> <td style="text-align: left"><strong>Causal Reasoning (COPA)</strong><br/><code class="language-plaintext highlighter-rouge">copa.hi</code></td> <td style="text-align: left">Selects the most plausible cause or effect for a given premise.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">0</code> (Choice 1)<br/><code class="language-plaintext highlighter-rouge">1</code> (Choice 2)</td> <td style="text-align: left"><strong>Premise:</strong> <em>“लड़के का पैर फिसल गया।”</em><br/><strong>Correct:</strong> <em>“वह गिर गया।”</em></td> </tr> <tr> <td style="text-align: left"><strong>Named Entity Recognition</strong><br/><code class="language-plaintext highlighter-rouge">wiki-ner.hi</code></td> <td style="text-align: left">Tags entities like Persons, Locations, and Organizations in text.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">O</code>, <code class="language-plaintext highlighter-rouge">B-PER</code>, <code class="language-plaintext highlighter-rouge">I-PER</code><br/><code class="language-plaintext highlighter-rouge">B-ORG</code>, <code class="language-plaintext highlighter-rouge">I-ORG</code><br/><code class="language-plaintext highlighter-rouge">B-LOC</code>, <code class="language-plaintext highlighter-rouge">I-LOC</code></td> <td style="text-align: left"><em>“<strong>राहुल</strong> (B-PER) <strong>गांधी</strong> (I-PER) <strong>दिल्ली</strong> (B-LOC) में हैं।”</em></td> </tr> <tr> <td style="text-align: left"><strong>Section Title Prediction</strong><br/><code class="language-plaintext highlighter-rouge">wstp.hi</code></td> <td style="text-align: left">Predicts the correct section title for a Wikipedia paragraph.</td> <td style="text-align: left"><code class="language-plaintext highlighter-rouge">0</code> (Title A)<br/><code class="language-plaintext highlighter-rouge">1</code> (Title B)<br/><code class="language-plaintext highlighter-rouge">2</code> (Title C)<br/><code class="language-plaintext highlighter-rouge">3</code> (Title D)</td> <td style="text-align: left"><strong>Text:</strong> (Paragraph about Cricket rules)<br/><strong>Correct:</strong> <em>“नियम” (Rules)</em></td> </tr> </tbody> </table> <p><strong>Step 4: Model Deployment to HuggingFace Hub</strong> We uploaded all models with a consistent naming convention for easy programmatic access: shreshthamodi02/bert-{attention_type}-hindi-{task_name}</p> <p>This created 25 publicly available models (5 attention types × 5 tasks).</p> <p>All of the datasets were split into training and testing set and had a sequence lenght of 512</p> <p>Naturally, we wanted our matrices to be a good representation of our entire corpus and hence we kept the values of k to be in the ranges: [64,128,256]</p> <p>So for each of the dataset mentioned below, we have finetuned about 5 different models</p> <p>All the models have been fine-tuned on Kaggle using the GPU P100 and are available for inference freely on Hugging Face.</p> <h3 id="modified-attention">Modified attention</h3> <p>The goal here is not to come up with a one-size-fits-all solution but rather to find out how each of these perform in practice and the pitfalls (if any) for several of these algorithms.</p> <h4 id="learned-sketch">Learned Sketch</h4> <p>Adapting the idea from linformer, the assumption here is that the attention matrix is low rank. Hence, instead of computing the entire $N \times N$ matrix, we take smaller versions of the projected Key ($\mathbf{K}$) and Value ($\mathbf{V}$) matrices. [Wang et al., 2020].<d-cite key="wang2020linformer"></d-cite></p> <p>Those smaller projection matrices are defined as $\mathbf{E} \in \mathbb{R}^{k \times N}$ and $\mathbf{F} \in \mathbb{R}^{k \times N}$, where $k \ll N$ such that</p> \[\mathbf{K}_{proj} = \mathbf{E}\mathbf{K}, \quad \mathbf{V}_{proj} = \mathbf{F}\mathbf{V}\] <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: Q (N x d), K (N x d), V (N x d)

Compressed matrices
   K<span class="p">_</span>proj = E * K    (k x d)
   V<span class="p">_</span>proj = F * V    (k x d)

Attention calculation
   Scores = (Q * K<span class="p">_</span>proj<span class="p">^</span>T) / sqrt(d)    (N x k)

weights
   Weights = Softmax(Scores)

Final product
   Output = Weights * V<span class="p">_</span>proj   (N x d)
</code></pre></div></div> <p><strong>Leverage Scores based attention</strong></p> <p>This architecture is adopted from LevAttention, where Leverage scores are used to identify the most influential keys (rows of K), and we restrict attention computation to only these selected keys. Instead of applying this to vision-based tasks as in the original paper, we adapt this approach for natural language tasks.</p> <p><strong>Statistical Leverage Scores</strong> are used here to identify the most “influential” keys in the sequence. High leverage indicates a key that is unique or critical to the subspace [Kannan et al., 2024].<d-cite key="kannan2025levattention"></d-cite></p> <p>For a matrix $\mathbf{K} \in \mathbb{R}^{n \times d}$ with $n &gt; d$ and full column rank, the statistical leverage score of the $i$-th row $\mathbf{k}_i$ is formally defined as:</p> \[\tau_i(\mathbf{K}) = \mathbf{k}_i^\top (\mathbf{K}^\top \mathbf{K})^{-1} \mathbf{k}_i\] <p>This corresponds to the $i$-th diagonal element of the projection matrix $\mathbf{P} = \mathbf{K}(\mathbf{K}^\top \mathbf{K})^{-1}\mathbf{K}^\top$, which projects onto the column space of $\mathbf{K}$ [Drineas et al., 2012; Mahoney et al., 2011]. <d-cite key="drineas2012fast, mahoney2011randomized"></d-cite> The leverage scores satisfy $0 \leq \tau_i \leq 1$ and $\sum_{i=1}^n \tau_i = d$, providing a natural probability distribution over the rows of $\mathbf{K}$.</p> <p>For a matrix $\mathbf{K} \in \mathbb{R}^{N \times d}$, the leverage score $\tau_i$ of the $i$-th row $\mathbf{k}_i$ is defined as: \(\tau_i(\mathbf{K}) = \mathbf{k}_i (\mathbf{K}^\top \mathbf{K})^{-1} \mathbf{k}_i^\top\)</p> <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K (N x d), Q (N x d), V (N x d), budget k, damping (for stability) lambda

Leverage Scores for K:
   G = K<span class="p">^</span>T * K + lambda * I
   G<span class="p">_</span>inv = inverse(G)
   For each row i in K:
       tau<span class="p">_</span>i = K<span class="p">_</span>i * G<span class="p">_</span>inv * K<span class="p">_</span>i<span class="p">^</span>T

Universal Set U calculation:
   U = <span class="p">{</span>indices of the top-k largest tau<span class="p">_</span>i<span class="p">}</span>

Attention Mask M:
   M = matrix of -infinity (N x N)
   For all query positions i and key positions j:
       if j in U: M[i,j] = 0
       else: M[i,j] = -infinity

Attention:
   attention<span class="p">_</span>scores = (Q * K<span class="p">^</span>T) / sqrt(d) + M
   attention<span class="p">_</span>weights = softmax(attention<span class="p">_</span>scores)
   output = attention<span class="p">_</span>weights * V
</code></pre></div></div> <h4 id="l1-weights-based-attention">L1 weights based attention</h4> <p>We have also tried to use Lewis weights for the key selection instead of Leverage scores for the attention mechanism.</p> <p>Lewis Weights are a generalization of leverage scores for $\ell_1$ norms. They provide a sensitivity measure for the $\ell_1$ norm, ensuring that the selected keys capture the geometry of the data distribution more robustly [Cohen &amp; Peng, 2015].<d-cite key="cohen2015lp"></d-cite></p> <p>For a matrix $\mathbf{K} \in \mathbb{R}^{n \times d}$, the $\ell_1$ Lewis weights $w_i$ are defined as the unique values satisfying: \(w_i = \tau_i(\mathbf{W}^{-1/2}\mathbf{K})\) where $\mathbf{W} = \text{diag}(w_1, \ldots, w_n)$ is the diagonal weight matrix, and $\tau_i(\cdot)$ denotes the standard $\ell_2$ leverage score of the re-weighted matrix.</p> <p>We are using</p> <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: K (N x d), Q (N x d), V (N x d)

Compute L1 Lewis Weights for K:
   Initialize: w<span class="p">_</span>i = 1 for all i
   Iterate T times:
      K<span class="p">_</span>tilde = K / sqrt(w)
      tau = LeverageScores(K<span class="p">_</span>tilde)  
      w = sqrt(w * tau)
   S = <span class="p">{</span>indices of largest w<span class="p">_</span>i<span class="p">}</span>

 Create Mask for Attention:
   M[i,j] = 0 if j in S else -inf

Compute Attention with Mask:
   attention<span class="p">_</span>scores = (Q * K<span class="p">^</span>T) / sqrt(d) + M
   attention<span class="p">_</span>weights = softmax(attention<span class="p">_</span>scores)
   output = attention<span class="p">_</span>weights * V

</code></pre></div></div> <h4 id="priority-sampling">Priority Sampling</h4> <p>Here we are using adapting the priority sampling algorithm for distilbert.</p> <p>We select the key vectors probabilistically, where the probability of selecting a key is proportional to its squared norm, using a threshold derived from the squared Frobenius norm of the key matrix [Daliri et al., 2025].<d-cite key="daliri2025matrix"></d-cite></p> <p>More formally,</p> <p>Let $\mathbf{K} \in \mathbb{R}^{k_len \times d}$ be the key matrix for a given head. lendi<br/> The squared Frobenius norm of the key matrix is given by: \(\mathbf{A}_{\text{norm}}^2 = \|\mathbf{K}\|_F^2 = \sum_{i=1}^{k\_len} \|\mathbf{k}_i\|_2^2\)</p> <p>For a target sample size $k$, the threshold $\tau$ is: \(\tau = \frac{k}{\mathbf{A}_{\text{norm}}^2}\)</p> <p>For each key vector $\mathbf{k}_i$, generate a random hash $h_i \sim \text{Uniform}(0, 1)$. The key is selected if: \(h_i \leq \tau \cdot \|\mathbf{k}_i\|_2^2\)</p> <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: K (N x d), k, random seed s

Generate Shared Randomness:
   h<span class="p">_</span>i ~ Uniform(0, 1) for i in 1..N 

Compute Ranks :
   r<span class="p">_</span>i = h<span class="p">_</span>i / ||K<span class="p">_</span>i||<span class="p">^</span>2   // Lower rank = Higher priority

Determine Threshold:
   tau = (k+1)-th smallest value in r

Select Keys:
   S = <span class="p">{</span>i | r<span class="p">_</span>i &lt;= tau<span class="p">}</span>

Attention:
   Output = Softmax((Q * K<span class="p">_</span>S<span class="p">^</span>T) / sqrt(d)) * V<span class="p">_</span>S
</code></pre></div></div> <p>This is what the structure for our training and inference models look like. For each task, we have fine-tuned 5 different attention models as given below and have used different types of attention methods for inference as well. The following table highlights the pair of attentions used for fine-tuneing and the corresponding pairs used for inference</p> <table> <thead> <tr> <th style="text-align: left">Training Model (Fine-tuned) ↓ \ Inference Method →</th> <th style="text-align: center">Vanilla Attention</th> <th style="text-align: center">Priority Sampling</th> <th style="text-align: center">Learned Sketch</th> <th style="text-align: center">LevAttention</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Vanilla Attention</strong></td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> </tr> <tr> <td style="text-align: left"><strong>Priority Sampling</strong></td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> </tr> <tr> <td style="text-align: left"><strong>Learned Sketch</strong></td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> </tr> <tr> <td style="text-align: left"><strong>L1 Attention</strong></td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> </tr> <tr> <td style="text-align: left"><strong>LevAttention</strong></td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> <td style="text-align: center">✅</td> </tr> </tbody> </table> <h3 id="inference-methodology">Inference Methodology</h3> <p><strong>Grid Dimensions</strong>:</p> <ul> <li><strong>5 Tasks</strong>: sentiment analysis, NLI, NER, discourse classification, WSTP</li> <li><strong>5 Trained attention types</strong>: What the model was originally trained with</li> <li><strong>5 Inference attention types</strong>: What we use during evaluation</li> </ul> <p><strong>Total Experimental Conditions</strong>: 5 × 5 × 5 = 125</p> <p><strong>Core Component 1: Argument Parser (<code class="language-plaintext highlighter-rouge">setup.py</code>)</strong></p> <p>The <code class="language-plaintext highlighter-rouge">setup.py</code> file functions as the central configuration hub for the experimental framework. It orchestrates the following critical operations:</p> <ul> <li><strong>Argument Parsing:</strong> Manages command-line arguments to distinguish between <code class="language-plaintext highlighter-rouge">trained_attn_type</code> and <code class="language-plaintext highlighter-rouge">inference_attn_type</code>.</li> <li><strong>Directory Management:</strong> Automates the creation of directory structures to ensure results are stored systematically.</li> <li><strong>Dynamic Module Loading:</strong> Handles the runtime initialization of specific attention modules based on the active experiment configuration.</li> <li><strong>Convention Mapping:</strong> Resolves naming inconsistencies by mapping between training and inference identifiers.</li> </ul> <p><strong>Core Component 2: Task-Specific Inference Scripts</strong></p> <p>Each NLP task utilizes a dedicated inference script. While these scripts adhere to a unified architectural template, they are specialized to handle task-specific nuances:</p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">sentiment_analysis.py</code>:</strong> Handles binary and multi-class classification, including the extraction of confidence scores.</li> <li><strong><code class="language-plaintext highlighter-rouge">nli.py</code>:</strong> Manages Natural Language Inference tasks, focusing on multiple-choice inference via pair-wise comparison.</li> <li><strong><code class="language-plaintext highlighter-rouge">ner.py</code>:</strong> Executes token-level classification and integrates <code class="language-plaintext highlighter-rouge">seqeval</code> metrics for rigorous entity-level evaluation.</li> <li><strong><code class="language-plaintext highlighter-rouge">discourse.py</code>:</strong> Performs multi-class classification tailored for discourse analysis, incorporating mode analysis.</li> <li><strong><code class="language-plaintext highlighter-rouge">wstp.py</code>:</strong> Processes multiple-choice questions with extended context windows (four options).</li> </ol> <p><strong>Core Component 3: Metrics Collection Framework</strong></p> <p>To ensure a holistic evaluation of the attention mechanisms, every inference run captures a comprehensive suite of metrics:</p> <ul> <li><strong>Performance Metrics:</strong> Standard, task-appropriate evaluators (Accuracy, F1-Score, Precision, Recall), alongside confidence distribution analysis and calibration error checking.</li> <li><strong>Efficiency Metrics:</strong> Computational profiling, including total inference time, per-batch latency, system throughput (samples/second), and GPU memory footprint (peak vs. average).</li> <li><strong>Environmental Metrics:</strong> Ecological impact estimation, tracking carbon emissions and energy consumption via <strong>CodeCarbon</strong>.</li> <li><strong>Attention-Specific Metrics:</strong> Intrinsic analysis of the mechanism, quantifying attention pattern entropy, sparsity ratios (for sparse attention variants), and head diversity.</li> </ul> <h3 id="results">Results</h3> <p>Looking at the experiments and the attention mechanism, it is clear that there is some speedup while maintaining accuracy. However, interesting thing to note is that the benifits offered by the modifications are not uniform across all the tasks suggesting either the need for optimizing the code further or looking for alternative ways to make the methods work for more complex tasks</p> <h6 id="sentiment-analysis">Sentiment Analysis</h6> <p>The task here was to classify the sentences into one of the three given labels.</p> <div style="font-family: 'Segoe UI', sans-serif; border: 1px solid #e1e4e8; border-radius: 8px; overflow: hidden; max-width: 600px; margin-bottom: 25px;"> <div style="background-color: #f6f8fa; padding: 12px 16px; border-bottom: 1px solid #e1e4e8; font-size: 12px; font-weight: 600; color: #57606a; text-transform: uppercase; display: flex; justify-content: space-between;"> <span>Input Text</span> <span>Predicted Label</span> </div> <div style="padding: 12px 16px; border-bottom: 1px solid #eaecef; display: flex; justify-content: space-between; align-items: center; background-color: #fff;"> <div style="font-size: 15px; color: #24292e; padding-right: 15px;">और खुश भी है।</div> <div style="background-color: #dafbe1; color: #1a7f37; border: 1px solid rgba(26, 127, 55, 0.2); padding: 4px 10px; border-radius: 12px; font-size: 12px; font-weight: 600; display: inline-flex; align-items: center; white-space: nowrap;"> <span style="margin-right: 6px;">Positive</span><span>▲</span> </div> </div> <div style="padding: 12px 16px; border-bottom: 1px solid #eaecef; display: flex; justify-content: space-between; align-items: center; background-color: #fff;"> <div style="font-size: 15px; color: #24292e; padding-right: 15px;">दानिश बड़ा होता है।</div> <div style="background-color: #f6f8fa; color: #57606a; border: 1px solid rgba(87, 96, 106, 0.2); padding: 4px 10px; border-radius: 12px; font-size: 12px; font-weight: 600; display: inline-flex; align-items: center; white-space: nowrap;"> <span style="margin-right: 6px;">Neutral</span><span>•</span> </div> </div> <div style="padding: 12px 16px; display: flex; justify-content: space-between; align-items: center; background-color: #fff;"> <div style="font-size: 15px; color: #24292e; padding-right: 15px;">लंबे समय तक अटकी रही।</div> <div style="background-color: #ffebe9; color: #cf222e; border: 1px solid rgba(207, 34, 46, 0.2); padding: 4px 10px; border-radius: 12px; font-size: 12px; font-weight: 600; display: inline-flex; align-items: center; white-space: nowrap;"> <span style="margin-right: 6px;">Negative</span><span>▼</span> </div> </div> </div> <p>A good idea to see how is our model predicting is using saliency plots. This is a relatively simple task which requires the model to understand the data</p> <div class="layout-vertical"> <div> <img src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_vanilla/inference_vanilla/prediction_dashboard_Instance_Analysis.png"/> <p>1. Vanilla Attention</p> </div> <div> <img src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_priority/inference_priority_sampling/prediction_dashboard_Instance_Analysis.png"/> <p>2. Priority Sampling</p> </div> <div> <img src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_levattention/inference_levattention/prediction_dashboard_Instance_Analysis.png"/> <p>3. LevAttention</p> </div> <div> <img src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_l1_attention/inference_l1/prediction_dashboard_Instance_Analysis.png"/> <p>4. L1 attention</p> </div> <div> <img src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_learnedsketch/inference_learned_sketch/prediction_dashboard_Instance_Analysis.png"/> <p>5. Learned Sketch</p> </div> </div> <p>Let us also check the attentiion head’s distribution for the attention mechanism</p> <div class="layout-vertical"> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_vanilla/inference_vanilla/attention_histograms_Global_Dist.png"/> <p>Vanilla Attention</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_priority/inference_priority_sampling/attention_histograms_Global_Dist.png"/> <p>Priority Sampling</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_levattention/inference_levattention/attention_histograms_Global_Dist.png"/> <p>Levattention</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_learnedsketch/inference_learned_sketch/attention_histograms_Global_Dist.png"/> <p>Learned Sketch</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/sentiment_analysis/trained_l1_attention/inference_l1/attention_histograms_Global_Dist.png"/> <p>L1 attention</p> </div> </div> <p>Looking at these, it is pretty clear that L1 and Priority based attention mechanism follow the vanilla most closely. Learned is the farthest from the original distribution. It could be due to the way we initalize the projection matrices.</p> <table> <thead> <tr> <th style="text-align: left">Trained_Attn</th> <th style="text-align: left">Inference_Attn</th> <th style="text-align: left">Accuracy</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">l1_attention</td> <td style="text-align: left">levattention</td> <td style="text-align: left">0.565</td> </tr> <tr> <td style="text-align: left">l1_attention</td> <td style="text-align: left">l1</td> <td style="text-align: left">0.565</td> </tr> <tr> <td style="text-align: left">l1_attention</td> <td style="text-align: left">vanilla</td> <td style="text-align: left">0.565</td> </tr> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">vanilla</td> <td style="text-align: left">0.539</td> </tr> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">l1</td> <td style="text-align: left">0.539</td> </tr> </tbody> </table> <p>Thus, the best attention mechanism for this task is likely L1 or Leverage score based sampling</p> <p>Another interesting thing to note is that they have taken more time in training and inference as compared to the others</p> <p>In terms of time, priority is the best</p> <p>Given below is the table which contains the best performing training and inference attention method for sentiment analysis</p> <h6 id="ner">NER</h6> <p>For NER, the task here was to categorize the parts of sentences after identifying them. Given below is what the example for NER sentence in our dataset looks like</p> <div style="font-family: sans-serif; line-height: 1.8; overflow-x: auto; white-space: nowrap; padding: 10px; border: 1px solid #e0e0e0; border-radius: 5px; background: #f9f9f9;"><span style="margin: 0 2px; color: #333;">जिंदगी</span><span style="margin: 0 2px; color: #333;">एक</span><span style="margin: 0 2px; color: #333;">रिहर्सल</span><span style="background-color: #FFD1DC; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>विष्णु</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">B-PER</span></span><span style="background-color: #FFD1DC; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>प्रभाकर</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">I-PER</span></span><span style="margin: 0 2px; color: #333;">द्वारा</span><span style="margin: 0 2px; color: #333;">रचित</span><span style="margin: 0 2px; color: #333;">कहानी</span><span style="margin: 0 2px; color: #333;">संग्रह</span><span style="margin: 0 2px; color: #333;">है।</span></div> <p><br/></p> <div style="font-family: sans-serif; line-height: 1.8; overflow-x: auto; white-space: nowrap; padding: 10px; border: 1px solid #e0e0e0; border-radius: 5px; background: #f9f9f9;"><span style="background-color: #AEC6CF; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>न्यूटन</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">B-ORG</span></span><span style="background-color: #AEC6CF; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>का</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">I-ORG</span></span><span style="background-color: #AEC6CF; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>शीतलन</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">I-ORG</span></span><span style="background-color: #AEC6CF; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>का</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">I-ORG</span></span><span style="background-color: #AEC6CF; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>नियम</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">I-ORG</span></span></div> <p><br/></p> <div style="font-family: sans-serif; line-height: 1.8; overflow-x: auto; white-space: nowrap; padding: 10px; border: 1px solid #e0e0e0; border-radius: 5px; background: #f9f9f9;"><span style="background-color: #C1E1C1; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>अण्टीगुआ</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">B-LOC</span></span><span style="background-color: #C1E1C1; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>और</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">I-LOC</span></span><span style="background-color: #C1E1C1; color: #333; padding: 4px 6px; border-radius: 6px; margin: 0 3px; display: inline-block;"><strong>बारबूडा</strong> <span style="font-size: 0.75em; opacity: 0.7; font-family: monospace;">I-LOC</span></span></div> <p><br/></p> <p><em>Attention heads Distribution Plots</em></p> <div class="layout-vertical"> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_vanilla/inference_vanilla/attention_histograms_Global_Dist.png"/> <p>Vanilla</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_priority/inference_priority_sampling/attention_histograms_Global_Dist.png"/> <p>Priority Sampling</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_levattention/inference_levattention/attention_histograms_Global_Dist.png"/> <p>Levattention</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_learnedsketch/inference_learned_sketch/attention_histograms_Global_Dist.png"/> <p>Learned Sketch</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_l1_attention/inference_l1/attention_histograms_Global_Dist.png"/> <p>L1 attention</p> </div> </div> <p><em>NER Tags</em></p> <div class="layout-vertical"> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_vanilla/inference_vanilla/ner_confidence_wrapped_.png"/> <p>Vanilla</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_priority/inference_priority_sampling/ner_confidence_wrapped_.png"/> <p>Priority Sampling</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_levattention/inference_levattention/ner_confidence_wrapped_.png"/> <p>Levattention</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_learnedsketch/inference_learned_sketch/ner_confidence_wrapped_.png"/> <p>Learned Sketch</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/ner/trained_l1_attention/inference_l1/ner_confidence_wrapped_.png"/> <p>L1 attention</p> </div> </div> <table> <thead> <tr> <th style="text-align: left">Trained_Attn</th> <th style="text-align: left">Inference_Attn</th> <th style="text-align: left">Accuracy</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">priority</td> <td style="text-align: left">vanilla</td> <td style="text-align: left">0.944</td> </tr> <tr> <td style="text-align: left">priority</td> <td style="text-align: left">l1</td> <td style="text-align: left">0.944</td> </tr> <tr> <td style="text-align: left">priority</td> <td style="text-align: left">levattention</td> <td style="text-align: left">0.944</td> </tr> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">vanilla</td> <td style="text-align: left">0.941</td> </tr> <tr> <td style="text-align: left">l1_attention</td> <td style="text-align: left">vanilla</td> <td style="text-align: left">0.941</td> </tr> </tbody> </table> <p>Unsurprisingly, learned sketch performed worse as compared to the other methods here as well. Priority sampling performed the best in terms of accuracy and time and likely could be a better choice for tasks like these as compared to the other methods</p> <p>Interestingly enough, this task benifits a lot from the optimized attention mechanism</p> <p>Given below is table which shows the best performing training and inference attention pair for NER</p> <h6 id="wstp">WSTP</h6> <p>Taken from the wikipedia dataset, we used our models to predict the title of the given text. Since this is multiple choice, it is one of the more complex tasks</p> <div style="font-family: sans-serif; border: 1px solid #e0e0e0; border-radius: 8px; padding: 20px; margin-bottom: 30px; background-color: white; box-shadow: 0 2px 5px rgba(0,0,0,0.05);"> <div style="color: #888; font-size: 12px; font-weight: bold; text-transform: uppercase; margin-bottom: 10px;"> Sample 1 </div> <div style="font-size: 16px; line-height: 1.6; color: #333; margin-bottom: 20px; padding-bottom: 20px; border-bottom: 1px solid #eee;"> उन्होंने सबसे पहले ब्रिटिश राज के दौरान पूर्ण स्वराज की मांग उठाई। लोकमान्य तिलक ने जनजागृति का कार्यक्रम पूरा करने के लिए महाराष्ट्र में गणेश उत्सव तथा शिवाजी उत्सव सप्ताह भर मनाना प्रारंभ किया। इन त्योहारों के माध्यम से जनता में देशप्रेम और अंग्रेजों के अन्यायों के विरुद्ध संघर्ष का साहस भरा गया। </div> <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;"> <div style="background-color: #f7fafc; border: 1px solid #cbd5e0; color: #4a5568; padding: 10px 15px; border-radius: 6px; font-size: 14px; display: flex; justify-content: space-between; align-items: center;"> <span><span style="opacity: 0.6; margin-right: 8px;">A</span> भारतीय राष्ट्रीय कांग्रेस</span> </div> <div style="background-color: #f7fafc; border: 1px solid #cbd5e0; color: #4a5568; padding: 10px 15px; border-radius: 6px; font-size: 14px; display: flex; justify-content: space-between; align-items: center;"> <span><span style="opacity: 0.6; margin-right: 8px;">B</span> राजनीतिक यात्रा</span> </div> <div style="background-color: #f7fafc; border: 1px solid #cbd5e0; color: #4a5568; padding: 10px 15px; border-radius: 6px; font-size: 14px; display: flex; justify-content: space-between; align-items: center;"> <span><span style="opacity: 0.6; margin-right: 8px;">C</span> माण्डले में कारावास</span> </div> <div style="background-color: #e6fffa; border: 1px solid #38b2ac; color: #234e52; font-weight: bold; padding: 10px 15px; border-radius: 6px; font-size: 14px; display: flex; justify-content: space-between; align-items: center;"> <span><span style="opacity: 0.6; margin-right: 8px;">D</span> सामाजिक योगदान और विरासत</span> <span>&#10003;</span> </div> </div> </div> <p><em>Attention Heads Distribution Plots</em></p> <div class="layout-vertical"> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/wstp/trained_vanilla/inference_vanilla/attention_histograms_Global_Dist.png"/> <p>Vanilla</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/wstp/trained_priority/inference_priority_sampling/attention_histograms_Global_Dist.png"/> <p>Priority Sampling</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/wstp/trained_levattention/inference_levattention/attention_histograms_Global_Dist.png"/> <p>Levattention</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/wstp/trained_learnedsketch/inference_learned_sketch/attention_histograms_Global_Dist.png"/> <p>Learned Sketch</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/wstp/trained_l1_attention/inference_l1/attention_histograms_Global_Dist.png"/> <p>L1 attention</p> </div> </div> <p>This is the task where our modifed attention took a lot more time as compared to the vanilla mechanism which likely means that similar tasks will not be benifitted from optimizing attention here</p> <p>Given below is the table which talks about the top inference and training attention pair in terms of accuracy</p> <table> <thead> <tr> <th style="text-align: left">Trained_Attn</th> <th style="text-align: left">Inference_Attn</th> <th style="text-align: right">Accuracy</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">vanilla</td> <td style="text-align: right">0.714</td> </tr> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">l1</td> <td style="text-align: right">0.714</td> </tr> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">levattention</td> <td style="text-align: right">0.714</td> </tr> <tr> <td style="text-align: left">vanilla</td> <td style="text-align: left">vanilla</td> <td style="text-align: right">0.71</td> </tr> <tr> <td style="text-align: left">vanilla</td> <td style="text-align: left">levattention</td> <td style="text-align: right">0.71</td> </tr> </tbody> </table> <p>However, out of all of the attention mechanism, the lev attention took the least amount of time and hence in terms of time and accuracy, lev attention is the better choice for more complex tasks.</p> <h6 id="nli">NLI</h6> <p>NLI is also taken up from the AI4Bharat series of datasets. Here, we look at the given text called premise and try to identify the cause and the effect relationship</p> <div style="font-family: sans-serif; max-width: 700px;"> <div style="border: 1px solid #e5e7eb; border-radius: 8px; overflow: hidden; margin-bottom: 30px; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1); background: white;"> <div style="background-color: #fffbeb; border-bottom: 1px solid #fcd34d; padding: 12px 20px; display: flex; justify-content: space-between; align-items: center;"> <span style="color: #d97706; font-weight: 800; font-size: 12px; letter-spacing: 1px; text-transform: uppercase;">Find the CAUSE</span> <span style="font-size: 16px;"></span> </div> <div style="padding: 25px; text-align: center; border-bottom: 1px solid #f3f4f6;"> <div style="font-size: 11px; color: #9ca3af; margin-bottom: 8px; text-transform: uppercase; font-weight: 600;">Premise</div> <div style="font-size: 18px; font-weight: 500; color: #1f2937; line-height: 1.5;">मेरे शरीर ने घास पर छाया डाली।</div> </div> <div style="display: flex; flex-direction: row;"> <div style="flex: 1; padding: 20px; background-color: #ecfdf5; border-right: 1px solid #e5e7eb; opacity: 1.0;"> <div style="display: flex; justify-content: space-between; margin-bottom: 8px; color: #064e3b;"> <span style="font-size: 11px; font-weight: bold;">OPTION 1</span> <span style="font-weight: bold; font-size: 16px;">&#10003;</span> </div> <div style="font-size: 14px; color: #064e3b; font-weight: bold; line-height: 1.5;">सूरज उग रहा था।</div> </div> <div style="flex: 1; padding: 20px; background-color: #f9fafb; opacity: 0.6;"> <div style="display: flex; justify-content: space-between; margin-bottom: 8px; color: #9ca3af;"> <span style="font-size: 11px; font-weight: bold;">OPTION 2</span> <span style="font-weight: bold; font-size: 16px;"></span> </div> <div style="font-size: 14px; color: #9ca3af; font-weight: normal; line-height: 1.5;">घास काटी गई।</div> </div> </div> </div> <div style="border: 1px solid #e5e7eb; border-radius: 8px; overflow: hidden; margin-bottom: 30px; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1); background: white;"> <div style="background-color: #eff6ff; border-bottom: 1px solid #bfdbfe; padding: 12px 20px; display: flex; justify-content: space-between; align-items: center;"> <span style="color: #2563eb; font-weight: 800; font-size: 12px; letter-spacing: 1px; text-transform: uppercase;">Find the EFFECT</span> <span style="font-size: 16px;"></span> </div> <div style="padding: 25px; text-align: center; border-bottom: 1px solid #f3f4f6;"> <div style="font-size: 11px; color: #9ca3af; margin-bottom: 8px; text-transform: uppercase; font-weight: 600;">Premise</div> <div style="font-size: 18px; font-weight: 500; color: #1f2937; line-height: 1.5;">चिकित्सक ने मरीज को गलत बताया।</div> </div> <div style="display: flex; flex-direction: row;"> <div style="flex: 1; padding: 20px; background-color: #ecfdf5; border-right: 1px solid #e5e7eb; opacity: 1.0;"> <div style="display: flex; justify-content: space-between; margin-bottom: 8px; color: #064e3b;"> <span style="font-size: 11px; font-weight: bold;">OPTION 1</span> <span style="font-weight: bold; font-size: 16px;">&#10003;</span> </div> <div style="font-size: 14px; color: #064e3b; font-weight: bold; line-height: 1.5;">मरीज ने चिकित्सक के खिलाफ कदाचार का मुकदमा दायर किया।</div> </div> <div style="flex: 1; padding: 20px; background-color: #f9fafb; opacity: 0.6;"> <div style="display: flex; justify-content: space-between; margin-bottom: 8px; color: #9ca3af;"> <span style="font-size: 11px; font-weight: bold;">OPTION 2</span> <span style="font-weight: bold; font-size: 16px;"></span> </div> <div style="font-size: 14px; color: #9ca3af; font-weight: normal; line-height: 1.5;">मरीज ने चिकित्सक को गोपनीय जानकारी दी।</div> </div> </div> </div> </div> <p><em>Attention Head Distribution Plots</em></p> <div class="layout-vertical"> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/copa_hindi/trained_vanilla/inference_vanilla/attention_histograms_Global_Dist.png"/> <p>Vanilla</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/copa_hindi/trained_priority/inference_priority_sampling/attention_histograms_Global_Dist.png"/> <p>Priority Sampling</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/copa_hindi/trained_levattention/inference_levattention/attention_histograms_Global_Dist.png"/> <p>Levattention</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/copa_hindi/trained_learnedsketch/inference_learned_sketch/attention_histograms_Global_Dist.png"/> <p>Learned Sketch</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/copa_hindi/trained_l1_attention/inference_l1/attention_histograms_Global_Dist.png"/> <p>L1 attention</p> </div> </div> <p>Given below is the table for accuracy and the type of the attention model used for training and inference</p> <table> <thead> <tr> <th style="text-align: left">Trained_Attn</th> <th style="text-align: left">Inference_Attn</th> <th style="text-align: right">Accuracy</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">vanilla</td> <td style="text-align: left">priority_sampling</td> <td style="text-align: right">0.563</td> </tr> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">priority_sampling</td> <td style="text-align: right">0.563</td> </tr> <tr> <td style="text-align: left">l1_attention</td> <td style="text-align: left">learned_sketch</td> <td style="text-align: right">0.528</td> </tr> <tr> <td style="text-align: left">priority</td> <td style="text-align: left">vanilla</td> <td style="text-align: right">0.526</td> </tr> <tr> <td style="text-align: left">priority</td> <td style="text-align: left">l1</td> <td style="text-align: right">0.526</td> </tr> </tbody> </table> <p>Priority comes out as a better choice in terms of time and inference for tasks similar to NLI</p> <h6 id="discourse">Discourse</h6> <p>With discourse we are trying to understand the sementics of a sentence and the the type of the sentence</p> <div style="font-family: sans-serif; max-width: 700px; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; margin-bottom: 30px;"> <div style="background: #fcfcfc; padding: 10px 15px; border-bottom: 1px solid #eee; color: #888; font-size: 11px; font-weight: bold; letter-spacing: 1px; text-transform: uppercase;"> </div> <div style="padding: 15px; border-bottom: 1px solid #f0f0f0; display: flex; align-items: flex-start; justify-content: space-between; gap: 15px; background-color: #fff;"> <div style="font-size: 15px; line-height: 1.6; color: #333; width: 75%;">एक कबूतर पंख फडफ़ड़ाता हुआ कहवाख़ाने के अन्दर आया और कुछ लोग मिलकर उसे बाहर निकालने की कोशिश करने लगे।</div> <div style="background-color: #e3f2fd; color: #0d47a1; border: 1px solid #90caf9; padding: 4px 10px; border-radius: 20px; font-size: 12px; font-weight: 600; white-space: nowrap; display: flex; align-items: center;"> <span style="margin-right: 6px;">Narrative</span><span>📖</span> </div> </div> <div style="padding: 15px; border-bottom: 1px solid #f0f0f0; display: flex; align-items: flex-start; justify-content: space-between; gap: 15px; background-color: #fff;"> <div style="font-size: 15px; line-height: 1.6; color: #333; width: 75%;">हर महीने दस दस के पाँच नोट वो अपने ख़फ़ीफ़ तौर पर काँपते हुए हाथों से पकड़ता और अपने पुराने वज़ा के लंबे कोट की अंदरूनी जेब में रख लेता।</div> <div style="background-color: #f3e5f5; color: #4a148c; border: 1px solid #ce93d8; padding: 4px 10px; border-radius: 20px; font-size: 12px; font-weight: 600; white-space: nowrap; display: flex; align-items: center;"> <span style="margin-right: 6px;">Descriptive</span><span>👁️</span> </div> </div> <div style="padding: 15px; border-bottom: 1px solid #f0f0f0; display: flex; align-items: flex-start; justify-content: space-between; gap: 15px; background-color: #fff;"> <div style="font-size: 15px; line-height: 1.6; color: #333; width: 75%;">आख़िर शरीफ़ ख़ान-दान से तअल्लुक़ है ”</div> <div style="background-color: #fce4ec; color: #880e4f; border: 1px solid #f48fb1; padding: 4px 10px; border-radius: 20px; font-size: 12px; font-weight: 600; white-space: nowrap; display: flex; align-items: center;"> <span style="margin-right: 6px;">Dialogue</span><span>💬</span> </div> </div> </div> <p><em>Attention Heads Distribution Plots</em></p> <div class="layout-vertical"> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_vanilla/inference_vanilla/attention_histograms_Global_Dist.png"/> <p>Vanilla</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_priority/inference_priority_sampling/attention_histograms_Global_Dist.png"/> <p>Priority Sampling</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_levattention/inference_levattention/attention_histograms_Global_Dist.png"/> <p>Levattention</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_learnedsketch/inference_learned_sketch/attention_histograms_Global_Dist.png"/> <p>Learned Sketch</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_l1_attention/inference_l1/attention_histograms_Global_Dist.png"/> <p>L1 attention</p> </div> </div> <p><em>Saliency Plots</em></p> <div class="layout-vertical"> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_vanilla/inference_vanilla/saliency_class4_Discourse_Saliency.png"/> <p>Vanilla</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_priority/inference_priority_sampling/saliency_class4_Discourse_Saliency.png"/> <p>Priority Sampling</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_levattention/inference_levattention/saliency_class4_Discourse_Saliency.png"/> <p>Levattention</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_learnedsketch/inference_learned_sketch/saliency_class1_Discourse_Saliency.png"/> <p>Learned Sketch</p> </div> <div> <img class="rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_l1_attention/inference_l1/saliency_class4_Discourse_Saliency.png"/> <p>L1 attention</p> </div> </div> <p><img class="img-fluid rounded" src="/2026/assets/img/2026-04-27-fastermatrices/discourse/trained_vanilla/inference_vanilla/discourse_dashboard_Mode_Probabilities.png" alt="dashboard marker"/></p> <p>Given below are the inference and training attention pairs which are best performing for the given task</p> <table> <thead> <tr> <th style="text-align: left">Trained_Attn</th> <th style="text-align: left">Inference_Attn</th> <th style="text-align: right">Accuracy</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">vanilla</td> <td style="text-align: right">0.794</td> </tr> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">l1</td> <td style="text-align: right">0.794</td> </tr> <tr> <td style="text-align: left">levattention</td> <td style="text-align: left">levattention</td> <td style="text-align: right">0.794</td> </tr> <tr> <td style="text-align: left">l1_attention</td> <td style="text-align: left">levattention</td> <td style="text-align: right">0.794</td> </tr> <tr> <td style="text-align: left">l1_attention</td> <td style="text-align: left">l1</td> <td style="text-align: right">0.794</td> </tr> </tbody> </table> <p>Learned sketch performed poorly here as well. For tasks similar to discourse, using a vanilla fine-tuned model and using other models for inference is a better idea. Using priority sampling based attention is also a good idea for these</p> <p>There is no one-sized fits all method really as we can see. However, there are some methods which on average are better than the others let us look at them and compare the accuracies and the time taken for each of these</p> <h5 id="comparing-accuracies-across-tasks">Comparing accuracies across tasks</h5> <p><img class="img-fluid rounded" src="/2026/assets/img/2026-04-27-fastermatrices/attention_accuracy.png" alt="dashboard marker"/></p> <p>As you can see, in terms of accuracy, for simpler tasks all of the methods except learned sketch achieve good accuracy, one of the reasons for that could be the fact that the dimensions that we are sketching for could be too little to capture the nuances effectively. However, if we were to increase the dimensions then it would take significantly more time and would not achieve the promised speedup. An interesting experiment would be to reproduce this on a larger dataset with a heavier model to see if there are more speedups as compared to DistilBert architecture</p> <p>Levattention is a method which consistently matches the accuracy given by vanilla attention. For simpler tasks, all of the attention mechanisms seem to give satisfactory results but for tasks which are complex, L1 and Leverage score based attention mechanisms outperform everyone else</p> <h5 id="comparing-the-inference-across-tasks">Comparing the inference across tasks</h5> <p><img class="img-fluid rounded" src="/2026/assets/img/2026-04-27-fastermatrices/corrected_time_diff_heatmap.png" alt="Time Spent"/></p> <p>In terms of the time taken here, this graph shows the change in the percentage time per attention as compared to the vanilla.</p> <p>As accurate as leverage score based sampling is, it seems to take even more time than vanilla based attention mechanism. One of the reasons for that could be the leverage score calculation; We are yet to explore more efficient ways to calculate leverage scores. Since we are applying the leverage scores based selection only for keys, it might be the case that we need to apply it either for values or the query matrix. To utilize the full potential of this method, another interesting thing would be to also look at sharing the leverage scores across the layers and the head</p> <p>Priority sampling is a promising method as it provides speedup for simpler tasks yet it retains considerable accuracy as compared to the other methods.</p> <p>In terms of time, L1 sampling based attention is also not far behind Leverage score based sampling and hence we need to look more into the optimization of these mechanisms to better suit the architectures.</p> <p>Learned sketch offers very promising results in terms of time but the accuracy stagnates across the epoch suggesting some errors with the implementation. Regardless, the sketching based mechanisms offer greater time speedup</p> <p>In order to reap the full benifits of these methods, inputs with larger context lenght should be considered which is where they will give considerable speedup</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[This post offers a comprehensive overview of sketching and sampling algorithms for DistilBERT]]></summary></entry><entry><title type="html">The Coverage Boundary: Why High-Fidelity Primitives Don’t Compose</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/fidelity-trap/" rel="alternate" type="text/html" title="The Coverage Boundary: Why High-Fidelity Primitives Don’t Compose"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/fidelity-trap</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/fidelity-trap/"><![CDATA[<h2 id="the-question">The Question</h2> <p>Can neural networks learn compositional rules that generalize beyond their training data?</p> <p>The compositional generalization literature has established that models can succeed on <em>novel combinations of known primitives</em>: a system that learns “red circle” and “blue square” can often generate “red square” <d-cite key="keysers2020cfq,park2021benchmark"></d-cite>. But this hides a critical assumption:</p> <blockquote> <p><strong>What exactly counts as a “known” primitive?</strong></p> </blockquote> <p>Consider a generative model pre-trained to produce images of the digit 7. Does high visual fidelity, the ability to render a photorealistic 7, constitute “knowing” the digit well enough to use it compositionally in relational tasks like “generate X &gt; Y”?</p> <p>Intuitively, we assume better primitives yield better composition. If a model can generate a crisp, perfect digit, it must understand that digit.</p> <p><strong>We found the opposite.</strong></p> <p>In a controlled experiment, we show that high-fidelity primitives trained adversarially (GANs) hit a <em>glass ceiling</em> of composability, while low-fidelity “blotchy” primitives trained pedagogically achieve perfect transfer.</p> <hr/> <h2 id="background-coverage-and-compositionality">Background: Coverage and Compositionality</h2> <p>Recent work has clarified that compositional generalization is constrained by the <em>coverage</em> of primitives in training data. Benchmarks like SCAN and its descendants show that models struggle on held-out combinations when key primitives never appear in the right structural contexts <d-cite key="keysers2020cfq"></d-cite>. The <strong>Coverage Principle</strong> formalizes this: for pattern-matching learners, reliable generalization is only possible within the “coverage” of functionally equivalent fragments seen during training <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite>. In other words, coverage is a <strong>necessary condition</strong> for compositional generalization.</p> <p>Our experiments take this as a starting point. We instantiate the Coverage Principle in an intentionally simple generative setting and then ask a deeper question: <strong>even when coverage is satisfied, do all primitives admit compositional use?</strong></p> <hr/> <h2 id="the-experiment">The Experiment</h2> <p>To investigate this boundary, we designed a deliberately simple experiment using <strong>Relational MNIST</strong>. The task: generate three-digit displays of the form <code class="language-plaintext highlighter-rouge">[X][&gt;][Y]</code> where <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">Y</code> are MNIST-style digits and <code class="language-plaintext highlighter-rouge">X &gt; Y</code> numerically.</p> <p>The simplicity is intentional. MNIST is the petri dish, not the ecology. If the coverage boundary failed to appear here, in the most controlled possible environment, it would suggest the phenomenon is an artifact of complexity. That it appears so sharply in this minimal setting implies a fundamental property of neural compositionality that scale may <em>mask</em> but cannot <em>cure</em>.</p> <p>Our approach follows <strong>pedagogical training with frozen primitives</strong>. We pre-trained a <em>single-digit weaver</em> to generate individual digits <code class="language-plaintext highlighter-rouge">[0-9]</code>, then froze it and trained only a compositional layer, the <em>latent splitter</em>, to route latent codes for generating relational displays.</p> <p>Crucially, we compared two types of teachers for the primitive generator:</p> <ol> <li><strong>Adversarial (GAN):</strong> Optimized to fool a discriminator, producing sharp, high-fidelity digits rich in texture.</li> <li><strong>Pedagogical (Ours):</strong> Optimized for structural reconstruction, producing abstract, low-frequency representations that preserve structure but discard texture.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1: The Fidelity Trap. Left: Standard GAN samples, high contrast and sharp edges, but carrying pseudo-texture learned to satisfy an adversarial discriminator. Right: Our pedagogical samples, visually blotchy and diffuse, prioritizing structural clarity over textural noise. </div> <hr/> <h2 id="experimental-architecture--controls">Experimental Architecture &amp; Controls</h2> <p>We separate <em>primitive competence</em> from <em>relational competence</em> by freezing the primitive generator and training only the relational layer.</p> <ul> <li><strong>Single-Digit Weaver (Frozen).</strong> Pre-trained on digits <code class="language-plaintext highlighter-rouge">[0-9]</code> using either adversarial (GAN) or pedagogical objectives. Once trained, its weights are frozen.</li> <li><strong>Latent Splitter (Trainable).</strong> Receives a latent code and learns to route it into <code class="language-plaintext highlighter-rouge">(X, &gt;, Y)</code> displays, implementing relational structure over the same primitives.</li> <li><strong>Static Judge (Ground Truth Oracle).</strong> Evaluates whether <code class="language-plaintext highlighter-rouge">X &gt; Y</code> holds numerically. The judge is fixed and never trained. The judge sees only the rendered digits and not the internal latent codes, preventing any trivial leakage.</li> </ul> <p>Key controls:</p> <ul> <li>The primitive generator <strong>never</strong> sees the relational test set.</li> <li>The relational layer (latent splitter) is trained only on training relations; held-out relations are used purely for evaluation.</li> <li>After early experiments revealed collusion when the student could influence the teacher, we removed all student-to-teacher reward paths: the teacher’s objective depends solely on student performance as evaluated by the static judge.</li> <li>During all relational experiments, primitive generators are <strong>frozen</strong>, ensuring that differences in performance arise from the training objectives used to build primitives, not from additional fine-tuning.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2: Experimental architecture. We freeze the primitive generator (whether GAN or Pedagogical) and train only the relational routing layer. </div> <hr/> <h2 id="the-fidelity-trap">The Fidelity Trap</h2> <p>A surprising observation emerged during primitive training. The pedagogical teacher generated digits that were visually <em>blotchy</em>, diffuse, soft-edged, and structurally abstract.</p> <p>This apparent degradation acts as a <strong>semantic bottleneck</strong>. By discarding texture, the pedagogical objective forces the latent space to represent only the structural information required for compositional reasoning. Importantly, this does not imply that high-fidelity rendering is undesirable, only that it should be <em>decoupled</em> from structural learning. A plausible training recipe is two-stage: first learn the concept under a “Contract of Meaning” (low fidelity, high structure), then layer the “Contract of Appearance” (high fidelity) only after the compositional logic is secured.</p> <p>In the discriminative setting, Geirhos et al. famously showed that ImageNet-trained CNNs are strongly biased toward texture, and that increasing shape bias improves robustness and generalization <d-cite key="geirhos2018imagenet"></d-cite>. Our results suggest an analogous phenomenon on the generative side: adversarial objectives encourage texture-rich primitives that look good but compose poorly, whereas pedagogical objectives yield “blotchy” but primitives that compose perfectly.</p> <p>This matters methodologically: because our primitives are consistent with topology rather than texture, logical failures in the relational task cannot be attributed to pixel-level distribution shift. The model knew the abstract form of “7” perfectly. The only remaining question was whether it could <em>use</em> that knowledge compositionally.</p> <hr/> <h2 id="the-coverage-boundary">The Coverage Boundary</h2> <p>We first asked whether primitives could compose <em>without</em> specific relational training coverage.</p> <ul> <li> <p><strong>Condition (Phase 1.5):</strong> Train relational displays only for digits <code class="language-plaintext highlighter-rouge">[0-4]</code> (10 valid <code class="language-plaintext highlighter-rouge">X &gt; Y</code> pairs). Test on relational displays for digits <code class="language-plaintext highlighter-rouge">[5-9]</code>, which are <strong>completely unseen</strong> in relational context.</p> </li> <li> <p><strong>Result:</strong> <strong>0% digit accuracy</strong> and ~<strong>chance-level relation accuracy</strong> on the novel digits.</p> </li> </ul> <p>The model produced recognizable digits in isolation but garbage in relational contexts. This concretely instantiates the <strong>Coverage Principle</strong> <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite> in a generative setting:</p> <blockquote> <p><strong>Primitive Competence</strong> (being able to draw a 7) <strong>does not grant</strong> <strong>Compositional License</strong> (using 7 correctly in a relation).</p> </blockquote> <p>As the Coverage Principle predicts, license is only acquired when a primitive appears in a <em>relational</em> context during training. Coverage is necessary, but, as we show next, it is not sufficient.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3: The coverage boundary and glass ceiling. Same architecture, different training objectives, opposite outcomes. </div> <hr/> <h2 id="the-showdown-the-glass-ceiling-of-adversarial-training">The Showdown: The Glass Ceiling of Adversarial Training</h2> <p>Once we established that coverage is necessary, we asked the deeper question:</p> <blockquote> <p><strong>Is coverage sufficient?</strong></p> </blockquote> <p>If we give an adversarial model every advantage, full relational coverage, identical architecture, and visually superior primitives, can it match pedagogical performance?</p> <p>We ran the experiment on <strong>Novel Combinations</strong> (Phase 1.6):</p> <ul> <li> <p><strong>Training:</strong> Digits <code class="language-plaintext highlighter-rouge">[0-9]</code>, with 41 of 45 valid <code class="language-plaintext highlighter-rouge">X &gt; Y</code> pairs. We hold out four specific relations (e.g., <code class="language-plaintext highlighter-rouge">7 &gt; 3</code>, <code class="language-plaintext highlighter-rouge">8 &gt; 2</code>, <code class="language-plaintext highlighter-rouge">9 &gt; 1</code>, <code class="language-plaintext highlighter-rouge">6 &gt; 4</code>). Every digit appears in many relational contexts during training.</p> </li> <li> <p><strong>Testing:</strong> Only the 4 held-out relations. These are <em>novel combinations of seen digits</em>.</p> </li> </ul> <p><strong>Results:</strong></p> <table> <thead> <tr> <th>Training Objective</th> <th>Primitives</th> <th>Held-out Relation Accuracy</th> </tr> </thead> <tbody> <tr> <td>Pedagogical (Ours)</td> <td>Blotchy</td> <td><strong>100.0%</strong></td> </tr> <tr> <td>Adversarial (GAN)</td> <td>Crisp</td> <td><strong>81.1%</strong></td> </tr> </tbody> </table> <p>Similar symptoms have been reported at scale in text-to-image systems: models can render individual concepts with high fidelity yet catastrophically fail on compositional prompts (negation, counting, spatial relations), even when evaluation metrics like FID remain strong <d-cite key="park2021benchmark,huang2023t2i,vatsa2025rightlookswrongreasons"></d-cite>. These works document the <strong>what</strong>. Our result isolates a candidate <strong>why</strong>: adversarial objectives encourage texture-heavy representations that cannot be perfectly recomposed, even under full relational coverage.</p> <p>The adversarial model is not “broken.” 81% is not failure, it is a <em>ceiling</em>. The model had full relational coverage. It had seen every digit in compositional context. Yet it could not fully compose. A natural question is whether this 81% ceiling would disappear at larger scales. While increasing parameters or data might push performance upward by brute-force memorization of more relational pairs, the <strong>structural tax remains</strong>. The pedagogical model reaches 100% with minimal data because its primitive representations are composition-friendly from the start. In contrast, adversarially trained primitives must continually burn capacity to maintain textural fidelity. Thus the “Glass Ceiling” should be interpreted not as an absolute limit at infinite scale, but as a measure of <strong>compositional inefficiency</strong> introduced by adversarial objectives.</p> <p>This is the <strong>Glass Ceiling of Adversarial Training</strong>. The model pays a <strong>tax on composition</strong>: capacity spent maintaining textural fidelity entangles the latent space in ways that resist perfect reassembly. No amount of additional coverage can break through, because the limitation appears structural rather than statistical.</p> <p>By contrast, our pedagogical primitives, although visually worse, compose perfectly—consistent with cleaner underlying structure.</p> <hr/> <h2 id="why-it-matters-contract-of-appearance-vs-contract-of-meaning">Why It Matters: Contract of Appearance vs. Contract of Meaning</h2> <p>This experiment is a critique of how we train generative models.</p> <p>Modern practice follows a Contract of Appearance. Adversarial objectives (GANs) and preference optimization (RLHF/RLAIF) reward models for producing outputs that match surface statistics—textures, sharpness, or human-rated plausibility. Appearance is not inherently problematic; indeed, it is crucial in many applications. The difficulty emerges when appearance is optimized too early, before the underlying structure is stabilized. Premature optimization entangles texture with structure, forcing a model to satisfy a discriminator’s aesthetic constraints at the same time it is trying to learn a rule. This entanglement imposes a structural tax on composition. As our GAN results show, this produces high-fidelity primitives that look perfect to a critic but are hollow to a composer. They possess <strong>Primitive Competence</strong> but lack <strong>Compositional License</strong>. This same pattern appears in large language models trained with reinforcement learning from human feedback (RLHF/RLAIF): optimizing for human-rated plausibility can privilege surface agreement over structural understanding, with downstream costs to robustness and compositional generalization <d-cite key="vatsa2025rightlookswrongreasons"></d-cite>.</p> <p>Our pedagogical approach enforces a <strong>Contract of Meaning</strong>. By restricting the model to “blotchy,” low-frequency primitives, we create a <strong>semantic bottleneck</strong> that forces the latent space to prioritze invariant structure over texture. The model must learn the concept, the topology of the digit, because the texture is unavailable. High-fidelity appearance could be layered on <em>after</em> struture is learned, but confounding the two objectives during early training degrades compositional generalization.</p> <p>This distinction matters for safety and robustness. A model that understands meaning can be trusted to handle unseen combinations; a model trained under a Contract of Appearance may look correct while behaving unpredictably outside its training manifold.</p> <p>Although demonstrated here on MNIST for clarity, the Coverage Boundary and Glass Ceiling are <strong>architectural</strong> phenomena, not dataset quirks. Large-scale generative training (GANs, diffusion, preference tuning) may be subject to the same fidelity trap: objectives that reward appearance can actively degrade compositional reasoning, even when coverage is abundant.</p> <hr/> <h2 id="summary">Summary</h2> <table> <thead> <tr> <th>Experiment</th> <th>What’s Novel</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td><strong>Phase 1.5</strong></td> <td>Novel Primitives (No Relational Coverage)</td> <td><strong>0% Transfer</strong> — The Coverage Boundary</td> </tr> <tr> <td><strong>Phase 1.6</strong></td> <td>Adversarial Primitives (Full Coverage)</td> <td><strong>81.1% Accuracy</strong> — The Glass Ceiling</td> </tr> <tr> <td><strong>Phase 1.6</strong></td> <td>Pedagogical Primitives (Full Coverage)</td> <td><strong>100% Accuracy</strong> — The Contract of Meaning</td> </tr> </tbody> </table> <p>The Coverage Boundary tells us <em>when</em> composition is possible. The Glass Ceiling tells us <em>whether</em> the primitives are capable of it.</p> <p>You need both: primitives shaped for meaning, and coverage that licenses their use.</p> <p>Code and experimental details will be released upon acceptance.</p> <hr/> <h2 id="related-work">Related Work</h2> <p>Our setup connects to several strands of prior work. Compositional generalization benchmarks such as SCAN and its extensions highlight the importance of primitive coverage in sequence-to-sequence models <d-cite key="keysers2020cfq,friedman2022findingdatasetshortcutsgrammar"></d-cite>. The <strong>Coverage Principle</strong> of Chang et al. (2025) formalizes coverage as a necessary condition for pattern-matching learners, a condition our “Coverage Boundary” experiment instantiates in a generative regime <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite>.</p> <p>On the generative side, compositional text-to-image benchmarks repeatedly find that models with excellent perceptual quality metrics still fail on novel combinations of attributes and objects <d-cite key="park2021benchmark,huang2023t2i"></d-cite>. Vatsa et al. (2025) describe this as “right looks, wrong reasons,” emphasizing failures of compositional fidelity in modern diffusion models <d-cite key="vatsa2025rightlookswrongreasons"></d-cite>. Our <strong>Glass Ceiling</strong> result pinpoints adversarial objectives as one mechanism that can produce this pattern, even in a minimal MNIST petri dish.</p> <p>Our “blotchy but coherent” primitives resonate with work on shape vs. texture bias in CNNs <d-cite key="geirhos2018imagenet"></d-cite> and with emerging views of deep representations as learning topological manifolds amenable to symbolic or relational reuse. We view our pedagogical objective as a small, controlled example of <strong>training for meaning rather than appearance</strong>, a design choice that may scale to more realistic architectures and datasets.</p> <p>We view generative compositionality as an underexplored junction between representation learning and training objectives, and hope this minimal example encourages further mechanistic work.</p> <hr/> <h2 id="limitations--next-steps">Limitations &amp; Next Steps</h2> <p><strong>Toy domain.</strong> Our experiments use MNIST to make the phenomenon as visible and controllable as possible. Real-world data are higher dimensional and noisier, but if the fidelity trap appears in this simplest setting, we expect it to persist, if hidden, at scale.</p> <p><strong>Topology.</strong> While we infer topology from our results, claiming this will require further validation.</p> <p><strong>Frozen primitives.</strong> We freeze the digit generator when training relations to cleanly separate primitive learning from relational learning. Future work could study joint training and analyze how much compositional capacity can be recovered, or destroyed, when primitives continue to adapt.</p> <p><strong>Single relation.</strong> We focus on a single relational operator (<code class="language-plaintext highlighter-rouge">&gt;</code>). Extending to multiple relations (equality, ordering, arithmetic expressions) and to symbolic domains would test whether pedagogical primitives systematically support richer compositional logics.</p> <p><strong>Beyond MNIST.</strong> The natural next step is to apply pedagogical objectives to more complex visual and language domains, and to compare them directly against adversarial or preference-based objectives used in modern AI training pipelines.</p> <p>If the fidelity trap generalizes, then <strong>training models to teach rather than to mimic</strong> may be a necessary ingredient in building systems that truly understand, and safely extend, what they learn.</p>]]></content><author><name>Anonymous</name></author><category term="compositionality"/><category term="generalization"/><category term="neural networks"/><category term="representation learning"/><summary type="html"><![CDATA[A controlled experiment showing that adversarially trained primitives hit a glass ceiling on compositional generalization, while low-fidelity pedagogical primitives achieve perfect transfer.]]></summary></entry><entry><title type="html">Flow Where You Want</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/flow-where-you-want/" rel="alternate" type="text/html" title="Flow Where You Want"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/flow-where-you-want</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/flow-where-you-want/"><![CDATA[<div style="text-align: center; margin: 0;"> This post is also an executable notebook.<br/> Anonymized Colab Link: <a href="https://colab.research.google.com/drive/1QkU7NB3eqlPijv1b5GKuC97qdBUzzDVc?usp=sharing" target="_"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" style="display: inline-block;"/></a> <br/><br/> </div> <h1 id="introduction">Introduction</h1> <p>In this tutorial, we’ll explore inference-time “plugin” methods for flow matching and rectified flow generative models like FLUX or Stable Audio Open Small. Unlike classifier-free guidance (CFG) <d-cite key="cfg"></d-cite>, which requires training the model with your desired conditioning signal, these plugin guidance methods let you add controls at inference time—even for conditions the model never saw during training.</p> <p>This tutorial assumes familiarity with flow-based generative models, by which we mean “flow matching” <d-cite key="lipman2023flow"></d-cite> and/or “rectified flows” <d-cite key="rectified_flow"></d-cite>. See the blog posts <a href="https://drscotthawley.github.io/blog/posts/FlowModels.html">“Flow With What You Know”</a> <d-cite key="hawley2025flowwithwhat"></d-cite> and “A Visual Dive into Conditional Flow Matching” <d-cite key="gagneux2025cfm"></d-cite> for accessible overviews. The key insight is that flow models generate samples through iterative integration, and at each step we can add small velocity corrections to steer toward specific goals. This works for various objectives: generating specific classes, filling in missing regions, or satisfying other desired constraints.</p> <p>Our discussion will bring us up to date on guidance methods for latent-space rectified flow models. While there’s an extensive literature on guidance for diffusion models <d-cite key="daras2024survey"></d-cite><d-cite key="ye2024tfg"></d-cite> – see Sander Dieleman’s excellent blog post <d-cite key="dieleman2022guidance"></d-cite> for an overview — flow matching allows us to cast these in a more accessible and intuitive way. There’s some recent work unifying guidance for diffusion and flows <d-cite key="zander_greedy"></d-cite>, but in this tutorial we’ll focus on a simplified treatment for flows only.</p> <p>The paradigm of latent generative models is covered in another superb Dieleman post <d-cite key="dieleman2025latents"></d-cite>, and combining latent-space models with flow-based guidance gives us powerful, flexible tools for adding flexible controls to efficient generation.</p> <p>Let’s review the picture for flow-based generative modeling in latent space…</p> <h2 id="the-latent-flow-matching-setup">The Latent Flow-Matching Setup</h2> <p>The following diagrams illustrate the three key concepts:</p> <p><strong>a)</strong> A VAE compresses pixel-space images into compact latent representations. “E” is for encoder and “D” is for decoder:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/vae_flow_diag-VAE.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/vae_flow_diag-VAE.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>b)</strong> The flow model operates in this latent space, transforming noise (“Source”, t=0) into structured data (“Target”, t=1) through iterative integration. The decoder then converts the final latents back to pixels.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/vae_flow_diag-Generation.drawio.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/vae_flow_diag-Generation.drawio.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>c)</strong> While general flows can follow curved trajectories, some of our methods will focus on flows with nearly straight trajectories which allows for estimating endpoints without many integration steps:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/vae_flow_diag-FlowTypes.drawio.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/vae_flow_diag-FlowTypes.drawio.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>These (nearly) straight trajectories can be obtained by “ReFlow” distillation of another model (covered in <d-cite key="rectified_flow"></d-cite><d-cite key="hawley2025flowwithwhat"></d-cite> or by insisting during training that the models yield paths agreeing with Optimal Transport such as the “minibatch OT” method of Tong et al <d-cite key="tong2024improving"></d-cite>. Even if the model’s trajectories aren’t super-straight, we’ll see that the guidance methods we use can be applied fairly generally anyway.</p> <h2 id="projecting-and-correcting">Projecting and Correcting</h2> <p>Intuitively, guidance amounts to “steering” during the integration of the flow model in order to end up at a desired end point. Think of paddling a kayak on a river: you look ahead to see where the current is taking you, and if that’s not where you want to go, you steer to correct your course.</p> <p>The analogy’s not quite right: you can’t just steer, you are going to have to paddle a little bit. In other words, you’re going to have to provide a bit of a <em>extra velocity</em> to correct the where the “current” flow is taking you.</p> <p>In flow matching, we go from a source data (distribution) at time \(t=0\) to target data at \(t=1\). Since this tutorial applies to latent space, we’ll use the letter \(z\) for position, such as \(z_t\) being the position at time \(t\).</p> <p>When you’re “looking ahead” to estimate where you’ll end up, you project linearly along the current velocity \(\vec{v_t}\) for a duration of the remaining time. Let’s call this estimate \(\widehat{z_1}\), your projected endpoint :</p> \[\widehat{z_1} = z_t + (1-t)\vec{v_t}\tag{1}\] <p>…but perhaps that’s not where you want to go. Where you want to go is a distance \(\Delta \widehat{z_1}\) from \(\widehat{z_1}\), and to get there you’ll have to make a”course correction” \(\Delta \hat{v}\), as shown in the following diagram:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/guidance_vectors-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/guidance_vectors-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/guidance_vectors-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/guidance_vectors.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>By similar triangles, \(\Delta \widehat{z_1} = (1-t)\Delta \vec{v}\), which means the course correction you want is</p> \[\Delta \vec{v} = { \Delta \widehat{z_1} \over 1-t }\tag{2}\] <div class="callout-block"> <h3 id="mathematical-details">Mathematical Details</h3> <p><em>Since you’re going to see more math once you try to read the scholarly literature on these topics, let’s go a bit further into the math…</em></p> <p>So \(\Delta \widehat{z_1}\) is a measure of the <em>deviation</em> from the desired endpoint. Now, in practical application we won’t actually use the “distance” \(\Delta \widehat{z_1}\), but we’ll use something that functions <em>like</em> a distance, such as a K-L divergence or Mean Squared Error (MSE), which are familiar loss functions from neural network training.</p> <p>When doing inference, this deviation serves the same function as a “loss” does when training models something we will seek to minimize – via gradient descent! – except we’ll vary the flow positions \(z\) instead of the model weights. More specifically, we’ll consider the “likelihood” \(p( \widehat{z_1} | y )\) of getting a \(z_1\) that matches a given control \(y\), and we’ll seek to maximize that likelihood, or equivalently to <em>minimize the negative</em> log-likelihood.</p> <p>The expression \(-\nabla_{\widehat{z_1}} \log p( \widehat{z_1} | y )\) essentially answers the question, “in which direction should I adjust \(\widehat{z_1}\) so as to make \(p( \widehat{z_1} | y )\) more likely? Just like with gradient descent when training a network, this gives us a direction and a magnitude, which we then multiply by a “guidance strength” \(\eta\) (similar to a “learning rate” for gradient descent) to turn it into a step size.</p> </div> <p>Applying this gradient-based approach, our expression for $\Delta v$ will involve replacing $\Delta \widehat{z_1}$ in (2) with $- \eta \nabla_{\widehat{z_1}} \log p( \widehat{z_1} | y$:</p> \[\Delta \vec{v} = - \eta {1 \over 1-t } \nabla_{z_t} \log p( \widehat{z_1} | y ) \tag{3}\] <p>where we used the fact that $\nabla_{\widehat{z_1}} = \nabla_{z_t}$ (since $\widehat{z_1} \propto z_t$). The factor of $1/(1-t)$ means small corrections suffice early on, but later times require larger adjustments—though other time scalings are possible, as we’ll see.</p> <p>Now let’s apply this to a concrete example.</p> <h2 id="classifier-guidance">Classifier Guidance</h2> <p>If we want our model to generate a member of a particular class, we can use an external classifier to examine the generated samples. The constraint to minimize will be the difference between the desired class and the <code class="language-plaintext highlighter-rouge">argmax</code> of the classifier output (or some similar relationship that enforces the class compliance).</p> <p>For our flow model, we’ll use <a href="https://github.com/Ocrabit/dl_class_projects/blob/main/dl_experimentation/submissions/marco_submission.py">the winning submission</a> from the <a href="https://2025-dlaie-leaderboard.streamlit.app/">2025 DLAIE Leaderboard Contest</a> on unconditional latent flow matching of MNIST digits. For the classifier, we’ll use the <a href="https://github.com/DLAIE/2025-LeaderboardContest/blob/main/evaluate_submission.py">official evaluation classifier</a> from the same contest.</p> <h3 id="setup-the-flow-model-and-classifier">Setup the Flow Model and Classifier</h3> <p>Let’s generate and draw some sample images.</p> <details> <summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># generate some samples
</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">sub</span><span class="p">.</span><span class="nf">generate_samples</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">x1</span><span class="p">.</span><span class="n">shape</span>


<span class="k">def</span> <span class="nf">show_grid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">x1</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># add channels dim
</span>    <span class="n">grid</span> <span class="o">=</span> <span class="nf">make_grid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">grid</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">show_grid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="sh">"</span><span class="s">Sample generated images</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/gen_image_row_1-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/gen_image_row_1-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/gen_image_row_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/gen_image_row_1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now we’ll setup the (pretrained) classifier we’ll use for the guidance: Let’s make a plot showing the classifier’s output probabilities (aka likelihoods) across all classes, for all 10 samples. The samples will be the rows, and the class-likelihoods outputs from the classifier will be the columns, where brightness is correlated with likelihood.</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">show_probs</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">show probs as colormap intensities via imshow.
    have each row be a sample and each column be a class probability</span><span class="sh">"""</span>
    <span class="n">ncols</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="mi">2</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">ncols</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="n">axs</span> <span class="o">=</span> <span class="p">[</span><span class="n">axs</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># show a little version of the x image for each row
</span>        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="nf">make_grid</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">(),</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># show probabilities as an intensity map
</span>    <span class="n">im</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">ncols</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">probs</span><span class="p">.</span><span class="nf">cpu</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">ncols</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Class</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">ncols</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample #</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">ncols</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>

<span class="nf">show_probs</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x1</span><span class="p">)</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/show_probs_1-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/show_probs_1-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/show_probs_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/show_probs_1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>From the “random” distribution of generated digits, we see that this is an unconditional generative model: there’s nothing determining the classes of the outputs – until we add guidance, below! In a short while, we’ll reproduce that diagram, but we’ll use guidance to get one class per sample, in order, along the diagonal.</p> <p>To do that, we’re going to have to “break open” the <code class="language-plaintext highlighter-rouge">generate_samples</code> routine and even the <code class="language-plaintext highlighter-rouge">integrate_path</code> routine to allow us to <em>add a correction</em> to the velocity $v_t$ generated by the flow model at time $t$. That correction $\Delta v$ will be based on the classifier’s output using the <em>projected estimate</em> $\widehat{x_1}$ of the final data, which we’ll obtain via <em>linear extrapolation</em>.</p> <p>In our latent space model, we flow with latents $z$ which must be <em>decoded</em> using the VAE’s decoder $D$ :</p> \[\widehat{z_1} = z_t + (1-t) v_t\] \[\widehat{x_1} = D(\widehat{z_1})\] <p>The correction $\Delta v$ will generated from a constraint which in this case is just like regular “classifier loss” function in a supervised learning problem. The desired class label is the “target” and the classifier output of the projected estimate is the “prediction”.</p> <p>Our code will follow this general layout:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">v_t</span> <span class="o">=</span> <span class="nf">flow_model</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">z1_hat</span> <span class="o">=</span> <span class="n">z_t</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="n">v_t</span>               <span class="c1"># projected destination
</span><span class="n">x1_hat</span> <span class="o">=</span> <span class="n">sub</span><span class="p">.</span><span class="n">vae</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">z1_hat</span><span class="p">)</span>       <span class="c1"># decode it to pixel space
</span><span class="n">probs</span> <span class="o">=</span> <span class="nf">classify</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">x1_hat</span><span class="p">)</span>   <span class="c1"># classifer operates in pixel space
</span><span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>          <span class="c1"># "supervised learning"
</span><span class="n">delta_v</span> <span class="o">=</span> <span class="nf">magic_function</span><span class="p">(</span><span class="n">loss</span><span class="p">,...</span><span class="err">???</span><span class="p">)</span>  <span class="c1"># &lt;--- here's the part we need to work out
</span><span class="n">v_t</span> <span class="o">=</span> <span class="n">v_t</span> <span class="o">+</span> <span class="n">delta_v</span> <span class="o">*</span> <span class="n">guidance_strength</span>  <span class="c1"># we can set the strength of the correction</span></code></pre></figure> <p>To convert that <code class="language-plaintext highlighter-rouge">loss</code> into a “velocity,” we can take its gradient. When training a model, one typically takes the gradient with respect to the model weights. For inference-time guidance, however, we will take the gradient with respect to the flow coordinates $z$ in the latent space, thereby generating a vector in the latent space.</p> <p>PyTorch lets us compute the gradient with respect to anything (in the autograd graph). We just need to tell it what we want. And we need to be careful to make sure that the VAE and flow models stay frozen, so the only thing that’s allowed to change are the latents $z$.</p> <p>The cleanest way to pull this off, code-wise, is to create a function called <code class="language-plaintext highlighter-rouge">compute_v()</code> which for starters will just call the flow model, but then we’ll add to it with guidance info:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># wherever we used to just call flow_model(), we'll now call compute_v() instead
</span><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">compute_v</span><span class="p">(</span><span class="n">flow_model</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">guidance_dict</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">v_t</span> <span class="o">=</span> <span class="nf">flow_model</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">guidance_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">v_t</span> <span class="o">+=</span> <span class="nf">compute_dv</span><span class="p">(</span><span class="n">v_t</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">guidance_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">v_t</span>

<span class="nd">@torch.enable_grad</span><span class="p">()</span>  <span class="c1"># &lt;-- later, this will be a key for getting guidance
</span><span class="k">def</span> <span class="nf">compute_dv</span><span class="p">(</span><span class="n">v_t</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sh">"</span><span class="s">placeholder for now, will add guidance math later</span><span class="sh">"</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">v_t</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span> <span class="c1"># no correction yet; no gradients returned</span></code></pre></figure> <p>We’ll to use some typical “boilerplate” flow integration code, except we’ll add “<code class="language-plaintext highlighter-rouge">**kwargs</code>” everywhere so we can pass controls “all the way in” to the <code class="language-plaintext highlighter-rouge">compute_dv()</code> guidance routine, and pair <code class="language-plaintext highlighter-rouge">flow_model()</code> as an arg to <code class="language-plaintext highlighter-rouge">compute_v()</code> via <code class="language-plaintext highlighter-rouge">functools.partial</code>.</p> <details><summary>Show Flow Integration Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>  <span class="c1"># use partial to package flow_model with compute_v
</span>
<span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">rk4_step</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># regular rk4, + kwargs passthrough
</span>    <span class="c1"># f: callable (y, t) -&gt; dy/dt
</span>    <span class="n">k1</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">k2</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">k1</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">dt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">k3</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">k2</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">dt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">k4</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">k3</span><span class="p">,</span> <span class="n">t</span> <span class="o">+</span> <span class="n">dt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span> <span class="o">+</span> <span class="p">(</span><span class="n">dt</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">k1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k3</span> <span class="o">+</span> <span class="n">k4</span><span class="p">)</span>

<span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">warp_time</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Parametric Time Warping: s = slope in the middle.
        s=1 is linear time, s &lt; 1 goes slower near the middle, s&gt;1 goes slower near the ends
        s = 1.5 gets very close to the </span><span class="sh">"</span><span class="s">cosine schedule</span><span class="sh">"</span><span class="s">, i.e. (1-cos(pi*t))/2, i.e. sin^2(pi/2*x)</span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">s</span> <span class="o">&gt;</span> <span class="mf">1.5</span><span class="p">:</span> <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">s=</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s"> is out of bounds.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">tw</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span>
    <span class="k">if</span> <span class="n">dt</span><span class="p">:</span>  <span class="c1"># warped time-step requested; use derivative
</span>        <span class="k">return</span> <span class="n">tw</span><span class="p">,</span> <span class="n">dt</span> <span class="o">*</span> <span class="mi">12</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">12</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span> <span class="o">+</span> <span class="p">(</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tw</span>


<span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">integrate_path</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">initial_points</span><span class="p">,</span> <span class="n">step_fn</span><span class="o">=</span><span class="n">rk4_step</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">warp_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">latent_2d</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">t0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span>
    <span class="n">device</span><span class="p">,</span> <span class="n">model_dtype</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">dtype</span>
    <span class="n">current_points</span> <span class="o">=</span> <span class="n">initial_points</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">model_dtype</span><span class="p">).</span><span class="nf">clone</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">ts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">model_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">warp_fn</span><span class="p">:</span> <span class="n">ts</span> <span class="o">=</span> <span class="nf">warp_fn</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">latent_2d</span><span class="p">:</span> <span class="n">t_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">((</span><span class="n">current_points</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">model_dtype</span><span class="p">)</span>
    <span class="n">vel_model</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span><span class="n">compute_v</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>  <span class="c1"># here's the secret sauce
</span>    <span class="n">iterator</span> <span class="o">=</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prog_bar</span><span class="p">:</span> <span class="n">iterator</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">iterator</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Integrating Path</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
        <span class="n">t</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">ts</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">ts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">latent_2d</span><span class="p">:</span> <span class="n">t</span> <span class="o">=</span> <span class="n">t_batch</span><span class="p">.</span><span class="nf">fill_</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
        <span class="n">current_points</span> <span class="o">=</span> <span class="nf">step_fn</span><span class="p">(</span><span class="n">vel_model</span><span class="p">,</span> <span class="n">current_points</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">current_points</span>

<span class="k">def</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">z0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">t0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">z0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">([</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">sub</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">sub</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">z0</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">z0</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="nf">integrate_path</span><span class="p">(</span><span class="n">sub</span><span class="p">.</span><span class="n">flow_model</span><span class="p">,</span> <span class="n">z0</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">step_fn</span><span class="o">=</span><span class="n">rk4_step</span><span class="p">,</span> <span class="n">t0</span><span class="o">=</span><span class="n">t0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">gen_xhat</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">sub</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">gen_xhat</span></code></pre></figure> </details> <p>Now that we know that works, let’s upgrade <code class="language-plaintext highlighter-rouge">compute_dv()</code> to include the guidance correction. We’ll use the <code class="language-plaintext highlighter-rouge">torch.autograd.grad()</code> function to compute the gradient of the loss.<br/> First we have the <code class="language-plaintext highlighter-rouge">guidance_dict</code> that we’ll use to pass through our intentions through the various layers of routines to get to <code class="language-plaintext highlighter-rouge">compute_dv()</code>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">guidance_dict</span> <span class="o">=</span> \
    <span class="p">{</span><span class="sh">'</span><span class="s">classifier</span><span class="sh">'</span><span class="p">:</span> <span class="n">classifier</span><span class="p">,</span>     <span class="c1"># the classifier model to use
</span>    <span class="sh">'</span><span class="s">decode</span><span class="sh">'</span><span class="p">:</span> <span class="n">sub</span><span class="p">.</span><span class="n">decode</span><span class="p">,</span>         <span class="c1"># how to decode to pixel space for classifier
</span>    <span class="sh">'</span><span class="s">loss_fn</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">),</span> <span class="c1"># don't sum over batch dim
</span>    <span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>  <span class="c1"># desired class outcomes
</span>    <span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">:</span> <span class="mf">5.0</span><span class="p">,</span>              <span class="c1"># "guidance strength", you may vary this
</span>    <span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_max</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> <span class="c1"># t range to apply guidance, may vary these
</span>    <span class="p">}</span></code></pre></figure> <p>Next we have the fully-equipped <code class="language-plaintext highlighter-rouge">compute_dv()</code>. This code is overly-commented to make it easy to follow each step. (We replaced <code class="language-plaintext highlighter-rouge">guidance_dict</code> with <code class="language-plaintext highlighter-rouge">g</code> locally for brevity.) No other changes to any preceding code are necessary. We’ll be ready to do guided inference after this definition!</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nd">@torch.enable_grad</span><span class="p">()</span>  <span class="c1"># &lt;-- Needed to compute gradients if calling code has @torch.no_grad()
</span><span class="k">def</span> <span class="nf">compute_dv</span><span class="p">(</span><span class="n">v_t</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sh">"</span><span class="s">Compute the guidance correction to the flow velocity</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">]</span> <span class="ow">or</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">t_max</span><span class="sh">'</span><span class="p">]:</span> <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">v_t</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
    <span class="n">z</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>                   <span class="c1"># need to enable gradient tracking for z
</span>    <span class="n">z1_hat</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">v_t</span>               <span class="c1"># linear projection to estimated endpoint
</span>
    <span class="c1"># Decoding to pixel space (if decoder provided)
</span>    <span class="n">x1_hat</span> <span class="o">=</span> <span class="n">z1_hat</span> <span class="k">if</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">decode</span><span class="sh">'</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">F</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">decode</span><span class="sh">'</span><span class="p">](</span><span class="n">z1_hat</span><span class="p">)).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>

    <span class="n">logits</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="nf">classify</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">classifier</span><span class="sh">'</span><span class="p">],</span> <span class="n">x1_hat</span><span class="p">)</span>          <span class="c1"># run classifier
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">loss_fn</span><span class="sh">'</span><span class="p">](</span><span class="n">logits</span><span class="p">,</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">][:</span><span class="nf">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)])</span>     <span class="c1"># loss &lt;-&gt; "negative log likelihood"
</span>
    <span class="c1"># Compute grad wrt z. "grad_outputs=": don't sum over over batch, keep unique to each datum
</span>    <span class="n">grad_z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dv</span> <span class="o">=</span> <span class="o">-</span><span class="n">grad_z</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>   <span class="c1"># - minimizes, (1-t) makes it velocity, eps helps stability
</span>
    <span class="n">z</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>        <span class="c1"># cleanup (z is a tensor so local changes could propagate)
</span>    <span class="k">return</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">dv</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>  <span class="c1"># detach so no gradients returned</span></code></pre></figure> <p>Let’s now generate using classifier guidance on the flow model, and visualize the results:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># for reproducibility as we change other things
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
   <span class="n">x1</span> <span class="o">=</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">guidance_dict</span><span class="o">=</span><span class="n">guidance_dict</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
   <span class="n">logits</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="nf">classify</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
<span class="nf">show_probs</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x1</span><span class="p">)</span></code></pre></figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/show_probs_2-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/show_probs_2-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/show_probs_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/show_probs_2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Excellent! That was our desired goal: consecutive classes along the diagonal.</p> <p>To get a better survey of the guidance capabilities, let’s make a 10x10 grid of outputs with classes along each column:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1">#  [0,1,2,..9, 0,1,2,..9, ...]
</span><span class="n">guidance_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>         <span class="c1"># (optional) for reproducibility
</span><span class="n">x1</span> <span class="o">=</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="n">guidance_dict</span><span class="o">=</span><span class="n">guidance_dict</span><span class="p">)</span>
<span class="nf">show_grid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="sh">"</span><span class="s">Guided samples</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/class_guidance_grid-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/class_guidance_grid-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/class_guidance_grid-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/class_guidance_grid.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>That worked fine, and on a GPU it’s pretty fast, but on systems with only a CPU, it’s <em>painfully</em> slow. So instead, let’s…</p> <h2 id="train-a-latent-classifier">Train a Latent Classifier</h2> <p>We’ll train a model <code class="language-plaintext highlighter-rouge">z_classifier</code> that looks only in latent space, so we can use it as a guidance signal. This can be a very simple model consisting of a few <code class="language-plaintext highlighter-rouge">Linear</code> layers:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">LatentClassNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_d</span><span class="p">,</span> <span class="n">out_d</span><span class="p">)</span> <span class="k">for</span> <span class="n">in_d</span><span class="p">,</span> <span class="n">out_d</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:])])</span>   
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span> <span class="n">z</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">leaky_relu</span><span class="p">(</span><span class="nf">layer</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">z</span>

<span class="n">z_classifier</span> <span class="o">=</span> <span class="nc">LatentClassNet</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></code></pre></figure> <p>This classifier will operate on <em>latent</em> encodings of the MNIST dataset. So let’s save the encoded latents to disk and load them into memory. These will be our training and test data.</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#| code-fold: true
</span>
<span class="c1"># You can probably skip this code block. It just runs MNIST
# through the VAE's encoder and saves it to disk. 
</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">Subset</span>
<span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>
<span class="kn">from</span> <span class="n">glob</span> <span class="kn">import</span> <span class="n">glob</span> 
<span class="kn">import</span> <span class="n">math</span> 

<span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">encode_dataset</span><span class="p">(</span><span class="n">vae</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Encode dataset into VAE latents (z = mu), saving progress in temp chunk files.
    We use temp chunks in case execution gets interrupted, we can try again &amp; resume. 
    </span><span class="sh">"""</span>
    <span class="n">device</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">vae</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span>
    <span class="n">total_chunks</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">/</span> <span class="n">chunk_size</span><span class="p">)</span> 
    <span class="c1"># check for existing chunk files 
</span>    <span class="n">basename</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">tmp_chunk_</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="sh">"</span>
    <span class="n">chunk_files</span> <span class="o">=</span> <span class="nf">glob</span><span class="p">(</span><span class="n">basename</span><span class="o">+</span><span class="sh">'</span><span class="s">*.pt</span><span class="sh">'</span><span class="p">)</span> 
    <span class="n">existing_chunks</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">chunk_files</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s">: Found </span><span class="si">{</span><span class="n">existing_chunks</span><span class="si">}</span><span class="s"> of </span><span class="si">{</span><span class="n">total_chunks</span><span class="si">}</span><span class="s"> expected chunks. Generating remaining...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">existing_chunks</span><span class="p">,</span> <span class="n">total_chunks</span><span class="p">):</span> 
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">chunk </span><span class="si">{</span><span class="n">c</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">total_chunks</span><span class="si">}</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span><span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">c</span><span class="o">*</span><span class="n">chunk_size</span><span class="p">,</span> <span class="p">(</span><span class="n">c</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">chunk_size</span> <span class="p">))</span>
        <span class="n">data_subset</span> <span class="o">=</span> <span class="nc">Subset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>        
        <span class="n">loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">data_subset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">all_latents</span><span class="p">,</span> <span class="n">all_labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="n">must_flatten</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="p">,</span><span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">)</span> <span class="c1"># dumb progress bar 
</span>                <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="c1"># next bit is so it should work with linear layers or conv
</span>                <span class="k">if</span> <span class="n">must_flatten</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">must_flatten</span><span class="o">==</span><span class="bp">False</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span> <span class="n">z</span><span class="p">,</span> <span class="n">must_flatten</span> <span class="o">=</span> <span class="n">vae</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">False</span>
                    <span class="k">except</span> <span class="nb">RuntimeError</span><span class="p">:</span>
                        <span class="n">z</span> <span class="o">=</span> <span class="n">vae</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                        <span class="n">must_flatten</span> <span class="o">=</span> <span class="bp">True</span>
                <span class="k">else</span><span class="p">:</span> <span class="n">z</span> <span class="o">=</span> <span class="n">vae</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">z</span>
                <span class="n">all_latents</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">mu</span><span class="p">.</span><span class="nf">cpu</span><span class="p">())</span>
                <span class="n">all_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">chunk_latents</span><span class="p">,</span> <span class="n">chunk_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">all_latents</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">all_labels</span><span class="p">)</span>
        <span class="n">tmp_filename</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">basename</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">c</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">.pt</span><span class="sh">"</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">|  Saving chunk to </span><span class="si">{</span><span class="n">tmp_filename</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span> <span class="sh">'</span><span class="s">latents</span><span class="sh">'</span><span class="p">:</span><span class="n">chunk_latents</span><span class="p">,</span> <span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">:</span><span class="n">chunk_labels</span> <span class="p">},</span> <span class="n">tmp_filename</span><span class="p">)</span>
    
    <span class="c1"># Assemble all the chunks from files and return tensors.
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Assembling</span><span class="sh">"</span><span class="p">,</span> <span class="n">basename</span><span class="o">+</span><span class="sh">'</span><span class="s">*.pt ...</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">all_latents</span><span class="p">,</span> <span class="n">all_labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">chunk_file</span> <span class="ow">in</span> <span class="nf">glob</span><span class="p">(</span><span class="n">basename</span><span class="o">+</span><span class="sh">'</span><span class="s">*.pt</span><span class="sh">'</span><span class="p">):</span> 
        <span class="n">chunk_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">chunk_file</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">all_latents</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">chunk_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">latents</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">all_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">chunk_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">])</span> 
    <span class="n">latents</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">all_latents</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">all_labels</span><span class="p">)</span>
    <span class="c1">#for f in glob(tmp_file_base+'*.pt'): os.remove(f)   # clean up 
</span>    <span class="k">return</span> <span class="n">latents</span><span class="p">,</span> <span class="n">labels</span> 

<span class="k">def</span> <span class="nf">encode_mnist</span><span class="p">(</span><span class="n">vae</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Acquiring train &amp; test MNIST image datasets...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">train_ds</span> <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">())</span>
    <span class="n">test_ds</span>  <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">())</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Encoding dataset to latents...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">train_latents</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="nf">encode_dataset</span><span class="p">(</span><span class="n">vae</span><span class="p">,</span> <span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">test_latents</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="nf">encode_dataset</span><span class="p">(</span><span class="n">vae</span><span class="p">,</span> <span class="n">test_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nf">glob</span><span class="p">(</span><span class="sh">'</span><span class="s">tmp_chunk_t*_c*.pt</span><span class="sh">'</span><span class="p">):</span> <span class="n">os</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>  <span class="c1"># clean up
</span>
    <span class="k">if</span> <span class="n">filename</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Saving to </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s"> ...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">({</span> <span class="sh">'</span><span class="s">train_z</span><span class="sh">'</span><span class="p">:</span> <span class="n">train_latents</span><span class="p">,</span>     <span class="sh">'</span><span class="s">test_z</span><span class="sh">'</span><span class="p">:</span> <span class="n">test_latents</span><span class="p">,</span>
                     <span class="sh">'</span><span class="s">train_labels</span><span class="sh">'</span><span class="p">:</span> <span class="n">train_labels</span><span class="p">,</span> <span class="sh">'</span><span class="s">test_labels</span><span class="sh">'</span><span class="p">:</span> <span class="n">test_labels</span> <span class="p">},</span> <span class="n">filename</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_latents</span><span class="p">,</span> <span class="n">train_labels</span>


<span class="c1"># Encode the dataset
</span><span class="n">latent_data_filename</span> <span class="o">=</span> <span class="sh">'</span><span class="s">mnist_latents.pt</span><span class="sh">'</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">exists</span><span class="p">(</span><span class="n">latent_data_filename</span><span class="p">):</span>
    <span class="n">train_latents</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="nf">encode_mnist</span><span class="p">(</span><span class="n">sub</span><span class="p">.</span><span class="n">vae</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="n">latent_data_filename</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="k">if</span> <span class="sh">'</span><span class="s">MyDrive</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">filename</span><span class="p">:</span>
        <span class="kn">from</span> <span class="n">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
        <span class="n">drive</span><span class="p">.</span><span class="nf">mount</span><span class="p">(</span><span class="sh">'</span><span class="s">/content/drive</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">data_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data_dict</span>

<span class="n">data_dict</span> <span class="o">=</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">latent_data_filename</span><span class="p">)</span>
<span class="n">train_z</span><span class="p">,</span> <span class="n">test_z</span> <span class="o">=</span> <span class="n">data_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">train_z</span><span class="sh">'</span><span class="p">],</span> <span class="n">data_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">test_z</span><span class="sh">'</span><span class="p">]</span>
<span class="n">train_z</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_z</span><span class="p">.</span><span class="n">shape</span>

<span class="c1"># Create datasets from the latent tensors
</span><span class="n">train_latent_ds</span> <span class="o">=</span> <span class="nc">TensorDataset</span><span class="p">(</span><span class="n">train_z</span><span class="p">,</span> <span class="n">data_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">train_labels</span><span class="sh">'</span><span class="p">][:</span><span class="n">train_z</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">test_latent_ds</span> <span class="o">=</span> <span class="nc">TensorDataset</span><span class="p">(</span><span class="n">test_z</span><span class="p">,</span> <span class="n">data_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">test_labels</span><span class="sh">'</span><span class="p">])</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">train_latent_dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_latent_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_latent_dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_latent_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Train batches: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">train_latent_dl</span><span class="p">)</span><span class="si">}</span><span class="s">, Test batches: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">test_latent_dl</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># print single latent size
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Latent size: </span><span class="si">{</span><span class="n">train_latent_ds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> </details> <p>Then we’ll run the training loop…</p> <details> <summary>Training Loop Code and Execution</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Latent classifier training loop
</span><span class="n">optimizer</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">z_classifier</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">8</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">z_classifier</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">pbar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">train_latent_dl</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">latents</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="nf">z_classifier</span><span class="p">(</span><span class="n">latents</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">loss</span>   <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
        <span class="n">pbar</span><span class="p">.</span><span class="nf">set_postfix</span><span class="p">({</span><span class="sh">'</span><span class="s">train_loss</span><span class="sh">'</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">})</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
    <span class="c1"># Validation
</span>    <span class="n">z_classifier</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">val_latents</span><span class="p">,</span> <span class="n">val_labels</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">test_latent_dl</span><span class="p">))</span>
    <span class="n">val_logits</span> <span class="o">=</span> <span class="nf">z_classifier</span><span class="p">(</span><span class="n">val_latents</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">val_loss</span>   <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">val_logits</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">val_acc</span>    <span class="o">=</span> <span class="p">(</span><span class="n">val_logits</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">val_labels</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">mean</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s">: train_loss=</span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, val_loss=</span><span class="si">{</span><span class="n">val_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, val_acc=</span><span class="si">{</span><span class="n">val_acc</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> ``` Epoch 1/8: train_loss=0.9673, val_loss=1.1017, val_acc=0.6211 Epoch 2/8: train_loss=0.1901, val_loss=0.1919, val_acc=0.9375 Epoch 3/8: train_loss=0.0512, val_loss=0.1205, val_acc=0.9570 Epoch 4/8: train_loss=0.1193, val_loss=0.1022, val_acc=0.9668 Epoch 5/8: train_loss=0.0810, val_loss=0.0948, val_acc=0.9648 Epoch 6/8: train_loss=0.1569, val_loss=0.0815, val_acc=0.9707 Epoch 7/8: train_loss=0.0504, val_loss=0.0841, val_acc=0.9629 Epoch 8/8: train_loss=0.0408, val_loss=0.0792, val_acc=0.9746 ``` </details> <p>Let’s test our newly-trained latent classifier, to make sure it works before trying to use it for guidance. We’ll pull up data samples with known ground-truth “target” labels, and compare these to the predictions from the classifier. If the targets and predictions match up, we’re good to go:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">z</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="n">test_latent_ds</span><span class="p">[</span><span class="mi">20</span><span class="p">:</span><span class="mi">30</span><span class="p">]</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
<span class="nf">show_grid</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">sub</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">pred_class</span> <span class="o">=</span> <span class="nf">classify</span><span class="p">(</span><span class="n">z_classifier</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">use_argmax</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Target labels:   </span><span class="sh">"</span><span class="p">,</span><span class="n">L</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predicted labels:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pred_class</span><span class="p">.</span><span class="nf">cpu</span><span class="p">())</span></code></pre></figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/gen_image_row_2-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/gen_image_row_2-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/gen_image_row_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/gen_image_row_2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Target labels:    tensor([9, 6, 6, 5, 4, 0, 7, 4, 0, 1])
Predicted labels: tensor([9, 6, 6, 5, 4, 0, 7, 4, 0, 1])
</code></pre></div></div> <p>Good! They match up. Let’s move on…</p> <h3 id="latents-only-guidance">Latents-Only Guidance</h3> <p>Now that we have a trained classifier that operates in latent space, we can run basically the same code as before, only it will execute much faster…</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">guidance_dict</span> <span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">classifier</span><span class="sh">'</span><span class="p">:</span> <span class="n">z_classifier</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">decode</span><span class="sh">'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>    <span class="c1"># no decoding, latent space only
</span>                <span class="sh">'</span><span class="s">loss_fn</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">),</span> <span class="c1"># don't sum across batch dim
</span>                <span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                <span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">:</span> <span class="mf">5.0</span><span class="p">,</span>   <span class="c1"># "guidance strength"
</span>                <span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>  <span class="sh">'</span><span class="s">t_max</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> <span class="p">}</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># remove for new samples each time
</span><span class="n">x1</span> <span class="o">=</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">guidance_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]),</span> <span class="n">guidance_dict</span><span class="o">=</span><span class="n">guidance_dict</span><span class="p">)</span>
<span class="nf">show_grid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="sh">"</span><span class="s">Latent-Only Guidance</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/latent_guidance_grid-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/latent_guidance_grid-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/latent_guidance_grid-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/latent_guidance_grid.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>That executes very quickly, even on a CPU, and the results are just as good as before. Since we no longer have to propagate gradients through the much larger VAE decoder model and pixel-space classifer, we can get answers a lot faster via our small latents-only classifier.</p> <p>Next, we’ll explore another application of guidance, for which our guidance signal doesn’t depend on a separate trained (classifier) model at all: inpainting.</p> <h2 id="inpainting">Inpainting</h2> <p>When inpainting, we have some “mask” inside which some of the data have been removed, and we want to use the model to fill in the missing part in a way that matches with the surrounding pixels. Let’s take a look at an example from MNIST, where we show an original image, the mask and the masked-out image:</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>

<span class="n">test_ds</span>  <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">())</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">test_ds</span><span class="p">[</span><span class="mi">7</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">([</span><span class="n">H</span><span class="p">,</span><span class="n">W</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>   <span class="c1"># 1 = keep pixels
</span><span class="n">M</span><span class="p">[</span><span class="n">H</span><span class="o">//</span><span class="mi">3</span><span class="p">:</span><span class="mi">2</span><span class="o">*</span><span class="n">H</span><span class="o">//</span><span class="mi">3</span><span class="p">,</span> <span class="n">W</span><span class="o">//</span><span class="mi">3</span><span class="p">:</span><span class="mi">2</span><span class="o">*</span><span class="n">W</span><span class="o">//</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>                         <span class="c1"># 0 = mask out
</span><span class="n">x_masked</span> <span class="o">=</span> <span class="n">M</span><span class="o">*</span><span class="n">x</span>
<span class="nf">show_grid</span><span class="p">(</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x_masked</span><span class="p">],</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span><span class="sh">"</span><span class="s">      Original      |      Mask      |  Masked Image</span><span class="sh">"</span> <span class="p">)</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/inp_mask_example-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/inp_mask_example-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/inp_mask_example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/inp_mask_example.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Another example would be a picture of a face where you’ve blocked out the nose and you want the model to fill in a nose. Some of the “filling in” is obtained nearly “for free” because the model has only been exposed to data that satisfies the manifold or probability distribution of the training data – e.g. If it was trained on faces, then it only ever saw faces with noses and hence can only generate faces with noses – but the real trick is to do it “well” and have the inpainted region match up nicely with the surrounding pixels.</p> <p>There’s a wealth of information on guidance as it was originally applied to diffusion models. We recommend Sander Dieleman’s blog post, <a href="https://sander.ai/2022/05/26/guidance.html">“Guidance: a cheat code for diffusion models”</a>, for an extremely informative survey. Yet because of the stochastic/random nature of the diffusion path, there are several “complicating” aspects of diffusion guidance that we’re going to gloss over in this tutorial because in the case of deterministic, smooth flow-model trajectories, things become a lot more intuitive.</p> <p>We’ll follow a method by Pokle et al <d-cite key="pokle2024trainingfree"></d-cite>, a method that applies to general linear inverse problems of which inpainting is a particular case, and we’ll simplify their method to adapt it for <em>just inpainting.</em> For a more rigorous treatment connecting flow-based inverse problem solving to posterior sampling, see also FlowDPS <d-cite key="flowdps"></d-cite>, which extends diffusion inverse solvers into the flow framework.</p> <p>The method involves generating an <em>entire</em> new image \(x_1\) that everywhere <em>outside the mask matches up</em> with the pixels in user-supplied (masked) image \(y\). So the constraint will be, given a 2D mask \(M\) (where \(M\)=1 means there’s an original pixel there, and \(M\)=0 is the masked-out region), to require that our estimate image \(\widehat{x_1}\) (i.e. the decoded image version of the estimated latents \(\widehat{z_1}\) ) satisfies \(M*\widehat{x_1} = M* y\) <d-footnote>where "$*$" denotes the elementwise or Hadamard product</d-footnote>, or in a “residual form”, we’ll just compute the Mean Squared Error (MSE) of \(M*(\widehat{x_1}-y)\):</p> \[{\rm Constraint:} = M^2 * (\widehat{x_1}-y)^2\] <p>(and if we want, we can use the fact that \(M\) being a binary mask means \(M^2 = M\)).</p> <p>If we want to do latent-only inpainting (which will be the fastest), then the same constraint applies just with the simplification \(\widehat{x_1} = \widehat{z_1}\)</p> <p>The authors of the paper recommend only doing guidance from t equals 0.2 onward because prior to that, it’s hard to make any meaningful estimate.. In fact, they don’t even integrate before \(t = 0.2\). They just interpolate between the source and the target data to get their starting point at \(t = 0.2\).</p> <p>To use our constraint in the guidance equation (3) for computing \(\Delta v\,\), we’ll need to turn our constraint into a likelihood by raising it to an exponential power – so we get a Gaussian! But the guidance equation includes a logarithm that immediately <em>undoes</em> our exponentiation:</p> \[\Delta v = - {\eta \over 1-t} \nabla_{z_t}\ {\color{red}{\text{l̸o̸g̸}} \, \color{red}{\text{e̸x̸p̸}}} \left( M^2 * (\widehat{x_1}-y)^2 \right).\] <p>The gradient part is \(\nabla_{z_t} M^2 *(\widehat{x_1}-y)^2 = 2M^2*(\widehat{x_1}-y) {\partial \widehat{x_1} \over \partial z_t }\)</p> <p>If we’re inpainting in latent space and not using the decoder for the constraint, then \({\partial \widehat{x_1} / \partial z_t } = 1\). Otherwise that term will require evaluation via PyTorch’s <code class="language-plaintext highlighter-rouge">autograd</code> (=slow).</p> <p>Our earlier time scaling was \(\gamma_t = 1/(1-t)\); turns out that doesn’t work very well in practice when it comes to inpainting. Instead, we’ll use a different time scaling that delivers good (albeit not perfect) results: \(\gamma_t = (1-t)/t\).</p> <p>Thus our full equation for the velocity correction will be:</p> \[\Delta \vec{v} = -\eta\, \gamma_t\, M^2 *(\widehat{x_1} - y){\partial\widehat{x_1}\over\partial{z_t}}, \ \ \ \ \ \ \ \ \ \gamma_t = {1-t\over t}\] <p>where we absorbed the factor of 2 into \(\eta\), and the last partial derivative term can be one if we do latent-only inpainting.</p> <p>Let’s implement this in code, using two different versions of the gradient calculation, depending on whether we can do it all in latent space or if we need to propagate gradients through the decoder:</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nd">@torch.no_grad</span><span class="p">()</span>  <span class="c1"># gradients computed analytically!
</span><span class="k">def</span> <span class="nf">ip_latents_grad</span><span class="p">(</span><span class="n">v_t</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sh">"</span><span class="s">gradients for latent-only inpainting, fast</span><span class="sh">"</span>
    <span class="n">z1_hat</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="n">v_t</span>
    <span class="k">return</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">M_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">z1_hat</span> <span class="o">-</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">])</span>  <span class="c1">#  x1_hat = z1_hat, dz1_hat/dz_t=1
</span>
<span class="nd">@torch.enable_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">ip_pixels_grad</span><span class="p">(</span><span class="n">v_t</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sh">"</span><span class="s">gradients for pixel-space inpainting. need to use decoder &amp; track via autograd, = slow</span><span class="sh">"</span>
    <span class="n">z</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">z1_hat</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="n">v_t</span>
    <span class="n">x1_hat</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">decode</span><span class="sh">'</span><span class="p">](</span><span class="n">z1_hat</span><span class="p">)).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span> <span class="c1"># Hard-coded for 28x28; adjust for other datasets
</span>    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">M_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1_hat</span> <span class="o">-</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">grad_z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">x1_hat</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_x</span><span class="p">,</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># mults grad_x by dx1_hat/dz1_hat
</span>    <span class="n">z</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad_z</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>  <span class="c1"># don't send gradients onward
</span>
<span class="k">def</span> <span class="nf">t_timescale</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">timescale</span><span class="o">=</span><span class="sh">'</span><span class="s">mine</span><span class="sh">'</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sh">"</span><span class="s">our choice for adaptive time scale</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">timescale</span> <span class="o">==</span><span class="sh">'</span><span class="s">simple</span><span class="sh">'</span><span class="p">:</span> <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span>                          <span class="c1"># our earlier scale; doesn't work
</span>    <span class="k">elif</span> <span class="n">timescale</span><span class="o">==</span><span class="sh">'</span><span class="s">pokle</span><span class="sh">'</span><span class="p">:</span> <span class="nf">return </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">t</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>     <span class="c1"># from pokle et al; can't get it to work
</span>    <span class="k">elif</span> <span class="n">timescale</span><span class="o">==</span><span class="sh">'</span><span class="s">constant</span><span class="sh">'</span><span class="p">:</span> <span class="k">return</span> <span class="mi">4</span>  <span class="c1"># or any constant. The 4 is from Pokle et al
</span>    <span class="k">else</span><span class="p">:</span> <span class="nf">return </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">/</span><span class="n">t</span>  <span class="c1"># This works pretty well! strong guidance at start -&gt; zero at end
</span>
<span class="k">def</span> <span class="nf">compute_dv_inpainting</span><span class="p">(</span><span class="n">v_t</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sh">"</span><span class="s">wrapper to call appropriate gradient-computation routine</span><span class="sh">"</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">]</span> <span class="ow">or</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">t_max</span><span class="sh">'</span><span class="p">]:</span> <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">v_t</span><span class="p">)</span>
    <span class="n">grad_fn</span> <span class="o">=</span> <span class="n">ip_latents_grad</span> <span class="k">if</span> <span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">decode</span><span class="sh">'</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">ip_pixels_grad</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">v_t</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">dv</span> <span class="o">=</span> <span class="o">-</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="nf">t_timescale</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="k">return</span> <span class="n">dv</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span></code></pre></figure> </details> <h3 id="do-the-inpainting">Do the Inpainting</h3> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">test_ds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">)])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">M</span><span class="o">*</span><span class="n">y</span>
<span class="nf">show_grid</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span> <span class="sh">"</span><span class="s">Masked Images</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/inp_masked_images-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/inp_masked_images-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/inp_masked_images-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/inp_masked_images.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>And now we run the inpainting code…</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">compute_dv</span> <span class="o">=</span> <span class="n">compute_dv_inpainting</span>  <span class="c1"># register our new guidance routine
</span>
<span class="n">inpainting_dict</span> <span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">decode</span><span class="sh">'</span><span class="p">:</span> <span class="n">sub</span><span class="p">.</span><span class="n">decode</span><span class="p">,</span>         <span class="c1"># how to decode to pixel space for classifier
</span>                <span class="sh">'</span><span class="s">M_sq</span><span class="sh">'</span><span class="p">:</span> <span class="p">(</span><span class="n">M</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">:</span> <span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                <span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>              <span class="c1"># "guidance strength", you may vary this
</span>                <span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_max</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.999</span><span class="p">}</span> <span class="c1"># t range to apply guidance, may vary these
</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># for reproducibility as we change other things
</span>    <span class="n">t0</span> <span class="o">=</span> <span class="mf">0.2</span>             <span class="c1"># starting time as per Pokle et al
</span>    <span class="n">z0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">([</span><span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">sub</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">sub</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">zy</span> <span class="o">=</span> <span class="n">sub</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>   <span class="c1"># encoded version of masked image
</span>    <span class="n">z0</span> <span class="o">=</span> <span class="n">z0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span> <span class="o">+</span> <span class="n">zy</span> <span class="o">*</span> <span class="n">t0</span>      <span class="c1"># interpolation init
</span>    <span class="n">inpainting_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">t0</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">t0</span><span class="o">=</span><span class="n">t0</span><span class="p">,</span> <span class="n">z0</span><span class="o">=</span><span class="n">z0</span><span class="p">,</span> <span class="n">guidance_dict</span><span class="o">=</span><span class="n">inpainting_dict</span><span class="p">,</span> <span class="n">warp_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="nf">show_grid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="sh">"</span><span class="s">Inpainted Images</span><span class="sh">"</span><span class="p">)</span> </code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/inp_inpainted_images-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/inp_inpainted_images-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/inp_inpainted_images-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/inp_inpainted_images.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We see that the generated images generally look great, although in some cases, the in-painting code has changed pixels even where the mask is 1. We can disallow this by just resetting those values to the pixels in $y$.</p> <p>Turning up the guidance strength would also enforce our constraint better, but turning up too high causes the whole thing to diverge and we get garbage out.</p> <p>In order to experiment with other methods more easily, we should do inpainting only in latent space, and for that we will need a model that supports spatial latents….</p> <h3 id="latent-only-inpainting">Latent-Only Inpainting</h3> <p>To do inpainting in latent space, we’ll need to switch models to one where the latents preserve the spatial structure of the original images.</p> <details> <summary>Get the spatial-latents model</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Get Spatial VAE &amp; FLow DiT Model
</span><span class="err">!</span><span class="n">wget</span> <span class="o">-</span><span class="n">q</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">clobber</span><span class="o">=</span><span class="n">off</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="p">.</span><span class="n">githubusercontent</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">dlaieburner</span><span class="o">/</span><span class="mi">2025</span><span class="o">-</span><span class="n">leaderboard</span><span class="o">/</span><span class="n">refs</span><span class="o">/</span><span class="n">heads</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">sample_submission_dit</span><span class="p">.</span><span class="n">py</span>

<span class="k">try</span><span class="p">:</span>
    <span class="k">del</span> <span class="n">SubmissionInterface</span> <span class="c1"># remove Marco's from earlier; make it reload
</span><span class="k">except</span> <span class="nb">NameError</span><span class="p">:</span>
    <span class="k">pass</span>  <span class="c1"># nevermind
</span>
<span class="kn">from</span> <span class="n">sample_submission_dit</span> <span class="kn">import</span> <span class="n">SubmissionInterface</span>

<span class="n">sub</span> <span class="o">=</span> <span class="nc">SubmissionInterface</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></code></pre></figure> </details> <p>Let’s take a look at the images and their spatial-latent representations:</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#| code-fold: true
</span>
<span class="c1"># viz images and spatial latents
</span><span class="kn">from</span> <span class="n">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="n">test_ds</span>  <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="nc">ToTensor</span><span class="p">())</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">test_ds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)])</span>
<span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">show_grid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">Images</span><span class="sh">"</span><span class="p">)</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">sub</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">show_grid</span><span class="p">((</span><span class="n">z1</span><span class="o">-</span><span class="n">z1</span><span class="p">.</span><span class="nf">min</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">z1</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span><span class="o">-</span><span class="n">z1</span><span class="p">.</span><span class="nf">min</span><span class="p">()),</span> <span class="sh">"</span><span class="s">Latents</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/spatial_latents_viz1-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/spatial_latents_viz1-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/spatial_latents_viz1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/spatial_latents_viz1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/spatial_latents_viz2-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/spatial_latents_viz2-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/spatial_latents_viz2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/spatial_latents_viz2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now we can run latent-only inpainting using the same code as before, only this time with the spatial-latent model and with <code class="language-plaintext highlighter-rouge">decode=None</code> in the guidance dictionary:</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">prepare_latent_mask</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">encoder</span><span class="p">):</span>
    <span class="n">z_y</span> <span class="o">=</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">M_z</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">interpolate</span><span class="p">(</span><span class="n">mask</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">z_y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:],</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">bilinear</span><span class="sh">'</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">M_z</span> <span class="o">=</span> <span class="p">(</span><span class="n">M_z</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>    <span class="c1"># binarize it
</span>    <span class="k">return</span> <span class="n">z_y</span> <span class="o">*</span> <span class="n">M_z</span><span class="p">,</span> <span class="n">M_z</span><span class="o">**</span><span class="mi">2</span>

<span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">latents_only_inpaint</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">inpainting_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">t0</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">warp_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">z_y</span><span class="p">,</span> <span class="n">M_sq</span> <span class="o">=</span> <span class="nf">prepare_latent_mask</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">sub</span><span class="p">.</span><span class="n">encode</span><span class="p">)</span>   
    <span class="n">inpainting_dict</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">'</span><span class="s">M_sq</span><span class="sh">'</span><span class="p">:</span> <span class="n">M_sq</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">:</span> <span class="n">z_y</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">:</span> <span class="n">t0</span><span class="p">})</span>
    <span class="n">z0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">z_y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span> <span class="o">+</span> <span class="n">z_y</span> <span class="o">*</span> <span class="n">t0</span>         <span class="c1"># Initialize via interpolation
</span>    <span class="k">return</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">t0</span><span class="o">=</span><span class="n">t0</span><span class="p">,</span> <span class="n">z0</span><span class="o">=</span><span class="n">z0</span><span class="p">,</span>  <span class="n">guidance_dict</span><span class="o">=</span><span class="n">inpainting_dict</span><span class="p">,</span> <span class="n">warp_fn</span><span class="o">=</span><span class="n">warp_fn</span><span class="p">)</span>

<span class="n">inpainting_dict</span> <span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">decode</span><span class="sh">'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>            <span class="c1"># now we're latents-only
</span>                <span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>             <span class="c1"># "guidance strength", you may vary this
</span>                <span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_max</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.999</span><span class="p">}</span> <span class="c1"># t range to apply guidance, may vary these
</span>
<span class="nf">show_grid</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span> <span class="sh">"</span><span class="s">pixel y</span><span class="sh">'</span><span class="s">s</span><span class="sh">"</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="nf">latents_only_inpaint</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">inpainting_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">show_grid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="sh">"</span><span class="s">inpainted images</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/latents_only_pixel_ys-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/latents_only_pixel_ys-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/latents_only_pixel_ys-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/latents_only_pixel_ys.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/latents_only_inpainted_images-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/latents_only_inpainted_images-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/latents_only_inpainted_images-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/latents_only_inpainted_images.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As before, the latent-only execution is very fast, and the results are quite good. There are some limitations to what’s being produced. The problem is due to the low resolution of our latents. Inpainting algorithms typically assume higher resolution in order to work accurately. But this lesson was designed to run quickly on CPUs and thus there is a trade-off.</p> <p>Now, if we want to enforce the original pixels where the mask is one, we can do that at every stage of the integration process. We just need to modify the integration process to overwrite $z$ wherever <code class="language-plaintext highlighter-rouge">Mz</code> is 1.</p> <p>Next up is a different method that can achieve similar results, albeit differently…</p> <h1 id="pnp-flow-guidance-by-another-name">PnP-Flow: Guidance By Another Name</h1> <p>The term ‘guidance’ typically refers to velocity modifications, but PnP-Flow by Martin et al <d-cite key="pnp_flow"></d-cite> achieves similar results by adjusting latent positions $z$ directly.<d-footnote>The paper by Pokle et al we cited earlier also included a related method, however the PnP-Flow method isn't restricted to linear problems. Plus, Anne Gagneux provided [code](https://github.com/annegnx/PnP-Flow) for PnP-Flow! Gagneux's repo even provides code for the position-only (non-velocity) algorithm from Pokle et al aka "OT-ODE".</d-footnote>\(^,\)<d-footnote>Differences between our variables and those in the PnP-Flow paper: For us, $z$ are integrated flow latent variables between $z_0$ (source) and $z_1$ (target), whereas $x$ are the pixel-space representations via our VAE's decoder $D$ such that $D(z)=x$. In PnP-Flow, $x$ is the integrated flow variable, $z$ is used only for their interpolation/overwrite step, and $D$ is the "denoiser" aka their flow model.</d-footnote></p> <p>PnP-Flow assumes straight-line trajectories, making the forward projection trivial: $\widehat{z_1}$ is reached by simple linear extrapolation. Instead of incrementally moving $z$ from $t=0$ to $t=1$, PnP-Flow projects forward to $\widehat{z_1}$ and iterates on that estimate through a series of correction and projection steps. The first step applies our gradient correction:</p> \[{\rm Step\ 1.}\ \ \ \ \ \ \ \ \ \ \ \ z_1^* := \widehat{z_1} - \eta\,\gamma_t \nabla F(\widehat{z_1},y)\] <p>where $z_1^*$ (my notation) is our goal i.e. the endpoint of our projected course correction, and $F(\widehat{z_1},y)$ is our (log-exp probability) constraint. For the time scaling, the PnP-Flow authors recommend $\gamma_t = (1-t)^\alpha$ with $\alpha \in [0,1]$ is a hyperparameter chosen according to the task – e.g., they use $\alpha$’s as large as 0.8 for denoising tasks, 0.5 for box inpainting, and 0.01 for random inpainting. This choice of $\gamma_t$ is a bit different from our earlier one of $(1-t)/t$. Both go to zero as $t \rightarrow 1$, but approach it differently and have different asymptotics as $t\rightarrow 0$.</p> <p>In the graph below, we show our earlier choice of $(1 - t)/t$ in green and $(1 - t)^\alpha$ in purple for various choices of $\alpha$:</p> <center> <a href="https://www.desmos.com/calculator/bcp2wiyyid"> <iframe src="https://www.desmos.com/calculator/bcp2wiyyid?embed" frameborder="0" scrolling="no" height="300px" width="200px"></iframe> <br/>Interactive Desmos Graph Link</a><br/><br/></center> <p>…where for “box inpainting” as we did above, they use $\alpha$=0.5.</p> <p>But PnP-Flow doesn’t stop there! Two other key steps remain. We then project backward to <em>overwrite</em> $z_t$ with a corrected value:</p> \[{\rm Step\ 2.}\ \ \ \ \ \ \ \ \ \ \ \ z_t := (1-t)\,z_0 + t\, z_1^*\] <p>We then compute a new projected estimate, same as we have before:</p> \[{\rm Step\ 3.}\ \ \ \ \ \ \widehat{z_1} := z_t + (1-t)\,v_t(z,t)\] <p>….and loop over Steps 1 to 3 for each value of $t$ in our set of (discrete) integration steps, i.e. after Step 3, we let $t := t+\Delta\,t$ and go back to Step 1. Our final value of $\widehat{z_1}$ will be the output.</p> <p>This image from the PnP-Flow paper may prove instructive, showing 3 different instances of the 3 PNP steps:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/pnp_flow_steps-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/pnp_flow_steps-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/pnp_flow_steps-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/pnp_flow_steps.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This has a superficial resemblance to the “<a href="https://github.com/Stability-AI/stable-audio-tools/blob/31932349d98c550c48711e7a5a40b24aa3d7c509/stable_audio_tools/inference/sampling.py#L221">ping-pong</a>” integration method used by the flow model Stable Audio Open Small (SAOS) <d-cite key="sao_small"></d-cite>, with a key distinction: the ping-pong integrator and updates the time-integrated latent variable $z$ (called “$x$” in SAOS), whereas for PnP-Flow it is the projection $\widehat{z_1}$ (called <a href="https://github.com/Stability-AI/stable-audio-tools/blob/31932349d98c550c48711e7a5a40b24aa3d7c509/stable_audio_tools/inference/sampling.py#L242">“denoised”</a> in SAOS) that is the primary variable that is maintained between steps. This is a subtle distinction but worth noting.<d-footnote>The near trivial nature of integration for near-OT paths of flow models means that one can implement a flow version of DITTO <d-cite key="ditto"></d-cite> that avoids the expensive back-integration needed for diffusion models. The result is also superficially similar to PnP-Flow, but with gradient steps applied to $z$ instead of $\widehat{z_1}$.</d-footnote></p> <p>To implement PnP-Flow in code, let’s replace our “integrator” with something specific to PnP-Flow:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">sample_pnpflow</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">z0</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_avg</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">warp_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">guidance_dict</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">ts</span> <span class="o">=</span> <span class="nf">warp_fn</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">z0</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">z0</span><span class="p">.</span><span class="n">dtype</span><span class="p">))</span>  
    <span class="n">z1_hat</span> <span class="o">=</span> <span class="n">z0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">ts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">guidance_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">M_sq</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">z1_hat</span> <span class="o">-</span> <span class="n">guidance_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">gamma_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">**</span> <span class="n">alpha</span>
        <span class="n">z1_star</span> <span class="o">=</span> <span class="n">z1_hat</span> <span class="o">-</span> <span class="n">guidance_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">gamma_t</span> <span class="o">*</span> <span class="n">grad</span>        
        <span class="n">projections</span> <span class="o">=</span> <span class="p">[]</span>           <span class="c1"># Average multiple noisy projections
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_avg</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">z1_star</span>  <span class="o">+</span>  <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">z1_star</span><span class="p">)</span> 
            <span class="n">projections</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">z</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="nf">model</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
        <span class="n">z1_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">projections</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z1_hat</span></code></pre></figure> <p>The model we used earlier for latents-only inpainting was trained to have straight trajectories, so we should be able to use it again here, just calling <code class="language-plaintext highlighter-rouge">sample_pnpflow</code> (instead of <code class="language-plaintext highlighter-rouge">integrate_path</code>). The results are as follows:</p> <details><summary>Show Code</summary> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1">#| code-fold: true
</span><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">pnp_flow_inpaint</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">inpainting_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">t0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Inpaint using PnP-Flow method</span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">z_y</span><span class="p">,</span> <span class="n">M_sq</span> <span class="o">=</span> <span class="nf">prepare_latent_mask</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">sub</span><span class="p">.</span><span class="n">encode</span><span class="p">)</span>
    <span class="n">inpainting_dict</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">'</span><span class="s">M_sq</span><span class="sh">'</span><span class="p">:</span> <span class="n">M_sq</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">:</span> <span class="n">z_y</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">:</span> <span class="n">t0</span><span class="p">})</span>
    <span class="n">z0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">z_y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t0</span><span class="p">)</span> <span class="o">+</span> <span class="n">z_y</span> <span class="o">*</span> <span class="n">t0</span>
    <span class="k">return</span> <span class="nf">sample_pnpflow</span><span class="p">(</span><span class="n">sub</span><span class="p">.</span><span class="n">flow_model</span><span class="p">,</span> <span class="n">z0</span><span class="p">,</span> <span class="n">guidance_dict</span><span class="o">=</span><span class="n">inpainting_dict</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="n">inpainting_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_min</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="sh">'</span><span class="s">t_max</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.999</span><span class="p">}</span>

<span class="nf">show_grid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="sh">"</span><span class="s">Masked pixel images (y)</span><span class="sh">"</span><span class="p">)</span>

<span class="n">z1</span> <span class="o">=</span> <span class="nf">pnp_flow_inpaint</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">inpainting_dict</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">sub</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z1</span><span class="p">)).</span><span class="nf">cpu</span><span class="p">()</span>   <span class="c1"># convert latents to pixels
</span><span class="nf">show_grid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Inpainted images (alpha=0.5, strength=</span><span class="si">{</span><span class="n">inpainting_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">strength</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> </details> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/pnpflow_masked_ys-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/pnpflow_masked_ys-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/pnpflow_masked_ys-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/pnpflow_masked_ys.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-where-you-want/pnpflow_inpainted_images-480.webp 480w,/2026/assets/img/2026-04-27-flow-where-you-want/pnpflow_inpainted_images-800.webp 800w,/2026/assets/img/2026-04-27-flow-where-you-want/pnpflow_inpainted_images-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-flow-where-you-want/pnpflow_inpainted_images.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Thus we see that works, though it also seems to “take some liberties”: most of the letters in the second group look like “boldface” versions of the top ones. This could be because of the low spatial resolution of the latents, e.g. that they are encoding information about curvature or other aspects of the shape.</p> <p>The results are similar to the previous inpainting method of Pokle et al, just a different way of doing it that may prove worthwhile.</p> <p>PnP-Flow is a general guidance method not limited to inpainting or even “linear” image degradations. I recommend looking into it further for other tasks and datasets — and let me know what sort of results you find!</p> <h1 id="summary">Summary</h1> <p>Training generative models can be expensive—lots of data, electricity, compute time. So what if you could take a pretrained model and add controls at inference time instead? That’s what this tutorial explored. While similar ideas are emerging for steering autoregressive models <d-cite key="zhao2025steeringautoregressive"></d-cite>, here we focused on pretrained flow models.</p> <p>The key idea is simple: at each integration step, you project forward to estimate where you’ll end up ($\,\widehat{z_1}\,$) , check how far that is from where you want to be, and add a small velocity correction to steer toward your goal. We applied this to an unconditional MNIST flow model for two tasks: generating specific digit classes via classifier guidance, and filling in masked-out regions via inpainting.</p> <p>We looked at four approaches. First, standard classifier guidance in pixel space—it works but it’s slow because you’re propagating gradients through the VAE decoder. Second, we trained a simple latent-space classifier and did the same thing much faster. Third, we implemented the linear inpainting method from Pokle et al, which operates directly on latents. Fourth, we tried PnP-Flow, which achieves guidance not by correcting velocities but by iteratively projecting samples forward and backward in time.</p> <p>The math here is much simpler than for typical diffusion methods because flow trajectories are smooth and deterministic. We’ve glossed over a lot of detail compared to the research papers, but hopefully this gives you enough to experiment with your own controls. There are limits to the effectiveness of guidance: small models that don’t generalize well won’t suddenly work miracles if you try to push them too far outside their training distribution. Nevertheless, these plugin methods are worth exploring as accessible ways to steer generative flows where you want them to go.</p>]]></content><author><name>Anonymous Authors</name></author><summary type="html"><![CDATA[This tutorial demonstrates how to add inference-time controls to pretrained flow-based generative models. Using an unconditional MNIST flow model, we apply classifier guidance and inpainting by adding velocity corrections during sampling. We also explore PnP-Flow, which satisfies constraints through iterative projection rather than velocity correction.]]></summary></entry><entry><title type="html">Ready For General Agents? Let’s Test It.</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/general-agent-evaluation/" rel="alternate" type="text/html" title="Ready For General Agents? Let’s Test It."/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/general-agent-evaluation</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/general-agent-evaluation/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>Recent progress in LLMs has pushed the field from domain-specific systems toward increasingly general-purpose models. A similar shift is emerging for AI agents: domain agents share reusable components and can already operate across multiple domains with minimal adaptation. This ability to integrate into new environments and solve entirely new classes of tasks gives general agents the potential for effectively unbounded real-world value. Yet current evaluation tools cannot measure this core capability. We organize existing work into a five-level taxonomy and identify the missing fifth level: general agent evaluation, which must assess how well an agent operates across many unfamiliar environments. We outline the challenges that prevent such evaluation today and propose the requirements for a protocol-agnostic framework that can reliably measure the generality and adaptability of emerging agent systems.</p> <h2 id="introduction">Introduction</h2> <p>Over the past few years, the NLP community has shifted from building domain-specific systems such as standalone summarization or translation models toward developing general-purpose language models <d-cite key="brown2020languagemodelsfewshotlearners"></d-cite><d-cite key="bommasani2022opportunitiesrisksfoundationmodels"></d-cite>. Many see this trend as a contemporary example of Richard Sutton’s bitter lesson <d-cite key="sutton2019bitter"></d-cite>: “The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.”</p> <p>This transition was not abrupt. It emerged from a long sequence of incremental advances that gradually expanded the scope and capability of models. Along the way, domain-specific solutions and increasingly general methods coexisted, each informing and accelerating the other.</p> <p>In this blog post, we argue that a similar shift is now unfolding in the field of AI agents: the field is moving from domain-specialized agents toward increasingly general-purpose ones. Agents that can address diverse types of multi-step tasks across different target domains and previously unseen environments.</p> <p>This development highlights the need for a unified evaluation framework for general-purpose agents that assesses their abilities across environments and compares different architectures. Such a framework is crucial for tracking progress, identifying gaps, and guiding the development of next-generation general agents. It is also essential for evaluating the core of generality itself: an agent’s ability to integrate into new environments and perform successfully.</p> <p>We begin by introducing a shared terminology for discussing domain and general agents, their evaluation, and the agent-to-environment communication protocol. Next, we describe how today’s domain agents are evolving toward greater generality and outline the effect we expect it to have on general agents. We then explore the benefits of general-purpose agents and illustrate their advantages through two representative use cases. This motivates the need for evaluation solutions that can assess general agent capabilities. For that end, we survey the current landscape of agent evaluation, presenting a five-level taxonomy, and detail the limitations that make existing approaches insufficient for easily evaluating general agents. We then assess whether existing agentic protocols can address these limitations. Finally, we outline key requirements that a general agent solution needs to fulfill.</p> <p>We hope this blog will help clarify what general agents are, increase awareness of their emergence from domain-specific agents, and highlight the gaps in their evaluation. More broadly, we aim for it to serve as a call to action that inspires a community-wide effort to develop rigorous, scalable, and actionable evaluation frameworks. We believe that as agents become more general and autonomous, such frameworks are essential to guide their progress.</p> <h2 id="preliminary">Preliminary</h2> <p>AI agents are autonomous, goal-driven systems that perform multi-step tasks by interacting with their environment <d-cite key="bandi2025rise"></d-cite>. The environment is the “world” the agent is situated in. The agent can observe and interact with the environment, obtaining observations according to its internal mechanism <d-cite key="cheng2024exploringlargelanguagemodel"></d-cite>. An agent deployed in the environment can interact with it to achieve its goal. Many of those terms were adopted and adjusted from the domain of reinforcement learning to the field of LLM and AI agents.</p> <p>AI agents are systems composed of interacting algorithmic components for reasoning, planning, memory preservation, code execution, and more. The orchestration of these components, often referred to as the agent’s architecture or scaffold, collectively determines the agent’s behavior. A large language model typically serves as the central computational element, providing core capabilities for perception, reasoning, and generation. The agent can also have access to external capabilities like code execution and search.</p> <p>Another important concept is the agent-to-environment communication protocol. This protocol interface describes the way the agent interacts with its environment. Main examples are web browsing, terminal, MCP, and tool schemas.</p> <p>Most current agents are being developed with a specific domain in mind <d-cite key="Wang_2024"></d-cite><d-cite key="yehudai2025surveyevaluationllmbasedagents"></d-cite>. Common examples are web agents and software engineering agents (SWE agents). In such cases, the agent is restricted to a relevant set of components and tools, and the environment is tailored to the target domain.</p> <p>To evaluate the capabilities of different domain agents, researchers defined domain-specific benchmarks. Such benchmarks require an environment in which the agent can operate, a set of tasks that describe the agent’s goal, and a metric that measures whether the agent has achieved its task. These benchmarks enable the assessment of the efficacy of domain agents and the comparison of different backbone LLMs.</p> <p>In contrast to these types of agents, general-purpose agents are designed to handle a diverse set of tasks across different environments. This requires them to be adaptable to different kinds of previously unseen environments, each with its own tools, requirements, and specific setup.</p> <p>In essence, general agents are defined by their capacity to integrate into new problem spaces, absorb their domain knowledge, refine their behavior through interaction, and ultimately master the tasks they encounter.</p> <h2 id="the-shift-to-general-agents">The Shift to General Agents</h2> <p>Large language models are general-purpose systems; they are designed to handle diverse tasks. Yet they have a fixed static knowledge of the world and can only interact with it by producing text. To overcome this challenge, researchers advise utilizing tools that allow LLMs to interact with the world (for example searching the web or running code). To further advance this ability, researchers also develop designed patterns such as ReAct <d-cite key="yao2023reactsynergizingreasoningacting"></d-cite> and CodeAct <d-cite key="wang2024executablecodeactionselicit"></d-cite> that facilitate a loop of interaction between the LLM and the environment. Such simple agents can be deployed in any environment to achieve multi-step tasks, making them early versions of general agents.</p> <p>Although these agents are simple, equipping them with the right tools can provide an effective solution. For example, many providers now recommend loop-based agents or agent frameworks as a standard pattern for application development. Even massively used LLM interfaces for both consumer and developer have quietly evolved from a conversation with an LLM to an interaction with an agent equipped with tools for coding and searching.</p> <p>On the other hand, such agents fall short when compared to more complex and specialized agents on target domains. Domain agents utilize more structure; they tend to have components for planning, memory, state tracking, tool use, and error handling to support reliable, iterative interaction with the domain environment. They top domain-specific leaderboards and provide real-world value. For example, SWE agents are already solving millions of GitHub issues without intervention <d-cite key="PRArena"></d-cite>, and deep research agents are being deployed to millions of users <d-cite key="McKay2025_OpenAIDeepResearch"></d-cite>.</p> <p>While different domain agents are by design different from one another, they share similar components. If we look at different SWE agents, such as Claude Code, Codex CLI, and different deep research agents such as OpenAI and Perplexity ones, they all share similar algorithmic components <d-cite key="bgauryy_open-docs_2025"></d-cite><d-cite key="langchain_ai_open_deep_research_2025"></d-cite>. Moreover, each of those components is not specific to its domain but is a general component that could be used for any target domain. As a result, such domain agents that rely on general components can be easily adopted to other domains and tasks. Anthropic wrote: “Over the past several months, Claude Code has become far more than a coding tool. At Anthropic, we’ve been using it for deep research, video creation, and note-taking, among countless other non-coding applications. In fact, it has begun to power almost all of our major agent loops.” Additionally, they released their Claude Agent SDK to serve as building blocks for developing other agents<sup id="fnref:claude-sdk"><a href="#fn:claude-sdk" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. This demonstrates that domain-specific agents are becoming more proficient and general, allowing them to target a wider set of tasks. It also shows the new type of general agents, composed of plug-and-play general components that can be applied to a diverse set of domains across different environments.</p> <h2 id="the-promise-of-general-agents">The Promise of General Agents</h2> <p>In all machine learning tasks, simple and effective solutions are better than their specialized counterparts. Such solutions generalize better, are more robust, and are less prone to overfitting <d-cite key="shalev2014understanding"></d-cite><d-cite key="haussler1990andrzej"></d-cite>. Unlike domain agents that can be over-specialized and tailored to a specific task or domain, general agents work with different environments, forcing them to generalize. They receive diverse signals from a wide range of environments, requiring them to be robust. As a result, they tend to be more cost-effective.</p> <p>We examine two concrete examples of SWE and scientific agents that demonstrate that even simple versions of general agents have a lower cost of development and are more cost-effective. To quantify these qualities, we measure lines of code (LOC) and agent average cost per task.</p> <h3 id="case-1-scientific-agents">Case 1: Scientific Agents</h3> <p>ASTA Bench <d-cite key="bragg2025astabenchrigorousbenchmarkingai"></d-cite>, an effort towards benchmarking deep scientific research agents, provides a clear test. The specialized ASTA-v0 system scores 55% at $3.40 per task and contains subsystems exceeding 13,000 lines of code (LOC)<sup id="fnref:asta-loc"><a href="#fn:asta-loc" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Yet the second-best system is a 300-line ReAct general agent scoring 41% at just $0.31. On the literature-understanding subtask, ReAct scores 53%; while still below the Asta agent (62%) it outperforms both the specialized ASTA Paper Finder (21%) and OpenAI Deep Research (19%).</p> <table> <thead> <tr> <th>Agent</th> <th>LLMs used</th> <th>ASTA score</th> <th>Cost per task</th> <th>LOC</th> </tr> </thead> <tbody> <tr> <td>ASTA-v0</td> <td>Claude 4 Sonnet, Gemini 2.5 Flash, O3, GPT 4.1, GPT-4o</td> <td>53%</td> <td>$3.40</td> <td>&gt;13,768<sup id="fnref:asta-loc:1"><a href="#fn:asta-loc" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></td> </tr> <tr> <td>ReAct</td> <td>GPT-5</td> <td>44%</td> <td>$0.31</td> <td>358<sup id="fnref:react-code"><a href="#fn:react-code" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></td> </tr> </tbody> </table> <h3 id="case-2-swe-agents">Case 2: SWE Agents</h3> <p>In SWE-Bench <d-cite key="yang2025swesmith"></d-cite>, the specialized SWE-Agent scores 67%, but the tiny, domain-agnostic Mini SWE-Agent scores 65% while being roughly 30 times smaller and about 7 times cheaper per run.</p> <table> <thead> <tr> <th>Agent</th> <th>LLM</th> <th>SWE-Bench score</th> <th>Cost per task</th> <th>LOC</th> </tr> </thead> <tbody> <tr> <td>SWE-Agent</td> <td>Claude 4 Sonnet</td> <td>67%</td> <td>~$2.50</td> <td>4,161</td> </tr> <tr> <td>Mini SWE-Agent</td> <td>Claude 4 Sonnet</td> <td>65%</td> <td>$0.37</td> <td>131</td> </tr> </tbody> </table> <h3 id="the-promise">The Promise</h3> <p>As general agents mature, they will become more complex. Yet they hold great promise. Building a general agent can provide a singular solution applied to a wide range of cases. This can make such an effort much more impactful, similar to how OpenAI was able to surpass many task-specific solutions by providing a general-purpose LLM. It also provides a lower cost of development compared to building many domain-specific ones. Additionally, it reduces the development time required for building domain-specific agents, as they can start from a general agent and further adapt it to their target domain, similar to fine-tuning a general-purpose LLM for a specific task.</p> <h2 id="state-of-general-agent-evaluation">State of General Agent Evaluation</h2> <p>There is a shift in the field of AI agents from domain-specific agents to general-purpose ones, and a general agent can be easier to develop compared to many domain-specific agents, more robust, and more cost-effective. This shift raises the need for evaluation solutions that are suitable for evaluating a general agent. This requires a framework that enables running the same agent across different benchmarks and environments to evaluate adaptability. Yet there is no such solution.</p> <p>To address this gap, we organize the agent evaluation solutions space into five levels, starting from the specific to the more general. This organization helps outline the missing level for evaluating any general agent across environments.</p> <h3 id="level-1-agentic-skills-evaluation">Level 1: Agentic Skills Evaluation</h3> <p>The first level focuses on evaluating LLMs on agentic skills such as reasoning, planning, and tool calling without embedding them in a dynamic environment. Benchmarks provide a textual prompt and expect a textual response. The model’s response is assessed independently of any interaction loop or adaptive environment. These evaluations measure whether a model can demonstrate an agentic capability in principle, but they do not assess whether the model can deploy that capability reliably in realistic long-horizon tasks. Nonetheless, they can provide insight into the ability of a model to succeed in certain components.</p> <p>Representative benchmarks include GSM8K <d-cite key="cobbe2021trainingverifierssolvemath"></d-cite>, which evaluates step-by-step mathematical reasoning; HotPotQA <d-cite key="yang2018hotpotqadatasetdiverseexplainable"></d-cite>, which tests multi-hop question answering; and BFCL <d-cite key="patil2023gorilla"></d-cite>, which measures tool-use capabilities.</p> <h3 id="level-2-domain-agent-evaluation">Level 2: Domain-Agent Evaluation</h3> <p>The second level uses interactive environments, such as web browsers, applications, or terminal interfaces, with a set of tools that allow the agent to interact with them. The agent gets tasks requiring multiple steps that it needs to perform by interacting with the environment. The agent sequence of LLM and tool calls, named the agent trajectory, is then evaluated by an environment-specific metric that assesses whether the agent task was achieved.</p> <p>These benchmarks provide high value for assessing domain-specific agent capabilities. However, each benchmark often defines its own custom setup, leading to agent logic that is tightly coupled to each environment. As a result, some benchmarks are used to assess LLMs in agentic environments, while others require agents tailored to the specific benchmark, making it hard to compare the same agent across benchmarks.</p> <p>Representative examples include Tau-Bench <d-cite key="yao2024tau"></d-cite><d-cite key="barres2025tau2"></d-cite> for customer-service scenarios, AppWorld <d-cite key="trivedi-etal-2024-appworld"></d-cite> for multi-application tasks, WebArena <d-cite key="zhou2024webarenarealisticwebenvironment"></d-cite> for browser interactions, TerminalBench <d-cite key="tbench_2025"></d-cite> for Linux command-line tasks, and SWE-Bench <d-cite key="yang2025swesmith"></d-cite> for solving GitHub issues.</p> <h3 id="level-3-agentic-cross-model-evaluation">Level 3: Agentic Cross-Model Evaluation</h3> <p>The third level focuses on providing a standardized evaluation harness for reproducible agent evaluations across various domain-specific benchmarks. It can be used for comparing LLMs as the backbone of different agents, or to compare domain-specific agents on a single benchmark. Yet their standardization does not allow running the same agent across different benchmarks.</p> <p>A representative example is HAL <d-cite key="hal"></d-cite>, which compiles nine benchmarks from the second level. Each environment in HAL comes with its own fixed agent setup, and users can easily change the backbone model of an agent or add support to a new domain-specific agent. Hence, this level still does not support comparing different agent architectures across environments as needed for general agent assessment.</p> <h3 id="level-4-protocol-centric-agent-evaluation">Level 4: Protocol-Centric Agent Evaluation</h3> <p>The fourth level moves beyond fixed agent setups by defining a standardized interaction protocol, such as a unified browser API or terminal interface, that any agent can implement. Instead of each environment defining its own custom agent-to-environment communication protocol, this standardization forces the agent to follow a specific protocol to be consistently evaluated across many environments. This enables comparisons not just across models, but also across different agent architectures.</p> <p>However, protocol-centric frameworks still impose a specific mode of interaction. Agents built around fundamentally different communication protocols, such as those using the Model Context Protocol (MCP), cannot be evaluated in their native form. They must be forced through the protocol’s interface, which can obscure their design and distort performance. For example, Harbor <d-cite key="tbench_2025"></d-cite> evaluates agents through a command-line protocol, preventing MCP-based agents like Claude Code from being tested as intended.</p> <p>Representative examples include BrowserGym <d-cite key="chezelles2025browsergym"></d-cite>, which standardizes browser interaction across diverse web tasks, and Harbor <d-cite key="tbench_2025"></d-cite>, which provides a unified terminal protocol.</p> <h3 id="level-5-general-agent-evaluation">Level 5: General Agent Evaluation</h3> <p>A fifth level, still missing today, would provide a framework for general agent evaluation. It would enable the evaluation of the same agent across environments without being forced to use a specific communication protocol. It would reveal how an agent actually performs in realistic settings, how design choices influence outcomes, and how well agents generalize across diverse tasks and environments. Achieving this level of flexibility and fairness remains an open challenge, and the lack of a unified, protocol-agnostic evaluation paradigm is a central barrier to progress toward genuinely general-purpose agents.</p> <table> <thead> <tr> <th>Level</th> <th>Cross-model</th> <th>Agentic environment interaction</th> <th>Cross-environment</th> <th>Cross-agent</th> <th>Protocol-agnostic</th> <th>Examples</th> </tr> </thead> <tbody> <tr> <td>1: Agentic model skills</td> <td>Yes</td> <td>No</td> <td>No</td> <td>No</td> <td>No</td> <td>BFCL, GSM8K, HotPotQA</td> </tr> <tr> <td>2: Interactive agentic model</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>No</td> <td>No</td> <td>Tau-Bench, AppWorld, WebArena, TerminalBench</td> </tr> <tr> <td>3: Cross-model harness</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>No</td> <td>HAL</td> </tr> <tr> <td>4: Protocol-centric</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>BrowserGym, Harbor</td> </tr> <tr> <td>5: General agent evaluation</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>(missing)</td> </tr> </tbody> </table> <h2 id="challenges-in-general-agent-evaluation">Challenges in General Agent Evaluation</h2> <p>Benchmarking general agents in different environments is challenging. Existing benchmarks were typically designed with a specific agent domain and goal in mind, such as a user-facing conversational agent, a computer-using autonomous coder, or a web-navigation agent. These design choices led to incompatible setups, preventing the development of a single, unified evaluation protocol that any agent can be seamlessly integrated into. We outline key challenges that must be resolved to provide a unified standard for general agent evaluation.</p> <h3 id="lack-of-standardized-agent-interface">Lack of Standardized Agent Interface</h3> <p>Many benchmarks implicitly assume the tested agent possesses certain built-in, domain-specific capabilities.</p> <ul> <li>Tau-Bench assumes an agent that can inherently message or converse with a user <d-cite key="yao2024tau"></d-cite>:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseAgent</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">Generic</span><span class="p">[</span><span class="n">AgentState</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">Base agent class that defines the common interface for agents.</span><span class="sh">"""</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">generate_next_message</span><span class="p">(...,</span> <span class="n">message</span><span class="p">:</span> <span class="n">UserMessage</span> <span class="o">|</span> <span class="n">ToolMessage</span> <span class="o">|</span> <span class="n">MultiToolMessage</span><span class="p">..)</span> <span class="o">-&gt;</span> <span class="p">...</span> <span class="n">AssistanceMessage</span><span class="p">:</span>
        <span class="bp">...</span>
</code></pre></div></div> <ul> <li>WebArena assumes an agent whose entire perceptual and action space is mediated through a browser interface controlled by predefined actions <d-cite key="zhou2024webarenarealisticwebenvironment"></d-cite>:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Base class for the agent</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">next_action</span><span class="p">(...,</span> <span class="n">trajectory</span><span class="p">:</span> <span class="n">Trajectory</span><span class="p">,...)</span> <span class="o">-&gt;</span> <span class="n">Action</span><span class="p">:</span>
        <span class="c1"># ActionType: NONE, SCROLL, KEY_PRESS, MOUSE_CLICK, KEYBOARD_TYPE, MOUSE_HOVER, CLICK, TYPE, HOVER,
</span>        <span class="c1"># PAGE_FOCUS, NEW_TAB, GO_BACK, GO_FORWARD, GOTO_URL, PAGE_CLOSE, ...
</span>        <span class="bp">...</span>
</code></pre></div></div> <ul> <li>TerminalBench assumes an agent whose interaction interface is a computer with a command line <d-cite key="tbench_2025"></d-cite>:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseAgent</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">run</span><span class="p">(...,</span> <span class="n">environment</span><span class="p">:</span> <span class="n">BaseEnvironment</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">...</span>

<span class="k">class</span> <span class="nc">BaseEnvironment</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">exec</span><span class="p">(...,</span> <span class="n">command</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="n">ExecResult</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Executes a command in the environment...</span><span class="sh">"""</span>
</code></pre></div></div> <p>These assumptions are mutually incompatible. A web-browsing agent cannot converse with a user, while a chat-oriented agent cannot click on a web element. Such rigid, benchmark-specific communication protocols make cross-environment evaluation impossible without substantial ad hoc engineering.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-480.webp 480w,/2026/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-800.webp 800w,/2026/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="lack-of-standardized-environment-interfaces">Lack of Standardized Environment Interfaces</h3> <p>Benchmarks rarely specify, in an agent-agnostic way, what task the agent must perform, what information it should receive, or what actions it can take and how those actions affect the environment. SWE-Bench defines issues the agent should solve but does not supply standard instructions on how the issues should be solved, how the solution will be validated, or how the submitted solution should be structured. Without an explicit environment interface, every user must design their own, creating different setups for different agents.</p> <p>Other benchmarks communicate environment semantics only in a form tailored to a specific agent architecture. Tau-Bench provides important aspects of the task mixed with assumptions specific to a conversational LLM agent:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">AGENT_INSTRUCTION</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
You are a customer service agent that helps the user according to the &lt;policy&gt; provided below.
In each turn you can either:
- Send a message to the user.
- Make a tool call.
You cannot do both at the same time.

Try to be helpful and always follow the policy. Always make sure you generate valid JSON only.
</span><span class="sh">"""</span>
</code></pre></div></div> <p>Such constraints make sense for a conversational LLM but do not generalize to other types of agents, such as code-act agents. The lack of standardization prevents testing from scaling to many environments, which is essential for evaluating an agent’s ability to adapt.</p> <h3 id="lack-of-a-standardized-researcher-interface">Lack of a Standardized Researcher Interface</h3> <p>To support seamless experimentation without hours spent integrating each new environment, researcher-facing interfaces must also be standardized and simplified. Today, every environment demands its own setup, scripts, and output formats. Benchmarks report outcomes in different formats, store them in different locations, and follow different conventions. Even the basic metrics differ across representative benchmarks-not only in which quantities are tracked, but also in terminology and aggregation standards.</p> <table> <thead> <tr> <th>Metric type</th> <th>Tau-Bench</th> <th>AppWorld</th> <th>SWE-Bench</th> </tr> </thead> <tbody> <tr> <td>Success (bool)</td> <td>Reward = 1</td> <td>success</td> <td>Resolved</td> </tr> <tr> <td>Score (float)</td> <td>Reward</td> <td>Score</td> <td>-</td> </tr> <tr> <td>Termination</td> <td>Termination reason</td> <td>-</td> <td>-</td> </tr> <tr> <td>Duration</td> <td>Duration</td> <td>-</td> <td>-</td> </tr> <tr> <td>Number of interactions</td> <td>Num messages</td> <td>Steps</td> <td>-</td> </tr> <tr> <td>Agent cost</td> <td>Agent cost</td> <td>-</td> <td>-</td> </tr> <tr> <td>Environment cost</td> <td>User cost</td> <td>-</td> <td>-</td> </tr> <tr> <td>Task ID</td> <td>Task ID</td> <td>Task ID</td> <td>Instance ID</td> </tr> <tr> <td>Logs</td> <td>Message log</td> <td>Task logs</td> <td>Test logs</td> </tr> <tr> <td>Success rate (benchmark)</td> <td>Avg reward</td> <td>Task goal completion</td> <td>Resolved counts</td> </tr> <tr> <td>Cost aggregate (benchmark)</td> <td>Avg agent cost</td> <td>-</td> <td>-</td> </tr> </tbody> </table> <p>These gaps underscore the need for consolidation and standardization to make large-scale evaluation of general agents feasible.</p> <h2 id="existing-agent-environment-protocols">Existing Agent-Environment Protocols</h2> <p>Several protocols have recently emerged to standardize interaction, discovery, and data exchange in agentic systems. Some of these can, in principle, support evaluation. A2A provides a standardized way for agents to discover each other’s capabilities, negotiate interaction modes, and coordinate on collaborative tasks, which could inform how benchmarks deliver tasks to agents <d-cite key="a2a_protocol"></d-cite>. MCP defines a structured way for agents to access external tools, data resources, and prompt templates <d-cite key="anthropic_mcp_2024"></d-cite>. In an evaluation setting, a benchmark could expose its environment via an MCP server.</p> <p>We ask two questions:</p> <ol> <li>Should the research community adopt a single protocol for general-agent evaluation?</li> <li>Do current protocols satisfy the practical needs of evaluation today?</li> </ol> <p>Regarding the first question, we caution against prematurely standardizing evaluation processes on a single protocol. Lock-in at this stage risks constraining innovation as agent capabilities and demands still evolve. More fundamentally, evaluation must remain capable of assessing everything-including the protocol itself-making early commitment counterproductive.</p> <p>To address the second question, we use MCP as a case study for whether existing protocols can support end-to-end evaluation workflows. MCP defines three core primitives: tools (invocable operations), resources (exposed data/content), and prompts (parameterized templates), along with mechanisms for streaming events and updates. While MCP provides a promising foundation for unifying agent-environment interaction, several gaps prevent it from serving as a complete solution for general-agent evaluation:</p> <ul> <li>Missing support for benchmark task semantics: benchmarks center around tasks, but MCP does not offer a built-in way to represent or communicate them.</li> <li>Missing support for evaluation workflows: evaluation depends on standardized metrics reporting, aggregation, logging, experiment tracking, and reproducibility. MCP is intentionally agnostic to these needs.</li> <li>Inconsistent ecosystem adoption: many frameworks implement tool calling but not resources or prompts, resulting in inconsistent capabilities and substantial integration overhead.</li> </ul> <table> <thead> <tr> <th>Agent framework</th> <th>MCP tools</th> <th>MCP resources</th> <th>MCP prompts</th> </tr> </thead> <tbody> <tr> <td>Smolagents</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>Llama Stack</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>OpenAI Agents SDK</td> <td>Yes</td> <td>No</td> <td>Yes</td> </tr> <tr> <td>Codex CLI</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>Claude Code</td> <td>Yes</td> <td>Yes</td> <td>No</td> </tr> </tbody> </table> <p>These limitations indicate that while protocols like MCP and A2A lay important groundwork, they do not yet meet the full requirements of standardized general-agent evaluation.</p> <h2 id="general-agent-evaluation-framework">General Agent Evaluation Framework</h2> <p>Advancing general-purpose agents requires an evaluation framework that is itself general: capable of supporting different environments, diverse agent architectures, and multiple communication protocols. Standardizing such evaluation is intrinsically difficult. Environments vary widely, implicit assumptions fragment the space, and existing protocols address only parts of the challenge. Even promising standards such as MCP provide useful building blocks but remain incomplete. These pressures motivate the need for a unifying layer that can support evaluating any agent on any benchmark without restriction to a specific communication protocol.</p> <h3 id="a-meta-protocol-for-general-evaluation">A Meta-Protocol for General Evaluation</h3> <p>At the core of such a framework is a meta-protocol: an abstract, protocol-agnostic layer that defines the semantics of evaluation independently of any concrete agent-environment protocol. Current benchmarks implicitly bind evaluation to a specific communication protocol, thereby entangling agent performance with protocol-specific setup.</p> <p>The meta-protocol needs to specify how tasks, actions, observations, documentation, and termination conditions are represented while allowing different communication protocols. This abstraction allows interfaces such as web browsing, terminal, MCP, and tool schemas without altering the evaluation semantics. By enabling protocol modularity rather than protocol uniformity, a meta-protocol allows agents to be evaluated in their native interaction modes and makes cross-protocol comparisons meaningful.</p> <h3 id="core-characteristics-of-a-general-evaluation-framework">Core Characteristics of a General Evaluation Framework</h3> <ul> <li>Environment-agnostic agent interface. The agent-facing interface should function across any environment. Environments expose a minimal, standardized schema for actions, observations, tasks, and documentation, with all assumptions explicit and discoverable rather than embedded in reference agents.</li> <li>Agent-agnostic environment interface. Environments should not assume a specific agent architecture, reasoning style, or protocol. Simple ReAct agents, memory-augmented agents, MCP-based agents, browser agents, and command-line agents should all integrate without modification.</li> <li>Plug-and-play modularity. Models, agent architectures, interaction protocols, and environments should be swappable without altering the evaluation protocol. This enables controlled comparisons across dimensions within a unified setup.</li> <li>Standardized reporting and transparency. The framework should define consistent reporting conventions, including metrics for success, robustness, interaction efficiency, and cost. Observability support for logging and full trajectory tracking is essential.</li> </ul> <p>Such a framework would let us measure the core property that makes general agents uniquely valuable: adaptability to new environments with minimal re-engineering.</p> <h2 id="conclusions">Conclusions</h2> <p>General-purpose agents are arriving faster than our ability to measure them. Simple, modular systems already challenge specialized stacks, but fragmented benchmarks hide which ideas truly matter. A protocol-agnostic evaluation layer paired with clear environment contracts and consistent reporting would give the field a common yardstick for progress and illuminate how design choices transfer across domains.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:claude-sdk"> <p>https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk <a href="#fnref:claude-sdk" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:asta-loc"> <p>Based on the python files in https://github.com/allenai/asta-paper-finder/tree/main/agents/mabool/api/mabool. <a href="#fnref:asta-loc" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:asta-loc:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:react-code"> <p>Source: https://github.com/allenai/asta-bench/blob/f7e25392f4dda167f4e6d46b8c7c080eeeb4cc35/astabench/solvers/react/basic_agent.py. <a href="#fnref:react-code" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[General-purpose agents are emerging, but current evaluation tools cannot yet measure how well they adapt to unfamiliar environments or protocols; we outline the gaps and a path to a protocol-agnostic framework.]]></summary></entry></feed>